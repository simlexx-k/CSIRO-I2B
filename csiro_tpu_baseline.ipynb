{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "569731cc",
   "metadata": {},
   "source": [
    "# CSIRO Biomass TPU Baseline\n",
    "\n",
    "TPU-friendly notebook that trains a TensorFlow model end-to-end so we can compare TPU vs GPU performance without relying on GPU-only libraries (xgboost, lightgbm, catboost, timm, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72e6367",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e8026f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TPU strategy --------------------------------------------------------------\n",
    "try:\n",
    "    resolver = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n",
    "    strategy = tf.distribute.TPUStrategy(resolver)\n",
    "    print(f\"Running on TPU: {resolver.master()}\")\n",
    "except ValueError:\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "    print(\"TPU not found, falling back to default strategy\")\n",
    "\n",
    "def seed_all(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "@dataclass\n",
    "class CFG:\n",
    "    data_root: Path = Path(\"/kaggle/input/csiro-biomass\")\n",
    "    image_size: int = 448\n",
    "    batch_size: int = 16\n",
    "    epochs: int = 5\n",
    "    val_split: float = 0.15\n",
    "    seed: int = 42\n",
    "\n",
    "TARGET_NAMES = [\"Dry_Clover_g\", \"Dry_Dead_g\", \"Dry_Green_g\", \"Dry_Total_g\", \"GDM_g\"]\n",
    "seed_all(CFG.seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ff3660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metadata -------------------------------------------------------------\n",
    "train_csv = CFG.data_root / \"train.csv\"\n",
    "test_csv = CFG.data_root / \"test.csv\"\n",
    "train_df = pd.read_csv(train_csv)\n",
    "test_df = pd.read_csv(test_csv)\n",
    "\n",
    "def pivot_targets(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if \"target\" in df.columns:\n",
    "        wide = df.pivot_table(\n",
    "            index=[\"image_path\", \"Sampling_Date\", \"State\", \"Species\", \"Pre_GSHH_NDVI\", \"Height_Ave_cm\"],\n",
    "            columns=\"target_name\",\n",
    "            values=\"target\",\n",
    "            aggfunc=\"mean\",\n",
    "        ).reset_index()\n",
    "    else:\n",
    "        df = df.copy()\n",
    "        df[\"dummy\"] = 0.0\n",
    "        wide = df.pivot_table(index=\"image_path\", columns=\"target_name\", values=\"dummy\", aggfunc=\"mean\").reset_index()\n",
    "    return wide\n",
    "\n",
    "train_wide = pivot_targets(train_df)\n",
    "test_wide = pivot_targets(test_df)\n",
    "print(f\"Train records: {len(train_wide)}, Test records: {len(test_wide)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bec994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/validation split ----------------------------------------------------\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def make_image_path(rel_path: str) -> str:\n",
    "    return str(CFG.data_root / rel_path)\n",
    "\n",
    "train_wide[\"abs_path\"] = train_wide[\"image_path\"].apply(make_image_path)\n",
    "test_wide[\"abs_path\"] = test_wide[\"image_path\"].apply(make_image_path)\n",
    "\n",
    "train_idx, val_idx = train_test_split(\n",
    "    np.arange(len(train_wide)),\n",
    "    test_size=CFG.val_split,\n",
    "    random_state=CFG.seed,\n",
    "    shuffle=True,\n",
    ")\n",
    "train_meta = train_wide.iloc[train_idx].reset_index(drop=True)\n",
    "val_meta = train_wide.iloc[val_idx].reset_index(drop=True)\n",
    "print(f\"Train split: {len(train_meta)} | Val split: {len(val_meta)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f12da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.data pipelines ---------------------------------------------------------\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "def decode_image(path: tf.Tensor) -> tf.Tensor:\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, (CFG.image_size, CFG.image_size))\n",
    "    img = tf.cast(img, tf.float32) / 255.0\n",
    "    return img\n",
    "\n",
    "def augment(img: tf.Tensor) -> tf.Tensor:\n",
    "    img = tf.image.random_flip_left_right(img)\n",
    "    img = tf.image.random_flip_up_down(img)\n",
    "    return img\n",
    "\n",
    "def build_dataset(meta_df: pd.DataFrame, training: bool) -> tf.data.Dataset:\n",
    "    paths = meta_df[\"abs_path\"].values\n",
    "    targets = meta_df[TARGET_NAMES].values if training else None\n",
    "    path_ds = tf.data.Dataset.from_tensor_slices(paths)\n",
    "    img_ds = path_ds.map(decode_image, num_parallel_calls=AUTOTUNE)\n",
    "    if training:\n",
    "        img_ds = img_ds.map(lambda x: augment(x), num_parallel_calls=AUTOTUNE)\n",
    "    if training:\n",
    "        target_ds = tf.data.Dataset.from_tensor_slices(targets.astype(\"float32\"))\n",
    "        ds = tf.data.Dataset.zip((img_ds, target_ds))\n",
    "    else:\n",
    "        ds = img_ds.map(lambda x: (x,))\n",
    "    if training:\n",
    "        ds = ds.shuffle(2048, seed=CFG.seed)\n",
    "    ds = ds.batch(CFG.batch_size).prefetch(AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "train_ds = build_dataset(train_meta, training=True)\n",
    "val_ds = build_dataset(val_meta, training=True)\n",
    "test_ds = build_dataset(test_wide, training=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b6cb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition ----------------------------------------------------------\n",
    "with strategy.scope():\n",
    "    base = tf.keras.applications.EfficientNetV2S(\n",
    "        include_top=False,\n",
    "        input_shape=(CFG.image_size, CFG.image_size, 3),\n",
    "        weights='imagenet'\n",
    "    )\n",
    "    base.trainable = False  # fine-tune later if needed\n",
    "    inputs = tf.keras.Input(shape=(CFG.image_size, CFG.image_size, 3))\n",
    "    x = base(inputs, training=False)\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = tf.keras.layers.Dropout(0.25)(x)\n",
    "    x = tf.keras.layers.Dense(512, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    outputs = tf.keras.layers.Dense(len(TARGET_NAMES), activation='linear')(x)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss='mae',\n",
    "    metrics=[tf.keras.metrics.MeanAbsoluteError(name='mae')]\n",
    ")\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5864d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training -----------------------------------------------------------------\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(patience=2, factor=0.5, verbose=1),\n",
    "    tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True, verbose=1)\n",
    "]\n",
    "\n",
    "ehistory = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=CFG.epochs,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40f12c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference & submission ----------------------------------------------------\n",
    "preds = model.predict(test_ds, verbose=1)\n",
    "sub_df = pd.DataFrame(preds, columns=TARGET_NAMES)\n",
    "sub_df.insert(0, 'image_path', test_wide['image_path'])\n",
    "sub_df = sub_df.melt(id_vars='image_path', value_vars=TARGET_NAMES, var_name='target_name', value_name='target')\n",
    "sub_df = sub_df.merge(test_df[['sample_id', 'image_path', 'target_name']], on=['image_path', 'target_name'], how='left')\n",
    "submission = sub_df[['sample_id', 'target']].copy()\n",
    "submission['target'] = submission['target'].clip(min=0)\n",
    "submission.to_csv('submission_tpu.csv', index=False)\n",
    "submission.head()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
