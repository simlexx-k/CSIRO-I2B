{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":112509,"databundleVersionId":14254895,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":4537,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":3329,"modelId":986}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"47843f56-629b-4bcb-a106-cea915c82666","cell_type":"markdown","source":"# CSIRO - Image2Biomass with DINOv2 ViT\nFine-tunes Meta's DINOv2 Vision Transformer (via `timm`) on the CSIRO Image2Biomass dataset to predict five pasture biomass measurements from aerial imagery.\n","metadata":{}},{"id":"cfab2976-b2e0-4fd2-9bb5-834f1a34edfb","cell_type":"markdown","source":"## Approach Overview\nWe load the provided train/test CSVs, reshape labels into per-image targets, and split folds by image. A DINOv2 backbone feeds a lightweight regression head, optimizing Smooth L1 losses across all biomass components. After training, we ensemble fold checkpoints to generate predictions and write the final `submission.csv`.\n","metadata":{}},{"id":"049d3534-08c3-457f-b694-c5252040696e","cell_type":"code","source":"!pip install -q timm==0.9.16 albumentations==1.4.14 scikit-learn==1.3.2 scipy==1.11.4\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T10:43:04.085570Z","iopub.execute_input":"2025-12-29T10:43:04.086239Z","iopub.status.idle":"2025-12-29T10:43:07.494288Z","shell.execute_reply.started":"2025-12-29T10:43:04.086211Z","shell.execute_reply":"2025-12-29T10:43:07.493282Z"}},"outputs":[],"execution_count":16},{"id":"dcfefec6-8315-4ac1-99f7-9b41d4c97a11","cell_type":"code","source":"import os\nimport math\nimport random\nimport time\nfrom pathlib import Path\nfrom dataclasses import dataclass, asdict\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nimport torch \nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import OneCycleLR\nfrom sklearn.model_selection import GroupKFold\nimport timm\nfrom timm.data import create_transform\nprint(torch.__version__)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T10:43:11.564181Z","iopub.execute_input":"2025-12-29T10:43:11.564692Z","iopub.status.idle":"2025-12-29T10:43:11.569585Z","shell.execute_reply.started":"2025-12-29T10:43:11.564672Z","shell.execute_reply":"2025-12-29T10:43:11.568998Z"}},"outputs":[{"name":"stdout","text":"2.6.0+cu124\n","output_type":"stream"}],"execution_count":27},{"id":"a3004792-5215-49dc-ac44-652f9ee69612","cell_type":"code","source":"@dataclass\nclass cfg:\n    data_dir: Path = Path('/kaggle/input/csiro-biomass')\n    output_dir: Path = Path('.')\n    seed: int = 2024\n    img_size: int = 518\n    batch_size: int = 8\n    num_workers: int = 4\n    epochs: int = 5\n    lr: float = 2e-4\n    min_lr: float = 1e-6\n    weight_decay: float = 1e-4\n    backbone: str = 'vit_base_patch14_dinov2'\n    checkpoint_dir: Path = Path('/kaggle/input/dinov2/pytorch/giant/1')\n    hidden_dim: int = 512\n    drop_rate: float = 0.1\n    n_folds: int = 5\n    train_folds: tuple = (0,)\n    target_names: tuple = ('Dry_Clover_g','Dry_Dead_g','Dry_Green_g','Dry_Total_g','GDM_g')\n    num_targets: int = 5\n    use_amp: bool = True\n    grad_accum_steps: int = 1\n\ncfg = CFG()\n\npossible_dirs = [\n    Path('/kaggle/input/csiro-biomass'),\n    Path('/kaggle/input/CSIRO-I2B'),\n    Path('csiro-biomass'),\n]\nfor p in possible_dirs:\n    if p.exists():\n        cfg.data_dir = p\n        break\nprint(f\"Using data from: {cfg.data_dir}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T10:43:23.438220Z","iopub.execute_input":"2025-12-29T10:43:23.438871Z","iopub.status.idle":"2025-12-29T10:43:23.447523Z","shell.execute_reply.started":"2025-12-29T10:43:23.438849Z","shell.execute_reply":"2025-12-29T10:43:23.446459Z"}},"outputs":[{"name":"stdout","text":"Using data from: /kaggle/input/csiro-biomass\n","output_type":"stream"}],"execution_count":29},{"id":"35bed8be-3163-4de2-9904-e5fac6474726","cell_type":"code","source":"def seed_everything(seed: int = 42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nseed_everything(cfg.seed)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T10:43:26.037818Z","iopub.execute_input":"2025-12-29T10:43:26.038078Z","iopub.status.idle":"2025-12-29T10:43:26.043832Z","shell.execute_reply.started":"2025-12-29T10:43:26.038060Z","shell.execute_reply":"2025-12-29T10:43:26.043208Z"}},"outputs":[],"execution_count":30},{"id":"8feec992-ada7-41be-9bbe-3d73c5badc9e","cell_type":"code","source":"train_df = pd.read_csv(cfg.data_dir / 'train.csv')\ntest_df = pd.read_csv(cfg.data_dir / 'test.csv')\n\ntrain_targets = train_df.pivot_table(\n    index='image_path',\n    columns='target_name',\n    values='target'\n).reset_index().copy()\ntrain_targets.columns = ['image_path', *list(cfg.target_names)]\ntrain_targets[list(cfg.target_names)] = train_targets[list(cfg.target_names)].fillna(0.0).astype(float)\n\ntrain_targets['fold'] = -1\nkf = GroupKFold(n_splits=cfg.n_folds)\nfor fold, (_, val_idx) in enumerate(kf.split(train_targets, groups=train_targets['image_path'])):\n    train_targets.loc[val_idx, 'fold'] = fold\n\nprint(train_targets.head())\nprint(train_targets['fold'].value_counts())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T10:43:29.512634Z","iopub.execute_input":"2025-12-29T10:43:29.512920Z","iopub.status.idle":"2025-12-29T10:43:29.539948Z","shell.execute_reply.started":"2025-12-29T10:43:29.512899Z","shell.execute_reply":"2025-12-29T10:43:29.539339Z"}},"outputs":[{"name":"stdout","text":"               image_path  Dry_Clover_g  Dry_Dead_g  Dry_Green_g  Dry_Total_g  \\\n0  train/ID1011485656.jpg        0.0000     31.9984      16.2751      48.2735   \n1  train/ID1012260530.jpg        0.0000      0.0000       7.6000       7.6000   \n2  train/ID1025234388.jpg        6.0500      0.0000       0.0000       6.0500   \n3  train/ID1028611175.jpg        0.0000     30.9703      24.2376      55.2079   \n4  train/ID1035947949.jpg        0.4343     23.2239      10.5261      34.1844   \n\n     GDM_g  fold  \n0  16.2750     0  \n1   7.6000     4  \n2   6.0500     3  \n3  24.2376     2  \n4  10.9605     1  \nfold\n0    72\n1    72\n4    71\n3    71\n2    71\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":31},{"id":"d6e6f63d-642e-4f0b-a67f-ee749afdd127","cell_type":"code","source":"class BiomassDataset(Dataset):\n    def __init__(self, df: pd.DataFrame, mode: str, transform=None):\n        self.df = df.reset_index(drop=True)\n        self.mode = mode\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        image_path = cfg.data_dir / row['image_path']\n        image = Image.open(image_path).convert('RGB')\n        if self.transform:\n            image = self.transform(image)\n        else:\n            image = transforms.ToTensor()(image)\n        sample = {\n            'pixel_values': image,\n            'image_path': row['image_path']\n        }\n        if self.mode != 'test':\n            targets = row[list(cfg.target_names)].astype(float).values\n            target = torch.tensor(targets, dtype=torch.float32)\n            sample['targets'] = target\n        return sample","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T10:43:33.106135Z","iopub.execute_input":"2025-12-29T10:43:33.106941Z","iopub.status.idle":"2025-12-29T10:43:33.112963Z","shell.execute_reply.started":"2025-12-29T10:43:33.106919Z","shell.execute_reply":"2025-12-29T10:43:33.112149Z"}},"outputs":[],"execution_count":32},{"id":"74a00df1-9518-4d6a-8fe5-f6ea06b57717","cell_type":"code","source":"from torchvision import transforms\n\ndef get_transforms(is_train=True):\n    return create_transform(\n        input_size=(3, cfg.img_size, cfg.img_size),\n        is_training=is_train,\n        auto_augment='rand-m9-mstd0.5-inc1' if is_train else None,\n        interpolation='bicubic',\n        re_prob=0.25 if is_train else 0.0,\n        re_mode='pixel',\n        re_count=1,\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T10:43:36.813873Z","iopub.execute_input":"2025-12-29T10:43:36.814153Z","iopub.status.idle":"2025-12-29T10:43:36.818527Z","shell.execute_reply.started":"2025-12-29T10:43:36.814129Z","shell.execute_reply":"2025-12-29T10:43:36.817941Z"}},"outputs":[],"execution_count":33},{"id":"31ef70d1-a437-4c99-9583-af739617372b","cell_type":"code","source":"class DinoRegressor(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.backbone = timm.create_model(\n            cfg.backbone,\n            pretrained=False,\n            num_classes=0,\n            global_pool='' \n        )\n        checkpoint_path = None\n        if cfg.checkpoint_dir is not None:\n            ckpt_dir = Path(cfg.checkpoint_dir)\n            if ckpt_dir.exists():\n                for pattern in ('*.pth', '*.pt', '*.bin', '*.safetensors'):\n                    candidates = sorted(ckpt_dir.rglob(pattern))\n                    if candidates:\n                        checkpoint_path = candidates[0]\n                        break\n        if checkpoint_path is None:\n            raise FileNotFoundError('No DINOv2 checkpoint found under ' + str(cfg.checkpoint_dir))\n        state = torch.load(checkpoint_path, map_location='cpu')\n        if isinstance(state, dict):\n            for key in ('state_dict', 'model', 'model_state'):\n                if key in state:\n                    state = state[key]\n                    break\n        missing, unexpected = self.backbone.load_state_dict(state, strict=False)\n        if missing or unexpected:\n            print('Loaded checkpoint with missing keys:', missing)\n            print('Unexpected keys:', unexpected)\n        else:\n            print(f'Successfully loaded weights from {checkpoint_path}')\n        in_features = self.backbone.num_features\n        self.head = nn.Sequential(\n            nn.LayerNorm(in_features),\n            nn.Dropout(cfg.drop_rate),\n            nn.Linear(in_features, cfg.hidden_dim),\n            nn.GELU(),\n            nn.Dropout(cfg.drop_rate),\n            nn.Linear(cfg.hidden_dim, cfg.num_targets)\n        )\n\n    def forward(self, pixel_values):\n        feats = self.backbone(pixel_values)\n        if feats.ndim == 3:\n            feats = feats.mean(dim=1)\n        return self.head(feats)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T10:43:40.471425Z","iopub.execute_input":"2025-12-29T10:43:40.471961Z","iopub.status.idle":"2025-12-29T10:43:40.479215Z","shell.execute_reply.started":"2025-12-29T10:43:40.471939Z","shell.execute_reply":"2025-12-29T10:43:40.478539Z"}},"outputs":[],"execution_count":34},{"id":"0da93c08-389f-4aa8-8e06-e44c10859cf7","cell_type":"code","source":"class AverageMeter:\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count if self.count != 0 else 0\n\ndef get_dataloaders(train_df, valid_df):\n    train_dataset = BiomassDataset(train_df, mode='train', transform=get_transforms(True))\n    valid_dataset = BiomassDataset(valid_df, mode='valid', transform=get_transforms(False))\n\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=cfg.batch_size,\n        shuffle=True,\n        num_workers=cfg.num_workers,\n        pin_memory=True,\n        drop_last=True,\n    )\n    valid_loader = DataLoader(\n        valid_dataset,\n        batch_size=cfg.batch_size,\n        shuffle=False,\n        num_workers=cfg.num_workers,\n        pin_memory=True,\n        drop_last=False,\n    )\n    return train_loader, valid_loader\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T10:43:44.970604Z","iopub.execute_input":"2025-12-29T10:43:44.971290Z","iopub.status.idle":"2025-12-29T10:43:44.976873Z","shell.execute_reply.started":"2025-12-29T10:43:44.971266Z","shell.execute_reply":"2025-12-29T10:43:44.976210Z"}},"outputs":[],"execution_count":35},{"id":"5917ce57-d4ba-473b-9119-16fabc177d25","cell_type":"code","source":"def train_one_epoch(model, loader, criterion, optimizer, scheduler, scaler, device):\n    model.train()\n    loss_meter = AverageMeter()\n\n    for step, batch in enumerate(loader):\n        images = batch['pixel_values'].to(device, non_blocking=True)\n        targets = batch['targets'].to(device, non_blocking=True)\n        optimizer.zero_grad()\n        with torch.cuda.amp.autocast(enabled=cfg.use_amp):\n            preds = model(images)\n            loss = criterion(preds, targets)\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n        if scheduler is not None:\n            scheduler.step()\n        loss_meter.update(loss.item(), images.size(0))\n    return loss_meter.avg\n\n\ndef validate_one_epoch(model, loader, criterion, device):\n    model.eval()\n    loss_meter = AverageMeter()\n    preds_list = []\n    with torch.no_grad():\n        for batch in loader:\n            images = batch['pixel_values'].to(device, non_blocking=True)\n            targets = batch['targets'].to(device, non_blocking=True)\n            with torch.cuda.amp.autocast(enabled=cfg.use_amp):\n                preds = model(images)\n                loss = criterion(preds, targets)\n            loss_meter.update(loss.item(), images.size(0))\n            preds_list.append(preds.detach().cpu())\n    predictions = torch.cat(preds_list).numpy()\n    return loss_meter.avg, predictions\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T10:43:49.002748Z","iopub.execute_input":"2025-12-29T10:43:49.003283Z","iopub.status.idle":"2025-12-29T10:43:49.009903Z","shell.execute_reply.started":"2025-12-29T10:43:49.003259Z","shell.execute_reply":"2025-12-29T10:43:49.009267Z"}},"outputs":[],"execution_count":36},{"id":"bf6dcc10-66bc-4978-a251-bbb5e432c1d0","cell_type":"code","source":"DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\ncriterion = nn.SmoothL1Loss()\noof_predictions = np.zeros((len(train_targets), cfg.num_targets), dtype=np.float32)\ntest_images = pd.DataFrame({'image_path': test_df['image_path'].unique()})\n\nall_fold_models = []\n\nfor fold in range(cfg.n_folds):\n    if fold not in cfg.train_folds:\n        continue\n    print(f\"===== Fold {fold} =====\")\n    train_split = train_targets[train_targets['fold'] != fold].reset_index(drop=True)\n    valid_split = train_targets[train_targets['fold'] == fold].reset_index(drop=True)\n    train_loader, valid_loader = get_dataloaders(train_split, valid_split)\n\n    model = DinoRegressor(cfg).to(DEVICE)\n    optimizer = AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n    steps_per_epoch = len(train_loader)\n    scheduler = OneCycleLR(\n        optimizer,\n        max_lr=cfg.lr,\n        epochs=cfg.epochs,\n        steps_per_epoch=steps_per_epoch,\n        pct_start=0.1,\n        div_factor=25,\n        final_div_factor=cfg.lr / cfg.min_lr,\n    )\n    scaler = torch.cuda.amp.GradScaler(enabled=cfg.use_amp)\n\n    best_loss = float('inf')\n    best_state = None\n\n    for epoch in range(cfg.epochs):\n        start = time.time()\n        train_loss = train_one_epoch(model, train_loader, criterion, optimizer, scheduler, scaler, DEVICE)\n        val_loss, val_preds = validate_one_epoch(model, valid_loader, criterion, DEVICE)\n        duration = time.time() - start\n        print(f\"Epoch {epoch+1}/{cfg.epochs} | train: {train_loss:.4f} | valid: {val_loss:.4f} | {duration:.1f}s\")\n        if val_loss < best_loss:\n            best_loss = val_loss\n            best_state = model.state_dict()\n    model.load_state_dict(best_state)\n    all_fold_models.append(best_state)\n\n    fold_idx = valid_split.index\n    oof_predictions[fold_idx] = val_preds\n\nnp.save(cfg.output_dir / 'oof_predictions.npy', oof_predictions)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T10:43:52.272618Z","iopub.execute_input":"2025-12-29T10:43:52.272887Z","iopub.status.idle":"2025-12-29T10:46:32.558901Z","shell.execute_reply.started":"2025-12-29T10:43:52.272867Z","shell.execute_reply":"2025-12-29T10:46:32.558159Z"}},"outputs":[{"name":"stdout","text":"===== Fold 0 =====\nLoaded checkpoint with missing keys: ['cls_token', 'pos_embed', 'patch_embed.proj.weight', 'patch_embed.proj.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.attn.qkv.weight', 'blocks.0.attn.qkv.bias', 'blocks.0.attn.proj.weight', 'blocks.0.attn.proj.bias', 'blocks.0.ls1.gamma', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.0.mlp.fc1.weight', 'blocks.0.mlp.fc1.bias', 'blocks.0.mlp.fc2.weight', 'blocks.0.mlp.fc2.bias', 'blocks.0.ls2.gamma', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.attn.qkv.weight', 'blocks.1.attn.qkv.bias', 'blocks.1.attn.proj.weight', 'blocks.1.attn.proj.bias', 'blocks.1.ls1.gamma', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.1.mlp.fc1.weight', 'blocks.1.mlp.fc1.bias', 'blocks.1.mlp.fc2.weight', 'blocks.1.mlp.fc2.bias', 'blocks.1.ls2.gamma', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.attn.qkv.weight', 'blocks.2.attn.qkv.bias', 'blocks.2.attn.proj.weight', 'blocks.2.attn.proj.bias', 'blocks.2.ls1.gamma', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.2.mlp.fc1.weight', 'blocks.2.mlp.fc1.bias', 'blocks.2.mlp.fc2.weight', 'blocks.2.mlp.fc2.bias', 'blocks.2.ls2.gamma', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.attn.qkv.weight', 'blocks.3.attn.qkv.bias', 'blocks.3.attn.proj.weight', 'blocks.3.attn.proj.bias', 'blocks.3.ls1.gamma', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.3.mlp.fc1.weight', 'blocks.3.mlp.fc1.bias', 'blocks.3.mlp.fc2.weight', 'blocks.3.mlp.fc2.bias', 'blocks.3.ls2.gamma', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.attn.qkv.weight', 'blocks.4.attn.qkv.bias', 'blocks.4.attn.proj.weight', 'blocks.4.attn.proj.bias', 'blocks.4.ls1.gamma', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.4.mlp.fc1.weight', 'blocks.4.mlp.fc1.bias', 'blocks.4.mlp.fc2.weight', 'blocks.4.mlp.fc2.bias', 'blocks.4.ls2.gamma', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.attn.qkv.weight', 'blocks.5.attn.qkv.bias', 'blocks.5.attn.proj.weight', 'blocks.5.attn.proj.bias', 'blocks.5.ls1.gamma', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.5.mlp.fc1.weight', 'blocks.5.mlp.fc1.bias', 'blocks.5.mlp.fc2.weight', 'blocks.5.mlp.fc2.bias', 'blocks.5.ls2.gamma', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.attn.qkv.weight', 'blocks.6.attn.qkv.bias', 'blocks.6.attn.proj.weight', 'blocks.6.attn.proj.bias', 'blocks.6.ls1.gamma', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'blocks.6.mlp.fc1.weight', 'blocks.6.mlp.fc1.bias', 'blocks.6.mlp.fc2.weight', 'blocks.6.mlp.fc2.bias', 'blocks.6.ls2.gamma', 'blocks.7.norm1.weight', 'blocks.7.norm1.bias', 'blocks.7.attn.qkv.weight', 'blocks.7.attn.qkv.bias', 'blocks.7.attn.proj.weight', 'blocks.7.attn.proj.bias', 'blocks.7.ls1.gamma', 'blocks.7.norm2.weight', 'blocks.7.norm2.bias', 'blocks.7.mlp.fc1.weight', 'blocks.7.mlp.fc1.bias', 'blocks.7.mlp.fc2.weight', 'blocks.7.mlp.fc2.bias', 'blocks.7.ls2.gamma', 'blocks.8.norm1.weight', 'blocks.8.norm1.bias', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.qkv.bias', 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.8.ls1.gamma', 'blocks.8.norm2.weight', 'blocks.8.norm2.bias', 'blocks.8.mlp.fc1.weight', 'blocks.8.mlp.fc1.bias', 'blocks.8.mlp.fc2.weight', 'blocks.8.mlp.fc2.bias', 'blocks.8.ls2.gamma', 'blocks.9.norm1.weight', 'blocks.9.norm1.bias', 'blocks.9.attn.qkv.weight', 'blocks.9.attn.qkv.bias', 'blocks.9.attn.proj.weight', 'blocks.9.attn.proj.bias', 'blocks.9.ls1.gamma', 'blocks.9.norm2.weight', 'blocks.9.norm2.bias', 'blocks.9.mlp.fc1.weight', 'blocks.9.mlp.fc1.bias', 'blocks.9.mlp.fc2.weight', 'blocks.9.mlp.fc2.bias', 'blocks.9.ls2.gamma', 'blocks.10.norm1.weight', 'blocks.10.norm1.bias', 'blocks.10.attn.qkv.weight', 'blocks.10.attn.qkv.bias', 'blocks.10.attn.proj.weight', 'blocks.10.attn.proj.bias', 'blocks.10.ls1.gamma', 'blocks.10.norm2.weight', 'blocks.10.norm2.bias', 'blocks.10.mlp.fc1.weight', 'blocks.10.mlp.fc1.bias', 'blocks.10.mlp.fc2.weight', 'blocks.10.mlp.fc2.bias', 'blocks.10.ls2.gamma', 'blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.ls1.gamma', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.11.mlp.fc1.weight', 'blocks.11.mlp.fc1.bias', 'blocks.11.mlp.fc2.weight', 'blocks.11.mlp.fc2.bias', 'blocks.11.ls2.gamma', 'norm.weight', 'norm.bias']\nUnexpected keys: ['embeddings.cls_token', 'embeddings.mask_token', 'embeddings.position_embeddings', 'embeddings.patch_embeddings.projection.weight', 'embeddings.patch_embeddings.projection.bias', 'encoder.layer.0.norm1.weight', 'encoder.layer.0.norm1.bias', 'encoder.layer.0.attention.attention.query.weight', 'encoder.layer.0.attention.attention.query.bias', 'encoder.layer.0.attention.attention.key.weight', 'encoder.layer.0.attention.attention.key.bias', 'encoder.layer.0.attention.attention.value.weight', 'encoder.layer.0.attention.attention.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.layer_scale1.lambda1', 'encoder.layer.0.norm2.weight', 'encoder.layer.0.norm2.bias', 'encoder.layer.0.mlp.weights_in.weight', 'encoder.layer.0.mlp.weights_in.bias', 'encoder.layer.0.mlp.weights_out.weight', 'encoder.layer.0.mlp.weights_out.bias', 'encoder.layer.0.layer_scale2.lambda1', 'encoder.layer.1.norm1.weight', 'encoder.layer.1.norm1.bias', 'encoder.layer.1.attention.attention.query.weight', 'encoder.layer.1.attention.attention.query.bias', 'encoder.layer.1.attention.attention.key.weight', 'encoder.layer.1.attention.attention.key.bias', 'encoder.layer.1.attention.attention.value.weight', 'encoder.layer.1.attention.attention.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.layer_scale1.lambda1', 'encoder.layer.1.norm2.weight', 'encoder.layer.1.norm2.bias', 'encoder.layer.1.mlp.weights_in.weight', 'encoder.layer.1.mlp.weights_in.bias', 'encoder.layer.1.mlp.weights_out.weight', 'encoder.layer.1.mlp.weights_out.bias', 'encoder.layer.1.layer_scale2.lambda1', 'encoder.layer.2.norm1.weight', 'encoder.layer.2.norm1.bias', 'encoder.layer.2.attention.attention.query.weight', 'encoder.layer.2.attention.attention.query.bias', 'encoder.layer.2.attention.attention.key.weight', 'encoder.layer.2.attention.attention.key.bias', 'encoder.layer.2.attention.attention.value.weight', 'encoder.layer.2.attention.attention.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.layer_scale1.lambda1', 'encoder.layer.2.norm2.weight', 'encoder.layer.2.norm2.bias', 'encoder.layer.2.mlp.weights_in.weight', 'encoder.layer.2.mlp.weights_in.bias', 'encoder.layer.2.mlp.weights_out.weight', 'encoder.layer.2.mlp.weights_out.bias', 'encoder.layer.2.layer_scale2.lambda1', 'encoder.layer.3.norm1.weight', 'encoder.layer.3.norm1.bias', 'encoder.layer.3.attention.attention.query.weight', 'encoder.layer.3.attention.attention.query.bias', 'encoder.layer.3.attention.attention.key.weight', 'encoder.layer.3.attention.attention.key.bias', 'encoder.layer.3.attention.attention.value.weight', 'encoder.layer.3.attention.attention.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.layer_scale1.lambda1', 'encoder.layer.3.norm2.weight', 'encoder.layer.3.norm2.bias', 'encoder.layer.3.mlp.weights_in.weight', 'encoder.layer.3.mlp.weights_in.bias', 'encoder.layer.3.mlp.weights_out.weight', 'encoder.layer.3.mlp.weights_out.bias', 'encoder.layer.3.layer_scale2.lambda1', 'encoder.layer.4.norm1.weight', 'encoder.layer.4.norm1.bias', 'encoder.layer.4.attention.attention.query.weight', 'encoder.layer.4.attention.attention.query.bias', 'encoder.layer.4.attention.attention.key.weight', 'encoder.layer.4.attention.attention.key.bias', 'encoder.layer.4.attention.attention.value.weight', 'encoder.layer.4.attention.attention.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.layer_scale1.lambda1', 'encoder.layer.4.norm2.weight', 'encoder.layer.4.norm2.bias', 'encoder.layer.4.mlp.weights_in.weight', 'encoder.layer.4.mlp.weights_in.bias', 'encoder.layer.4.mlp.weights_out.weight', 'encoder.layer.4.mlp.weights_out.bias', 'encoder.layer.4.layer_scale2.lambda1', 'encoder.layer.5.norm1.weight', 'encoder.layer.5.norm1.bias', 'encoder.layer.5.attention.attention.query.weight', 'encoder.layer.5.attention.attention.query.bias', 'encoder.layer.5.attention.attention.key.weight', 'encoder.layer.5.attention.attention.key.bias', 'encoder.layer.5.attention.attention.value.weight', 'encoder.layer.5.attention.attention.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.layer_scale1.lambda1', 'encoder.layer.5.norm2.weight', 'encoder.layer.5.norm2.bias', 'encoder.layer.5.mlp.weights_in.weight', 'encoder.layer.5.mlp.weights_in.bias', 'encoder.layer.5.mlp.weights_out.weight', 'encoder.layer.5.mlp.weights_out.bias', 'encoder.layer.5.layer_scale2.lambda1', 'encoder.layer.6.norm1.weight', 'encoder.layer.6.norm1.bias', 'encoder.layer.6.attention.attention.query.weight', 'encoder.layer.6.attention.attention.query.bias', 'encoder.layer.6.attention.attention.key.weight', 'encoder.layer.6.attention.attention.key.bias', 'encoder.layer.6.attention.attention.value.weight', 'encoder.layer.6.attention.attention.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.layer_scale1.lambda1', 'encoder.layer.6.norm2.weight', 'encoder.layer.6.norm2.bias', 'encoder.layer.6.mlp.weights_in.weight', 'encoder.layer.6.mlp.weights_in.bias', 'encoder.layer.6.mlp.weights_out.weight', 'encoder.layer.6.mlp.weights_out.bias', 'encoder.layer.6.layer_scale2.lambda1', 'encoder.layer.7.norm1.weight', 'encoder.layer.7.norm1.bias', 'encoder.layer.7.attention.attention.query.weight', 'encoder.layer.7.attention.attention.query.bias', 'encoder.layer.7.attention.attention.key.weight', 'encoder.layer.7.attention.attention.key.bias', 'encoder.layer.7.attention.attention.value.weight', 'encoder.layer.7.attention.attention.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.layer_scale1.lambda1', 'encoder.layer.7.norm2.weight', 'encoder.layer.7.norm2.bias', 'encoder.layer.7.mlp.weights_in.weight', 'encoder.layer.7.mlp.weights_in.bias', 'encoder.layer.7.mlp.weights_out.weight', 'encoder.layer.7.mlp.weights_out.bias', 'encoder.layer.7.layer_scale2.lambda1', 'encoder.layer.8.norm1.weight', 'encoder.layer.8.norm1.bias', 'encoder.layer.8.attention.attention.query.weight', 'encoder.layer.8.attention.attention.query.bias', 'encoder.layer.8.attention.attention.key.weight', 'encoder.layer.8.attention.attention.key.bias', 'encoder.layer.8.attention.attention.value.weight', 'encoder.layer.8.attention.attention.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.layer_scale1.lambda1', 'encoder.layer.8.norm2.weight', 'encoder.layer.8.norm2.bias', 'encoder.layer.8.mlp.weights_in.weight', 'encoder.layer.8.mlp.weights_in.bias', 'encoder.layer.8.mlp.weights_out.weight', 'encoder.layer.8.mlp.weights_out.bias', 'encoder.layer.8.layer_scale2.lambda1', 'encoder.layer.9.norm1.weight', 'encoder.layer.9.norm1.bias', 'encoder.layer.9.attention.attention.query.weight', 'encoder.layer.9.attention.attention.query.bias', 'encoder.layer.9.attention.attention.key.weight', 'encoder.layer.9.attention.attention.key.bias', 'encoder.layer.9.attention.attention.value.weight', 'encoder.layer.9.attention.attention.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.layer_scale1.lambda1', 'encoder.layer.9.norm2.weight', 'encoder.layer.9.norm2.bias', 'encoder.layer.9.mlp.weights_in.weight', 'encoder.layer.9.mlp.weights_in.bias', 'encoder.layer.9.mlp.weights_out.weight', 'encoder.layer.9.mlp.weights_out.bias', 'encoder.layer.9.layer_scale2.lambda1', 'encoder.layer.10.norm1.weight', 'encoder.layer.10.norm1.bias', 'encoder.layer.10.attention.attention.query.weight', 'encoder.layer.10.attention.attention.query.bias', 'encoder.layer.10.attention.attention.key.weight', 'encoder.layer.10.attention.attention.key.bias', 'encoder.layer.10.attention.attention.value.weight', 'encoder.layer.10.attention.attention.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.layer_scale1.lambda1', 'encoder.layer.10.norm2.weight', 'encoder.layer.10.norm2.bias', 'encoder.layer.10.mlp.weights_in.weight', 'encoder.layer.10.mlp.weights_in.bias', 'encoder.layer.10.mlp.weights_out.weight', 'encoder.layer.10.mlp.weights_out.bias', 'encoder.layer.10.layer_scale2.lambda1', 'encoder.layer.11.norm1.weight', 'encoder.layer.11.norm1.bias', 'encoder.layer.11.attention.attention.query.weight', 'encoder.layer.11.attention.attention.query.bias', 'encoder.layer.11.attention.attention.key.weight', 'encoder.layer.11.attention.attention.key.bias', 'encoder.layer.11.attention.attention.value.weight', 'encoder.layer.11.attention.attention.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.layer_scale1.lambda1', 'encoder.layer.11.norm2.weight', 'encoder.layer.11.norm2.bias', 'encoder.layer.11.mlp.weights_in.weight', 'encoder.layer.11.mlp.weights_in.bias', 'encoder.layer.11.mlp.weights_out.weight', 'encoder.layer.11.mlp.weights_out.bias', 'encoder.layer.11.layer_scale2.lambda1', 'encoder.layer.12.norm1.weight', 'encoder.layer.12.norm1.bias', 'encoder.layer.12.attention.attention.query.weight', 'encoder.layer.12.attention.attention.query.bias', 'encoder.layer.12.attention.attention.key.weight', 'encoder.layer.12.attention.attention.key.bias', 'encoder.layer.12.attention.attention.value.weight', 'encoder.layer.12.attention.attention.value.bias', 'encoder.layer.12.attention.output.dense.weight', 'encoder.layer.12.attention.output.dense.bias', 'encoder.layer.12.layer_scale1.lambda1', 'encoder.layer.12.norm2.weight', 'encoder.layer.12.norm2.bias', 'encoder.layer.12.mlp.weights_in.weight', 'encoder.layer.12.mlp.weights_in.bias', 'encoder.layer.12.mlp.weights_out.weight', 'encoder.layer.12.mlp.weights_out.bias', 'encoder.layer.12.layer_scale2.lambda1', 'encoder.layer.13.norm1.weight', 'encoder.layer.13.norm1.bias', 'encoder.layer.13.attention.attention.query.weight', 'encoder.layer.13.attention.attention.query.bias', 'encoder.layer.13.attention.attention.key.weight', 'encoder.layer.13.attention.attention.key.bias', 'encoder.layer.13.attention.attention.value.weight', 'encoder.layer.13.attention.attention.value.bias', 'encoder.layer.13.attention.output.dense.weight', 'encoder.layer.13.attention.output.dense.bias', 'encoder.layer.13.layer_scale1.lambda1', 'encoder.layer.13.norm2.weight', 'encoder.layer.13.norm2.bias', 'encoder.layer.13.mlp.weights_in.weight', 'encoder.layer.13.mlp.weights_in.bias', 'encoder.layer.13.mlp.weights_out.weight', 'encoder.layer.13.mlp.weights_out.bias', 'encoder.layer.13.layer_scale2.lambda1', 'encoder.layer.14.norm1.weight', 'encoder.layer.14.norm1.bias', 'encoder.layer.14.attention.attention.query.weight', 'encoder.layer.14.attention.attention.query.bias', 'encoder.layer.14.attention.attention.key.weight', 'encoder.layer.14.attention.attention.key.bias', 'encoder.layer.14.attention.attention.value.weight', 'encoder.layer.14.attention.attention.value.bias', 'encoder.layer.14.attention.output.dense.weight', 'encoder.layer.14.attention.output.dense.bias', 'encoder.layer.14.layer_scale1.lambda1', 'encoder.layer.14.norm2.weight', 'encoder.layer.14.norm2.bias', 'encoder.layer.14.mlp.weights_in.weight', 'encoder.layer.14.mlp.weights_in.bias', 'encoder.layer.14.mlp.weights_out.weight', 'encoder.layer.14.mlp.weights_out.bias', 'encoder.layer.14.layer_scale2.lambda1', 'encoder.layer.15.norm1.weight', 'encoder.layer.15.norm1.bias', 'encoder.layer.15.attention.attention.query.weight', 'encoder.layer.15.attention.attention.query.bias', 'encoder.layer.15.attention.attention.key.weight', 'encoder.layer.15.attention.attention.key.bias', 'encoder.layer.15.attention.attention.value.weight', 'encoder.layer.15.attention.attention.value.bias', 'encoder.layer.15.attention.output.dense.weight', 'encoder.layer.15.attention.output.dense.bias', 'encoder.layer.15.layer_scale1.lambda1', 'encoder.layer.15.norm2.weight', 'encoder.layer.15.norm2.bias', 'encoder.layer.15.mlp.weights_in.weight', 'encoder.layer.15.mlp.weights_in.bias', 'encoder.layer.15.mlp.weights_out.weight', 'encoder.layer.15.mlp.weights_out.bias', 'encoder.layer.15.layer_scale2.lambda1', 'encoder.layer.16.norm1.weight', 'encoder.layer.16.norm1.bias', 'encoder.layer.16.attention.attention.query.weight', 'encoder.layer.16.attention.attention.query.bias', 'encoder.layer.16.attention.attention.key.weight', 'encoder.layer.16.attention.attention.key.bias', 'encoder.layer.16.attention.attention.value.weight', 'encoder.layer.16.attention.attention.value.bias', 'encoder.layer.16.attention.output.dense.weight', 'encoder.layer.16.attention.output.dense.bias', 'encoder.layer.16.layer_scale1.lambda1', 'encoder.layer.16.norm2.weight', 'encoder.layer.16.norm2.bias', 'encoder.layer.16.mlp.weights_in.weight', 'encoder.layer.16.mlp.weights_in.bias', 'encoder.layer.16.mlp.weights_out.weight', 'encoder.layer.16.mlp.weights_out.bias', 'encoder.layer.16.layer_scale2.lambda1', 'encoder.layer.17.norm1.weight', 'encoder.layer.17.norm1.bias', 'encoder.layer.17.attention.attention.query.weight', 'encoder.layer.17.attention.attention.query.bias', 'encoder.layer.17.attention.attention.key.weight', 'encoder.layer.17.attention.attention.key.bias', 'encoder.layer.17.attention.attention.value.weight', 'encoder.layer.17.attention.attention.value.bias', 'encoder.layer.17.attention.output.dense.weight', 'encoder.layer.17.attention.output.dense.bias', 'encoder.layer.17.layer_scale1.lambda1', 'encoder.layer.17.norm2.weight', 'encoder.layer.17.norm2.bias', 'encoder.layer.17.mlp.weights_in.weight', 'encoder.layer.17.mlp.weights_in.bias', 'encoder.layer.17.mlp.weights_out.weight', 'encoder.layer.17.mlp.weights_out.bias', 'encoder.layer.17.layer_scale2.lambda1', 'encoder.layer.18.norm1.weight', 'encoder.layer.18.norm1.bias', 'encoder.layer.18.attention.attention.query.weight', 'encoder.layer.18.attention.attention.query.bias', 'encoder.layer.18.attention.attention.key.weight', 'encoder.layer.18.attention.attention.key.bias', 'encoder.layer.18.attention.attention.value.weight', 'encoder.layer.18.attention.attention.value.bias', 'encoder.layer.18.attention.output.dense.weight', 'encoder.layer.18.attention.output.dense.bias', 'encoder.layer.18.layer_scale1.lambda1', 'encoder.layer.18.norm2.weight', 'encoder.layer.18.norm2.bias', 'encoder.layer.18.mlp.weights_in.weight', 'encoder.layer.18.mlp.weights_in.bias', 'encoder.layer.18.mlp.weights_out.weight', 'encoder.layer.18.mlp.weights_out.bias', 'encoder.layer.18.layer_scale2.lambda1', 'encoder.layer.19.norm1.weight', 'encoder.layer.19.norm1.bias', 'encoder.layer.19.attention.attention.query.weight', 'encoder.layer.19.attention.attention.query.bias', 'encoder.layer.19.attention.attention.key.weight', 'encoder.layer.19.attention.attention.key.bias', 'encoder.layer.19.attention.attention.value.weight', 'encoder.layer.19.attention.attention.value.bias', 'encoder.layer.19.attention.output.dense.weight', 'encoder.layer.19.attention.output.dense.bias', 'encoder.layer.19.layer_scale1.lambda1', 'encoder.layer.19.norm2.weight', 'encoder.layer.19.norm2.bias', 'encoder.layer.19.mlp.weights_in.weight', 'encoder.layer.19.mlp.weights_in.bias', 'encoder.layer.19.mlp.weights_out.weight', 'encoder.layer.19.mlp.weights_out.bias', 'encoder.layer.19.layer_scale2.lambda1', 'encoder.layer.20.norm1.weight', 'encoder.layer.20.norm1.bias', 'encoder.layer.20.attention.attention.query.weight', 'encoder.layer.20.attention.attention.query.bias', 'encoder.layer.20.attention.attention.key.weight', 'encoder.layer.20.attention.attention.key.bias', 'encoder.layer.20.attention.attention.value.weight', 'encoder.layer.20.attention.attention.value.bias', 'encoder.layer.20.attention.output.dense.weight', 'encoder.layer.20.attention.output.dense.bias', 'encoder.layer.20.layer_scale1.lambda1', 'encoder.layer.20.norm2.weight', 'encoder.layer.20.norm2.bias', 'encoder.layer.20.mlp.weights_in.weight', 'encoder.layer.20.mlp.weights_in.bias', 'encoder.layer.20.mlp.weights_out.weight', 'encoder.layer.20.mlp.weights_out.bias', 'encoder.layer.20.layer_scale2.lambda1', 'encoder.layer.21.norm1.weight', 'encoder.layer.21.norm1.bias', 'encoder.layer.21.attention.attention.query.weight', 'encoder.layer.21.attention.attention.query.bias', 'encoder.layer.21.attention.attention.key.weight', 'encoder.layer.21.attention.attention.key.bias', 'encoder.layer.21.attention.attention.value.weight', 'encoder.layer.21.attention.attention.value.bias', 'encoder.layer.21.attention.output.dense.weight', 'encoder.layer.21.attention.output.dense.bias', 'encoder.layer.21.layer_scale1.lambda1', 'encoder.layer.21.norm2.weight', 'encoder.layer.21.norm2.bias', 'encoder.layer.21.mlp.weights_in.weight', 'encoder.layer.21.mlp.weights_in.bias', 'encoder.layer.21.mlp.weights_out.weight', 'encoder.layer.21.mlp.weights_out.bias', 'encoder.layer.21.layer_scale2.lambda1', 'encoder.layer.22.norm1.weight', 'encoder.layer.22.norm1.bias', 'encoder.layer.22.attention.attention.query.weight', 'encoder.layer.22.attention.attention.query.bias', 'encoder.layer.22.attention.attention.key.weight', 'encoder.layer.22.attention.attention.key.bias', 'encoder.layer.22.attention.attention.value.weight', 'encoder.layer.22.attention.attention.value.bias', 'encoder.layer.22.attention.output.dense.weight', 'encoder.layer.22.attention.output.dense.bias', 'encoder.layer.22.layer_scale1.lambda1', 'encoder.layer.22.norm2.weight', 'encoder.layer.22.norm2.bias', 'encoder.layer.22.mlp.weights_in.weight', 'encoder.layer.22.mlp.weights_in.bias', 'encoder.layer.22.mlp.weights_out.weight', 'encoder.layer.22.mlp.weights_out.bias', 'encoder.layer.22.layer_scale2.lambda1', 'encoder.layer.23.norm1.weight', 'encoder.layer.23.norm1.bias', 'encoder.layer.23.attention.attention.query.weight', 'encoder.layer.23.attention.attention.query.bias', 'encoder.layer.23.attention.attention.key.weight', 'encoder.layer.23.attention.attention.key.bias', 'encoder.layer.23.attention.attention.value.weight', 'encoder.layer.23.attention.attention.value.bias', 'encoder.layer.23.attention.output.dense.weight', 'encoder.layer.23.attention.output.dense.bias', 'encoder.layer.23.layer_scale1.lambda1', 'encoder.layer.23.norm2.weight', 'encoder.layer.23.norm2.bias', 'encoder.layer.23.mlp.weights_in.weight', 'encoder.layer.23.mlp.weights_in.bias', 'encoder.layer.23.mlp.weights_out.weight', 'encoder.layer.23.mlp.weights_out.bias', 'encoder.layer.23.layer_scale2.lambda1', 'encoder.layer.24.norm1.weight', 'encoder.layer.24.norm1.bias', 'encoder.layer.24.attention.attention.query.weight', 'encoder.layer.24.attention.attention.query.bias', 'encoder.layer.24.attention.attention.key.weight', 'encoder.layer.24.attention.attention.key.bias', 'encoder.layer.24.attention.attention.value.weight', 'encoder.layer.24.attention.attention.value.bias', 'encoder.layer.24.attention.output.dense.weight', 'encoder.layer.24.attention.output.dense.bias', 'encoder.layer.24.layer_scale1.lambda1', 'encoder.layer.24.norm2.weight', 'encoder.layer.24.norm2.bias', 'encoder.layer.24.mlp.weights_in.weight', 'encoder.layer.24.mlp.weights_in.bias', 'encoder.layer.24.mlp.weights_out.weight', 'encoder.layer.24.mlp.weights_out.bias', 'encoder.layer.24.layer_scale2.lambda1', 'encoder.layer.25.norm1.weight', 'encoder.layer.25.norm1.bias', 'encoder.layer.25.attention.attention.query.weight', 'encoder.layer.25.attention.attention.query.bias', 'encoder.layer.25.attention.attention.key.weight', 'encoder.layer.25.attention.attention.key.bias', 'encoder.layer.25.attention.attention.value.weight', 'encoder.layer.25.attention.attention.value.bias', 'encoder.layer.25.attention.output.dense.weight', 'encoder.layer.25.attention.output.dense.bias', 'encoder.layer.25.layer_scale1.lambda1', 'encoder.layer.25.norm2.weight', 'encoder.layer.25.norm2.bias', 'encoder.layer.25.mlp.weights_in.weight', 'encoder.layer.25.mlp.weights_in.bias', 'encoder.layer.25.mlp.weights_out.weight', 'encoder.layer.25.mlp.weights_out.bias', 'encoder.layer.25.layer_scale2.lambda1', 'encoder.layer.26.norm1.weight', 'encoder.layer.26.norm1.bias', 'encoder.layer.26.attention.attention.query.weight', 'encoder.layer.26.attention.attention.query.bias', 'encoder.layer.26.attention.attention.key.weight', 'encoder.layer.26.attention.attention.key.bias', 'encoder.layer.26.attention.attention.value.weight', 'encoder.layer.26.attention.attention.value.bias', 'encoder.layer.26.attention.output.dense.weight', 'encoder.layer.26.attention.output.dense.bias', 'encoder.layer.26.layer_scale1.lambda1', 'encoder.layer.26.norm2.weight', 'encoder.layer.26.norm2.bias', 'encoder.layer.26.mlp.weights_in.weight', 'encoder.layer.26.mlp.weights_in.bias', 'encoder.layer.26.mlp.weights_out.weight', 'encoder.layer.26.mlp.weights_out.bias', 'encoder.layer.26.layer_scale2.lambda1', 'encoder.layer.27.norm1.weight', 'encoder.layer.27.norm1.bias', 'encoder.layer.27.attention.attention.query.weight', 'encoder.layer.27.attention.attention.query.bias', 'encoder.layer.27.attention.attention.key.weight', 'encoder.layer.27.attention.attention.key.bias', 'encoder.layer.27.attention.attention.value.weight', 'encoder.layer.27.attention.attention.value.bias', 'encoder.layer.27.attention.output.dense.weight', 'encoder.layer.27.attention.output.dense.bias', 'encoder.layer.27.layer_scale1.lambda1', 'encoder.layer.27.norm2.weight', 'encoder.layer.27.norm2.bias', 'encoder.layer.27.mlp.weights_in.weight', 'encoder.layer.27.mlp.weights_in.bias', 'encoder.layer.27.mlp.weights_out.weight', 'encoder.layer.27.mlp.weights_out.bias', 'encoder.layer.27.layer_scale2.lambda1', 'encoder.layer.28.norm1.weight', 'encoder.layer.28.norm1.bias', 'encoder.layer.28.attention.attention.query.weight', 'encoder.layer.28.attention.attention.query.bias', 'encoder.layer.28.attention.attention.key.weight', 'encoder.layer.28.attention.attention.key.bias', 'encoder.layer.28.attention.attention.value.weight', 'encoder.layer.28.attention.attention.value.bias', 'encoder.layer.28.attention.output.dense.weight', 'encoder.layer.28.attention.output.dense.bias', 'encoder.layer.28.layer_scale1.lambda1', 'encoder.layer.28.norm2.weight', 'encoder.layer.28.norm2.bias', 'encoder.layer.28.mlp.weights_in.weight', 'encoder.layer.28.mlp.weights_in.bias', 'encoder.layer.28.mlp.weights_out.weight', 'encoder.layer.28.mlp.weights_out.bias', 'encoder.layer.28.layer_scale2.lambda1', 'encoder.layer.29.norm1.weight', 'encoder.layer.29.norm1.bias', 'encoder.layer.29.attention.attention.query.weight', 'encoder.layer.29.attention.attention.query.bias', 'encoder.layer.29.attention.attention.key.weight', 'encoder.layer.29.attention.attention.key.bias', 'encoder.layer.29.attention.attention.value.weight', 'encoder.layer.29.attention.attention.value.bias', 'encoder.layer.29.attention.output.dense.weight', 'encoder.layer.29.attention.output.dense.bias', 'encoder.layer.29.layer_scale1.lambda1', 'encoder.layer.29.norm2.weight', 'encoder.layer.29.norm2.bias', 'encoder.layer.29.mlp.weights_in.weight', 'encoder.layer.29.mlp.weights_in.bias', 'encoder.layer.29.mlp.weights_out.weight', 'encoder.layer.29.mlp.weights_out.bias', 'encoder.layer.29.layer_scale2.lambda1', 'encoder.layer.30.norm1.weight', 'encoder.layer.30.norm1.bias', 'encoder.layer.30.attention.attention.query.weight', 'encoder.layer.30.attention.attention.query.bias', 'encoder.layer.30.attention.attention.key.weight', 'encoder.layer.30.attention.attention.key.bias', 'encoder.layer.30.attention.attention.value.weight', 'encoder.layer.30.attention.attention.value.bias', 'encoder.layer.30.attention.output.dense.weight', 'encoder.layer.30.attention.output.dense.bias', 'encoder.layer.30.layer_scale1.lambda1', 'encoder.layer.30.norm2.weight', 'encoder.layer.30.norm2.bias', 'encoder.layer.30.mlp.weights_in.weight', 'encoder.layer.30.mlp.weights_in.bias', 'encoder.layer.30.mlp.weights_out.weight', 'encoder.layer.30.mlp.weights_out.bias', 'encoder.layer.30.layer_scale2.lambda1', 'encoder.layer.31.norm1.weight', 'encoder.layer.31.norm1.bias', 'encoder.layer.31.attention.attention.query.weight', 'encoder.layer.31.attention.attention.query.bias', 'encoder.layer.31.attention.attention.key.weight', 'encoder.layer.31.attention.attention.key.bias', 'encoder.layer.31.attention.attention.value.weight', 'encoder.layer.31.attention.attention.value.bias', 'encoder.layer.31.attention.output.dense.weight', 'encoder.layer.31.attention.output.dense.bias', 'encoder.layer.31.layer_scale1.lambda1', 'encoder.layer.31.norm2.weight', 'encoder.layer.31.norm2.bias', 'encoder.layer.31.mlp.weights_in.weight', 'encoder.layer.31.mlp.weights_in.bias', 'encoder.layer.31.mlp.weights_out.weight', 'encoder.layer.31.mlp.weights_out.bias', 'encoder.layer.31.layer_scale2.lambda1', 'encoder.layer.32.norm1.weight', 'encoder.layer.32.norm1.bias', 'encoder.layer.32.attention.attention.query.weight', 'encoder.layer.32.attention.attention.query.bias', 'encoder.layer.32.attention.attention.key.weight', 'encoder.layer.32.attention.attention.key.bias', 'encoder.layer.32.attention.attention.value.weight', 'encoder.layer.32.attention.attention.value.bias', 'encoder.layer.32.attention.output.dense.weight', 'encoder.layer.32.attention.output.dense.bias', 'encoder.layer.32.layer_scale1.lambda1', 'encoder.layer.32.norm2.weight', 'encoder.layer.32.norm2.bias', 'encoder.layer.32.mlp.weights_in.weight', 'encoder.layer.32.mlp.weights_in.bias', 'encoder.layer.32.mlp.weights_out.weight', 'encoder.layer.32.mlp.weights_out.bias', 'encoder.layer.32.layer_scale2.lambda1', 'encoder.layer.33.norm1.weight', 'encoder.layer.33.norm1.bias', 'encoder.layer.33.attention.attention.query.weight', 'encoder.layer.33.attention.attention.query.bias', 'encoder.layer.33.attention.attention.key.weight', 'encoder.layer.33.attention.attention.key.bias', 'encoder.layer.33.attention.attention.value.weight', 'encoder.layer.33.attention.attention.value.bias', 'encoder.layer.33.attention.output.dense.weight', 'encoder.layer.33.attention.output.dense.bias', 'encoder.layer.33.layer_scale1.lambda1', 'encoder.layer.33.norm2.weight', 'encoder.layer.33.norm2.bias', 'encoder.layer.33.mlp.weights_in.weight', 'encoder.layer.33.mlp.weights_in.bias', 'encoder.layer.33.mlp.weights_out.weight', 'encoder.layer.33.mlp.weights_out.bias', 'encoder.layer.33.layer_scale2.lambda1', 'encoder.layer.34.norm1.weight', 'encoder.layer.34.norm1.bias', 'encoder.layer.34.attention.attention.query.weight', 'encoder.layer.34.attention.attention.query.bias', 'encoder.layer.34.attention.attention.key.weight', 'encoder.layer.34.attention.attention.key.bias', 'encoder.layer.34.attention.attention.value.weight', 'encoder.layer.34.attention.attention.value.bias', 'encoder.layer.34.attention.output.dense.weight', 'encoder.layer.34.attention.output.dense.bias', 'encoder.layer.34.layer_scale1.lambda1', 'encoder.layer.34.norm2.weight', 'encoder.layer.34.norm2.bias', 'encoder.layer.34.mlp.weights_in.weight', 'encoder.layer.34.mlp.weights_in.bias', 'encoder.layer.34.mlp.weights_out.weight', 'encoder.layer.34.mlp.weights_out.bias', 'encoder.layer.34.layer_scale2.lambda1', 'encoder.layer.35.norm1.weight', 'encoder.layer.35.norm1.bias', 'encoder.layer.35.attention.attention.query.weight', 'encoder.layer.35.attention.attention.query.bias', 'encoder.layer.35.attention.attention.key.weight', 'encoder.layer.35.attention.attention.key.bias', 'encoder.layer.35.attention.attention.value.weight', 'encoder.layer.35.attention.attention.value.bias', 'encoder.layer.35.attention.output.dense.weight', 'encoder.layer.35.attention.output.dense.bias', 'encoder.layer.35.layer_scale1.lambda1', 'encoder.layer.35.norm2.weight', 'encoder.layer.35.norm2.bias', 'encoder.layer.35.mlp.weights_in.weight', 'encoder.layer.35.mlp.weights_in.bias', 'encoder.layer.35.mlp.weights_out.weight', 'encoder.layer.35.mlp.weights_out.bias', 'encoder.layer.35.layer_scale2.lambda1', 'encoder.layer.36.norm1.weight', 'encoder.layer.36.norm1.bias', 'encoder.layer.36.attention.attention.query.weight', 'encoder.layer.36.attention.attention.query.bias', 'encoder.layer.36.attention.attention.key.weight', 'encoder.layer.36.attention.attention.key.bias', 'encoder.layer.36.attention.attention.value.weight', 'encoder.layer.36.attention.attention.value.bias', 'encoder.layer.36.attention.output.dense.weight', 'encoder.layer.36.attention.output.dense.bias', 'encoder.layer.36.layer_scale1.lambda1', 'encoder.layer.36.norm2.weight', 'encoder.layer.36.norm2.bias', 'encoder.layer.36.mlp.weights_in.weight', 'encoder.layer.36.mlp.weights_in.bias', 'encoder.layer.36.mlp.weights_out.weight', 'encoder.layer.36.mlp.weights_out.bias', 'encoder.layer.36.layer_scale2.lambda1', 'encoder.layer.37.norm1.weight', 'encoder.layer.37.norm1.bias', 'encoder.layer.37.attention.attention.query.weight', 'encoder.layer.37.attention.attention.query.bias', 'encoder.layer.37.attention.attention.key.weight', 'encoder.layer.37.attention.attention.key.bias', 'encoder.layer.37.attention.attention.value.weight', 'encoder.layer.37.attention.attention.value.bias', 'encoder.layer.37.attention.output.dense.weight', 'encoder.layer.37.attention.output.dense.bias', 'encoder.layer.37.layer_scale1.lambda1', 'encoder.layer.37.norm2.weight', 'encoder.layer.37.norm2.bias', 'encoder.layer.37.mlp.weights_in.weight', 'encoder.layer.37.mlp.weights_in.bias', 'encoder.layer.37.mlp.weights_out.weight', 'encoder.layer.37.mlp.weights_out.bias', 'encoder.layer.37.layer_scale2.lambda1', 'encoder.layer.38.norm1.weight', 'encoder.layer.38.norm1.bias', 'encoder.layer.38.attention.attention.query.weight', 'encoder.layer.38.attention.attention.query.bias', 'encoder.layer.38.attention.attention.key.weight', 'encoder.layer.38.attention.attention.key.bias', 'encoder.layer.38.attention.attention.value.weight', 'encoder.layer.38.attention.attention.value.bias', 'encoder.layer.38.attention.output.dense.weight', 'encoder.layer.38.attention.output.dense.bias', 'encoder.layer.38.layer_scale1.lambda1', 'encoder.layer.38.norm2.weight', 'encoder.layer.38.norm2.bias', 'encoder.layer.38.mlp.weights_in.weight', 'encoder.layer.38.mlp.weights_in.bias', 'encoder.layer.38.mlp.weights_out.weight', 'encoder.layer.38.mlp.weights_out.bias', 'encoder.layer.38.layer_scale2.lambda1', 'encoder.layer.39.norm1.weight', 'encoder.layer.39.norm1.bias', 'encoder.layer.39.attention.attention.query.weight', 'encoder.layer.39.attention.attention.query.bias', 'encoder.layer.39.attention.attention.key.weight', 'encoder.layer.39.attention.attention.key.bias', 'encoder.layer.39.attention.attention.value.weight', 'encoder.layer.39.attention.attention.value.bias', 'encoder.layer.39.attention.output.dense.weight', 'encoder.layer.39.attention.output.dense.bias', 'encoder.layer.39.layer_scale1.lambda1', 'encoder.layer.39.norm2.weight', 'encoder.layer.39.norm2.bias', 'encoder.layer.39.mlp.weights_in.weight', 'encoder.layer.39.mlp.weights_in.bias', 'encoder.layer.39.mlp.weights_out.weight', 'encoder.layer.39.mlp.weights_out.bias', 'encoder.layer.39.layer_scale2.lambda1', 'layernorm.weight', 'layernorm.bias']\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_47/532262637.py:28: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler(enabled=cfg.use_amp)\n/tmp/ipykernel_47/3107769954.py:9: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=cfg.use_amp):\n/tmp/ipykernel_47/3107769954.py:29: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=cfg.use_amp):\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5 | train: 22.1410 | valid: 17.3100 | 25.5s\nEpoch 2/5 | train: 14.8649 | valid: 14.7429 | 26.1s\nEpoch 3/5 | train: 13.5501 | valid: 14.5006 | 26.5s\nEpoch 4/5 | train: 13.6078 | valid: 14.4928 | 26.8s\nEpoch 5/5 | train: 13.5618 | valid: 14.5186 | 27.3s\n","output_type":"stream"}],"execution_count":37},{"id":"d2240157-689c-413d-b285-dd9243de22cc","cell_type":"code","source":"@torch.no_grad()\ndef infer(model_states, df):\n    models = []\n    for state in model_states:\n        model = DinoRegressor(cfg).to(DEVICE)\n        model.load_state_dict(state, strict=True)\n        model.eval()\n        models.append(model)\n    dataset = BiomassDataset(df, mode='test', transform=get_transforms(False))\n    loader = DataLoader(\n        dataset,\n        batch_size=cfg.batch_size,\n        shuffle=False,\n        num_workers=cfg.num_workers,\n        pin_memory=True,\n    )\n    image_preds = {}\n    for batch in loader:\n        images = batch['pixel_values'].to(DEVICE, non_blocking=True)\n        preds = torch.zeros((images.size(0), cfg.num_targets), device=DEVICE)\n        for model in models:\n            preds += model(images)\n        preds /= len(models)\n        preds = preds.detach().cpu().numpy()\n        for path, pred in zip(batch['image_path'], preds):\n            image_preds[path] = pred\n    return image_preds\n\nensemble_preds = infer(all_fold_models, test_images)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T10:46:45.384215Z","iopub.execute_input":"2025-12-29T10:46:45.385024Z","iopub.status.idle":"2025-12-29T10:46:50.084020Z","shell.execute_reply.started":"2025-12-29T10:46:45.384996Z","shell.execute_reply":"2025-12-29T10:46:50.082944Z"}},"outputs":[{"name":"stdout","text":"Loaded checkpoint with missing keys: ['cls_token', 'pos_embed', 'patch_embed.proj.weight', 'patch_embed.proj.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.attn.qkv.weight', 'blocks.0.attn.qkv.bias', 'blocks.0.attn.proj.weight', 'blocks.0.attn.proj.bias', 'blocks.0.ls1.gamma', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.0.mlp.fc1.weight', 'blocks.0.mlp.fc1.bias', 'blocks.0.mlp.fc2.weight', 'blocks.0.mlp.fc2.bias', 'blocks.0.ls2.gamma', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.attn.qkv.weight', 'blocks.1.attn.qkv.bias', 'blocks.1.attn.proj.weight', 'blocks.1.attn.proj.bias', 'blocks.1.ls1.gamma', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.1.mlp.fc1.weight', 'blocks.1.mlp.fc1.bias', 'blocks.1.mlp.fc2.weight', 'blocks.1.mlp.fc2.bias', 'blocks.1.ls2.gamma', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.attn.qkv.weight', 'blocks.2.attn.qkv.bias', 'blocks.2.attn.proj.weight', 'blocks.2.attn.proj.bias', 'blocks.2.ls1.gamma', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.2.mlp.fc1.weight', 'blocks.2.mlp.fc1.bias', 'blocks.2.mlp.fc2.weight', 'blocks.2.mlp.fc2.bias', 'blocks.2.ls2.gamma', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.attn.qkv.weight', 'blocks.3.attn.qkv.bias', 'blocks.3.attn.proj.weight', 'blocks.3.attn.proj.bias', 'blocks.3.ls1.gamma', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.3.mlp.fc1.weight', 'blocks.3.mlp.fc1.bias', 'blocks.3.mlp.fc2.weight', 'blocks.3.mlp.fc2.bias', 'blocks.3.ls2.gamma', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.attn.qkv.weight', 'blocks.4.attn.qkv.bias', 'blocks.4.attn.proj.weight', 'blocks.4.attn.proj.bias', 'blocks.4.ls1.gamma', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.4.mlp.fc1.weight', 'blocks.4.mlp.fc1.bias', 'blocks.4.mlp.fc2.weight', 'blocks.4.mlp.fc2.bias', 'blocks.4.ls2.gamma', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.attn.qkv.weight', 'blocks.5.attn.qkv.bias', 'blocks.5.attn.proj.weight', 'blocks.5.attn.proj.bias', 'blocks.5.ls1.gamma', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.5.mlp.fc1.weight', 'blocks.5.mlp.fc1.bias', 'blocks.5.mlp.fc2.weight', 'blocks.5.mlp.fc2.bias', 'blocks.5.ls2.gamma', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.attn.qkv.weight', 'blocks.6.attn.qkv.bias', 'blocks.6.attn.proj.weight', 'blocks.6.attn.proj.bias', 'blocks.6.ls1.gamma', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'blocks.6.mlp.fc1.weight', 'blocks.6.mlp.fc1.bias', 'blocks.6.mlp.fc2.weight', 'blocks.6.mlp.fc2.bias', 'blocks.6.ls2.gamma', 'blocks.7.norm1.weight', 'blocks.7.norm1.bias', 'blocks.7.attn.qkv.weight', 'blocks.7.attn.qkv.bias', 'blocks.7.attn.proj.weight', 'blocks.7.attn.proj.bias', 'blocks.7.ls1.gamma', 'blocks.7.norm2.weight', 'blocks.7.norm2.bias', 'blocks.7.mlp.fc1.weight', 'blocks.7.mlp.fc1.bias', 'blocks.7.mlp.fc2.weight', 'blocks.7.mlp.fc2.bias', 'blocks.7.ls2.gamma', 'blocks.8.norm1.weight', 'blocks.8.norm1.bias', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.qkv.bias', 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.8.ls1.gamma', 'blocks.8.norm2.weight', 'blocks.8.norm2.bias', 'blocks.8.mlp.fc1.weight', 'blocks.8.mlp.fc1.bias', 'blocks.8.mlp.fc2.weight', 'blocks.8.mlp.fc2.bias', 'blocks.8.ls2.gamma', 'blocks.9.norm1.weight', 'blocks.9.norm1.bias', 'blocks.9.attn.qkv.weight', 'blocks.9.attn.qkv.bias', 'blocks.9.attn.proj.weight', 'blocks.9.attn.proj.bias', 'blocks.9.ls1.gamma', 'blocks.9.norm2.weight', 'blocks.9.norm2.bias', 'blocks.9.mlp.fc1.weight', 'blocks.9.mlp.fc1.bias', 'blocks.9.mlp.fc2.weight', 'blocks.9.mlp.fc2.bias', 'blocks.9.ls2.gamma', 'blocks.10.norm1.weight', 'blocks.10.norm1.bias', 'blocks.10.attn.qkv.weight', 'blocks.10.attn.qkv.bias', 'blocks.10.attn.proj.weight', 'blocks.10.attn.proj.bias', 'blocks.10.ls1.gamma', 'blocks.10.norm2.weight', 'blocks.10.norm2.bias', 'blocks.10.mlp.fc1.weight', 'blocks.10.mlp.fc1.bias', 'blocks.10.mlp.fc2.weight', 'blocks.10.mlp.fc2.bias', 'blocks.10.ls2.gamma', 'blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.ls1.gamma', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.11.mlp.fc1.weight', 'blocks.11.mlp.fc1.bias', 'blocks.11.mlp.fc2.weight', 'blocks.11.mlp.fc2.bias', 'blocks.11.ls2.gamma', 'norm.weight', 'norm.bias']\nUnexpected keys: ['embeddings.cls_token', 'embeddings.mask_token', 'embeddings.position_embeddings', 'embeddings.patch_embeddings.projection.weight', 'embeddings.patch_embeddings.projection.bias', 'encoder.layer.0.norm1.weight', 'encoder.layer.0.norm1.bias', 'encoder.layer.0.attention.attention.query.weight', 'encoder.layer.0.attention.attention.query.bias', 'encoder.layer.0.attention.attention.key.weight', 'encoder.layer.0.attention.attention.key.bias', 'encoder.layer.0.attention.attention.value.weight', 'encoder.layer.0.attention.attention.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.layer_scale1.lambda1', 'encoder.layer.0.norm2.weight', 'encoder.layer.0.norm2.bias', 'encoder.layer.0.mlp.weights_in.weight', 'encoder.layer.0.mlp.weights_in.bias', 'encoder.layer.0.mlp.weights_out.weight', 'encoder.layer.0.mlp.weights_out.bias', 'encoder.layer.0.layer_scale2.lambda1', 'encoder.layer.1.norm1.weight', 'encoder.layer.1.norm1.bias', 'encoder.layer.1.attention.attention.query.weight', 'encoder.layer.1.attention.attention.query.bias', 'encoder.layer.1.attention.attention.key.weight', 'encoder.layer.1.attention.attention.key.bias', 'encoder.layer.1.attention.attention.value.weight', 'encoder.layer.1.attention.attention.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.layer_scale1.lambda1', 'encoder.layer.1.norm2.weight', 'encoder.layer.1.norm2.bias', 'encoder.layer.1.mlp.weights_in.weight', 'encoder.layer.1.mlp.weights_in.bias', 'encoder.layer.1.mlp.weights_out.weight', 'encoder.layer.1.mlp.weights_out.bias', 'encoder.layer.1.layer_scale2.lambda1', 'encoder.layer.2.norm1.weight', 'encoder.layer.2.norm1.bias', 'encoder.layer.2.attention.attention.query.weight', 'encoder.layer.2.attention.attention.query.bias', 'encoder.layer.2.attention.attention.key.weight', 'encoder.layer.2.attention.attention.key.bias', 'encoder.layer.2.attention.attention.value.weight', 'encoder.layer.2.attention.attention.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.layer_scale1.lambda1', 'encoder.layer.2.norm2.weight', 'encoder.layer.2.norm2.bias', 'encoder.layer.2.mlp.weights_in.weight', 'encoder.layer.2.mlp.weights_in.bias', 'encoder.layer.2.mlp.weights_out.weight', 'encoder.layer.2.mlp.weights_out.bias', 'encoder.layer.2.layer_scale2.lambda1', 'encoder.layer.3.norm1.weight', 'encoder.layer.3.norm1.bias', 'encoder.layer.3.attention.attention.query.weight', 'encoder.layer.3.attention.attention.query.bias', 'encoder.layer.3.attention.attention.key.weight', 'encoder.layer.3.attention.attention.key.bias', 'encoder.layer.3.attention.attention.value.weight', 'encoder.layer.3.attention.attention.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.layer_scale1.lambda1', 'encoder.layer.3.norm2.weight', 'encoder.layer.3.norm2.bias', 'encoder.layer.3.mlp.weights_in.weight', 'encoder.layer.3.mlp.weights_in.bias', 'encoder.layer.3.mlp.weights_out.weight', 'encoder.layer.3.mlp.weights_out.bias', 'encoder.layer.3.layer_scale2.lambda1', 'encoder.layer.4.norm1.weight', 'encoder.layer.4.norm1.bias', 'encoder.layer.4.attention.attention.query.weight', 'encoder.layer.4.attention.attention.query.bias', 'encoder.layer.4.attention.attention.key.weight', 'encoder.layer.4.attention.attention.key.bias', 'encoder.layer.4.attention.attention.value.weight', 'encoder.layer.4.attention.attention.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.layer_scale1.lambda1', 'encoder.layer.4.norm2.weight', 'encoder.layer.4.norm2.bias', 'encoder.layer.4.mlp.weights_in.weight', 'encoder.layer.4.mlp.weights_in.bias', 'encoder.layer.4.mlp.weights_out.weight', 'encoder.layer.4.mlp.weights_out.bias', 'encoder.layer.4.layer_scale2.lambda1', 'encoder.layer.5.norm1.weight', 'encoder.layer.5.norm1.bias', 'encoder.layer.5.attention.attention.query.weight', 'encoder.layer.5.attention.attention.query.bias', 'encoder.layer.5.attention.attention.key.weight', 'encoder.layer.5.attention.attention.key.bias', 'encoder.layer.5.attention.attention.value.weight', 'encoder.layer.5.attention.attention.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.layer_scale1.lambda1', 'encoder.layer.5.norm2.weight', 'encoder.layer.5.norm2.bias', 'encoder.layer.5.mlp.weights_in.weight', 'encoder.layer.5.mlp.weights_in.bias', 'encoder.layer.5.mlp.weights_out.weight', 'encoder.layer.5.mlp.weights_out.bias', 'encoder.layer.5.layer_scale2.lambda1', 'encoder.layer.6.norm1.weight', 'encoder.layer.6.norm1.bias', 'encoder.layer.6.attention.attention.query.weight', 'encoder.layer.6.attention.attention.query.bias', 'encoder.layer.6.attention.attention.key.weight', 'encoder.layer.6.attention.attention.key.bias', 'encoder.layer.6.attention.attention.value.weight', 'encoder.layer.6.attention.attention.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.layer_scale1.lambda1', 'encoder.layer.6.norm2.weight', 'encoder.layer.6.norm2.bias', 'encoder.layer.6.mlp.weights_in.weight', 'encoder.layer.6.mlp.weights_in.bias', 'encoder.layer.6.mlp.weights_out.weight', 'encoder.layer.6.mlp.weights_out.bias', 'encoder.layer.6.layer_scale2.lambda1', 'encoder.layer.7.norm1.weight', 'encoder.layer.7.norm1.bias', 'encoder.layer.7.attention.attention.query.weight', 'encoder.layer.7.attention.attention.query.bias', 'encoder.layer.7.attention.attention.key.weight', 'encoder.layer.7.attention.attention.key.bias', 'encoder.layer.7.attention.attention.value.weight', 'encoder.layer.7.attention.attention.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.layer_scale1.lambda1', 'encoder.layer.7.norm2.weight', 'encoder.layer.7.norm2.bias', 'encoder.layer.7.mlp.weights_in.weight', 'encoder.layer.7.mlp.weights_in.bias', 'encoder.layer.7.mlp.weights_out.weight', 'encoder.layer.7.mlp.weights_out.bias', 'encoder.layer.7.layer_scale2.lambda1', 'encoder.layer.8.norm1.weight', 'encoder.layer.8.norm1.bias', 'encoder.layer.8.attention.attention.query.weight', 'encoder.layer.8.attention.attention.query.bias', 'encoder.layer.8.attention.attention.key.weight', 'encoder.layer.8.attention.attention.key.bias', 'encoder.layer.8.attention.attention.value.weight', 'encoder.layer.8.attention.attention.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.layer_scale1.lambda1', 'encoder.layer.8.norm2.weight', 'encoder.layer.8.norm2.bias', 'encoder.layer.8.mlp.weights_in.weight', 'encoder.layer.8.mlp.weights_in.bias', 'encoder.layer.8.mlp.weights_out.weight', 'encoder.layer.8.mlp.weights_out.bias', 'encoder.layer.8.layer_scale2.lambda1', 'encoder.layer.9.norm1.weight', 'encoder.layer.9.norm1.bias', 'encoder.layer.9.attention.attention.query.weight', 'encoder.layer.9.attention.attention.query.bias', 'encoder.layer.9.attention.attention.key.weight', 'encoder.layer.9.attention.attention.key.bias', 'encoder.layer.9.attention.attention.value.weight', 'encoder.layer.9.attention.attention.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.layer_scale1.lambda1', 'encoder.layer.9.norm2.weight', 'encoder.layer.9.norm2.bias', 'encoder.layer.9.mlp.weights_in.weight', 'encoder.layer.9.mlp.weights_in.bias', 'encoder.layer.9.mlp.weights_out.weight', 'encoder.layer.9.mlp.weights_out.bias', 'encoder.layer.9.layer_scale2.lambda1', 'encoder.layer.10.norm1.weight', 'encoder.layer.10.norm1.bias', 'encoder.layer.10.attention.attention.query.weight', 'encoder.layer.10.attention.attention.query.bias', 'encoder.layer.10.attention.attention.key.weight', 'encoder.layer.10.attention.attention.key.bias', 'encoder.layer.10.attention.attention.value.weight', 'encoder.layer.10.attention.attention.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.layer_scale1.lambda1', 'encoder.layer.10.norm2.weight', 'encoder.layer.10.norm2.bias', 'encoder.layer.10.mlp.weights_in.weight', 'encoder.layer.10.mlp.weights_in.bias', 'encoder.layer.10.mlp.weights_out.weight', 'encoder.layer.10.mlp.weights_out.bias', 'encoder.layer.10.layer_scale2.lambda1', 'encoder.layer.11.norm1.weight', 'encoder.layer.11.norm1.bias', 'encoder.layer.11.attention.attention.query.weight', 'encoder.layer.11.attention.attention.query.bias', 'encoder.layer.11.attention.attention.key.weight', 'encoder.layer.11.attention.attention.key.bias', 'encoder.layer.11.attention.attention.value.weight', 'encoder.layer.11.attention.attention.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.layer_scale1.lambda1', 'encoder.layer.11.norm2.weight', 'encoder.layer.11.norm2.bias', 'encoder.layer.11.mlp.weights_in.weight', 'encoder.layer.11.mlp.weights_in.bias', 'encoder.layer.11.mlp.weights_out.weight', 'encoder.layer.11.mlp.weights_out.bias', 'encoder.layer.11.layer_scale2.lambda1', 'encoder.layer.12.norm1.weight', 'encoder.layer.12.norm1.bias', 'encoder.layer.12.attention.attention.query.weight', 'encoder.layer.12.attention.attention.query.bias', 'encoder.layer.12.attention.attention.key.weight', 'encoder.layer.12.attention.attention.key.bias', 'encoder.layer.12.attention.attention.value.weight', 'encoder.layer.12.attention.attention.value.bias', 'encoder.layer.12.attention.output.dense.weight', 'encoder.layer.12.attention.output.dense.bias', 'encoder.layer.12.layer_scale1.lambda1', 'encoder.layer.12.norm2.weight', 'encoder.layer.12.norm2.bias', 'encoder.layer.12.mlp.weights_in.weight', 'encoder.layer.12.mlp.weights_in.bias', 'encoder.layer.12.mlp.weights_out.weight', 'encoder.layer.12.mlp.weights_out.bias', 'encoder.layer.12.layer_scale2.lambda1', 'encoder.layer.13.norm1.weight', 'encoder.layer.13.norm1.bias', 'encoder.layer.13.attention.attention.query.weight', 'encoder.layer.13.attention.attention.query.bias', 'encoder.layer.13.attention.attention.key.weight', 'encoder.layer.13.attention.attention.key.bias', 'encoder.layer.13.attention.attention.value.weight', 'encoder.layer.13.attention.attention.value.bias', 'encoder.layer.13.attention.output.dense.weight', 'encoder.layer.13.attention.output.dense.bias', 'encoder.layer.13.layer_scale1.lambda1', 'encoder.layer.13.norm2.weight', 'encoder.layer.13.norm2.bias', 'encoder.layer.13.mlp.weights_in.weight', 'encoder.layer.13.mlp.weights_in.bias', 'encoder.layer.13.mlp.weights_out.weight', 'encoder.layer.13.mlp.weights_out.bias', 'encoder.layer.13.layer_scale2.lambda1', 'encoder.layer.14.norm1.weight', 'encoder.layer.14.norm1.bias', 'encoder.layer.14.attention.attention.query.weight', 'encoder.layer.14.attention.attention.query.bias', 'encoder.layer.14.attention.attention.key.weight', 'encoder.layer.14.attention.attention.key.bias', 'encoder.layer.14.attention.attention.value.weight', 'encoder.layer.14.attention.attention.value.bias', 'encoder.layer.14.attention.output.dense.weight', 'encoder.layer.14.attention.output.dense.bias', 'encoder.layer.14.layer_scale1.lambda1', 'encoder.layer.14.norm2.weight', 'encoder.layer.14.norm2.bias', 'encoder.layer.14.mlp.weights_in.weight', 'encoder.layer.14.mlp.weights_in.bias', 'encoder.layer.14.mlp.weights_out.weight', 'encoder.layer.14.mlp.weights_out.bias', 'encoder.layer.14.layer_scale2.lambda1', 'encoder.layer.15.norm1.weight', 'encoder.layer.15.norm1.bias', 'encoder.layer.15.attention.attention.query.weight', 'encoder.layer.15.attention.attention.query.bias', 'encoder.layer.15.attention.attention.key.weight', 'encoder.layer.15.attention.attention.key.bias', 'encoder.layer.15.attention.attention.value.weight', 'encoder.layer.15.attention.attention.value.bias', 'encoder.layer.15.attention.output.dense.weight', 'encoder.layer.15.attention.output.dense.bias', 'encoder.layer.15.layer_scale1.lambda1', 'encoder.layer.15.norm2.weight', 'encoder.layer.15.norm2.bias', 'encoder.layer.15.mlp.weights_in.weight', 'encoder.layer.15.mlp.weights_in.bias', 'encoder.layer.15.mlp.weights_out.weight', 'encoder.layer.15.mlp.weights_out.bias', 'encoder.layer.15.layer_scale2.lambda1', 'encoder.layer.16.norm1.weight', 'encoder.layer.16.norm1.bias', 'encoder.layer.16.attention.attention.query.weight', 'encoder.layer.16.attention.attention.query.bias', 'encoder.layer.16.attention.attention.key.weight', 'encoder.layer.16.attention.attention.key.bias', 'encoder.layer.16.attention.attention.value.weight', 'encoder.layer.16.attention.attention.value.bias', 'encoder.layer.16.attention.output.dense.weight', 'encoder.layer.16.attention.output.dense.bias', 'encoder.layer.16.layer_scale1.lambda1', 'encoder.layer.16.norm2.weight', 'encoder.layer.16.norm2.bias', 'encoder.layer.16.mlp.weights_in.weight', 'encoder.layer.16.mlp.weights_in.bias', 'encoder.layer.16.mlp.weights_out.weight', 'encoder.layer.16.mlp.weights_out.bias', 'encoder.layer.16.layer_scale2.lambda1', 'encoder.layer.17.norm1.weight', 'encoder.layer.17.norm1.bias', 'encoder.layer.17.attention.attention.query.weight', 'encoder.layer.17.attention.attention.query.bias', 'encoder.layer.17.attention.attention.key.weight', 'encoder.layer.17.attention.attention.key.bias', 'encoder.layer.17.attention.attention.value.weight', 'encoder.layer.17.attention.attention.value.bias', 'encoder.layer.17.attention.output.dense.weight', 'encoder.layer.17.attention.output.dense.bias', 'encoder.layer.17.layer_scale1.lambda1', 'encoder.layer.17.norm2.weight', 'encoder.layer.17.norm2.bias', 'encoder.layer.17.mlp.weights_in.weight', 'encoder.layer.17.mlp.weights_in.bias', 'encoder.layer.17.mlp.weights_out.weight', 'encoder.layer.17.mlp.weights_out.bias', 'encoder.layer.17.layer_scale2.lambda1', 'encoder.layer.18.norm1.weight', 'encoder.layer.18.norm1.bias', 'encoder.layer.18.attention.attention.query.weight', 'encoder.layer.18.attention.attention.query.bias', 'encoder.layer.18.attention.attention.key.weight', 'encoder.layer.18.attention.attention.key.bias', 'encoder.layer.18.attention.attention.value.weight', 'encoder.layer.18.attention.attention.value.bias', 'encoder.layer.18.attention.output.dense.weight', 'encoder.layer.18.attention.output.dense.bias', 'encoder.layer.18.layer_scale1.lambda1', 'encoder.layer.18.norm2.weight', 'encoder.layer.18.norm2.bias', 'encoder.layer.18.mlp.weights_in.weight', 'encoder.layer.18.mlp.weights_in.bias', 'encoder.layer.18.mlp.weights_out.weight', 'encoder.layer.18.mlp.weights_out.bias', 'encoder.layer.18.layer_scale2.lambda1', 'encoder.layer.19.norm1.weight', 'encoder.layer.19.norm1.bias', 'encoder.layer.19.attention.attention.query.weight', 'encoder.layer.19.attention.attention.query.bias', 'encoder.layer.19.attention.attention.key.weight', 'encoder.layer.19.attention.attention.key.bias', 'encoder.layer.19.attention.attention.value.weight', 'encoder.layer.19.attention.attention.value.bias', 'encoder.layer.19.attention.output.dense.weight', 'encoder.layer.19.attention.output.dense.bias', 'encoder.layer.19.layer_scale1.lambda1', 'encoder.layer.19.norm2.weight', 'encoder.layer.19.norm2.bias', 'encoder.layer.19.mlp.weights_in.weight', 'encoder.layer.19.mlp.weights_in.bias', 'encoder.layer.19.mlp.weights_out.weight', 'encoder.layer.19.mlp.weights_out.bias', 'encoder.layer.19.layer_scale2.lambda1', 'encoder.layer.20.norm1.weight', 'encoder.layer.20.norm1.bias', 'encoder.layer.20.attention.attention.query.weight', 'encoder.layer.20.attention.attention.query.bias', 'encoder.layer.20.attention.attention.key.weight', 'encoder.layer.20.attention.attention.key.bias', 'encoder.layer.20.attention.attention.value.weight', 'encoder.layer.20.attention.attention.value.bias', 'encoder.layer.20.attention.output.dense.weight', 'encoder.layer.20.attention.output.dense.bias', 'encoder.layer.20.layer_scale1.lambda1', 'encoder.layer.20.norm2.weight', 'encoder.layer.20.norm2.bias', 'encoder.layer.20.mlp.weights_in.weight', 'encoder.layer.20.mlp.weights_in.bias', 'encoder.layer.20.mlp.weights_out.weight', 'encoder.layer.20.mlp.weights_out.bias', 'encoder.layer.20.layer_scale2.lambda1', 'encoder.layer.21.norm1.weight', 'encoder.layer.21.norm1.bias', 'encoder.layer.21.attention.attention.query.weight', 'encoder.layer.21.attention.attention.query.bias', 'encoder.layer.21.attention.attention.key.weight', 'encoder.layer.21.attention.attention.key.bias', 'encoder.layer.21.attention.attention.value.weight', 'encoder.layer.21.attention.attention.value.bias', 'encoder.layer.21.attention.output.dense.weight', 'encoder.layer.21.attention.output.dense.bias', 'encoder.layer.21.layer_scale1.lambda1', 'encoder.layer.21.norm2.weight', 'encoder.layer.21.norm2.bias', 'encoder.layer.21.mlp.weights_in.weight', 'encoder.layer.21.mlp.weights_in.bias', 'encoder.layer.21.mlp.weights_out.weight', 'encoder.layer.21.mlp.weights_out.bias', 'encoder.layer.21.layer_scale2.lambda1', 'encoder.layer.22.norm1.weight', 'encoder.layer.22.norm1.bias', 'encoder.layer.22.attention.attention.query.weight', 'encoder.layer.22.attention.attention.query.bias', 'encoder.layer.22.attention.attention.key.weight', 'encoder.layer.22.attention.attention.key.bias', 'encoder.layer.22.attention.attention.value.weight', 'encoder.layer.22.attention.attention.value.bias', 'encoder.layer.22.attention.output.dense.weight', 'encoder.layer.22.attention.output.dense.bias', 'encoder.layer.22.layer_scale1.lambda1', 'encoder.layer.22.norm2.weight', 'encoder.layer.22.norm2.bias', 'encoder.layer.22.mlp.weights_in.weight', 'encoder.layer.22.mlp.weights_in.bias', 'encoder.layer.22.mlp.weights_out.weight', 'encoder.layer.22.mlp.weights_out.bias', 'encoder.layer.22.layer_scale2.lambda1', 'encoder.layer.23.norm1.weight', 'encoder.layer.23.norm1.bias', 'encoder.layer.23.attention.attention.query.weight', 'encoder.layer.23.attention.attention.query.bias', 'encoder.layer.23.attention.attention.key.weight', 'encoder.layer.23.attention.attention.key.bias', 'encoder.layer.23.attention.attention.value.weight', 'encoder.layer.23.attention.attention.value.bias', 'encoder.layer.23.attention.output.dense.weight', 'encoder.layer.23.attention.output.dense.bias', 'encoder.layer.23.layer_scale1.lambda1', 'encoder.layer.23.norm2.weight', 'encoder.layer.23.norm2.bias', 'encoder.layer.23.mlp.weights_in.weight', 'encoder.layer.23.mlp.weights_in.bias', 'encoder.layer.23.mlp.weights_out.weight', 'encoder.layer.23.mlp.weights_out.bias', 'encoder.layer.23.layer_scale2.lambda1', 'encoder.layer.24.norm1.weight', 'encoder.layer.24.norm1.bias', 'encoder.layer.24.attention.attention.query.weight', 'encoder.layer.24.attention.attention.query.bias', 'encoder.layer.24.attention.attention.key.weight', 'encoder.layer.24.attention.attention.key.bias', 'encoder.layer.24.attention.attention.value.weight', 'encoder.layer.24.attention.attention.value.bias', 'encoder.layer.24.attention.output.dense.weight', 'encoder.layer.24.attention.output.dense.bias', 'encoder.layer.24.layer_scale1.lambda1', 'encoder.layer.24.norm2.weight', 'encoder.layer.24.norm2.bias', 'encoder.layer.24.mlp.weights_in.weight', 'encoder.layer.24.mlp.weights_in.bias', 'encoder.layer.24.mlp.weights_out.weight', 'encoder.layer.24.mlp.weights_out.bias', 'encoder.layer.24.layer_scale2.lambda1', 'encoder.layer.25.norm1.weight', 'encoder.layer.25.norm1.bias', 'encoder.layer.25.attention.attention.query.weight', 'encoder.layer.25.attention.attention.query.bias', 'encoder.layer.25.attention.attention.key.weight', 'encoder.layer.25.attention.attention.key.bias', 'encoder.layer.25.attention.attention.value.weight', 'encoder.layer.25.attention.attention.value.bias', 'encoder.layer.25.attention.output.dense.weight', 'encoder.layer.25.attention.output.dense.bias', 'encoder.layer.25.layer_scale1.lambda1', 'encoder.layer.25.norm2.weight', 'encoder.layer.25.norm2.bias', 'encoder.layer.25.mlp.weights_in.weight', 'encoder.layer.25.mlp.weights_in.bias', 'encoder.layer.25.mlp.weights_out.weight', 'encoder.layer.25.mlp.weights_out.bias', 'encoder.layer.25.layer_scale2.lambda1', 'encoder.layer.26.norm1.weight', 'encoder.layer.26.norm1.bias', 'encoder.layer.26.attention.attention.query.weight', 'encoder.layer.26.attention.attention.query.bias', 'encoder.layer.26.attention.attention.key.weight', 'encoder.layer.26.attention.attention.key.bias', 'encoder.layer.26.attention.attention.value.weight', 'encoder.layer.26.attention.attention.value.bias', 'encoder.layer.26.attention.output.dense.weight', 'encoder.layer.26.attention.output.dense.bias', 'encoder.layer.26.layer_scale1.lambda1', 'encoder.layer.26.norm2.weight', 'encoder.layer.26.norm2.bias', 'encoder.layer.26.mlp.weights_in.weight', 'encoder.layer.26.mlp.weights_in.bias', 'encoder.layer.26.mlp.weights_out.weight', 'encoder.layer.26.mlp.weights_out.bias', 'encoder.layer.26.layer_scale2.lambda1', 'encoder.layer.27.norm1.weight', 'encoder.layer.27.norm1.bias', 'encoder.layer.27.attention.attention.query.weight', 'encoder.layer.27.attention.attention.query.bias', 'encoder.layer.27.attention.attention.key.weight', 'encoder.layer.27.attention.attention.key.bias', 'encoder.layer.27.attention.attention.value.weight', 'encoder.layer.27.attention.attention.value.bias', 'encoder.layer.27.attention.output.dense.weight', 'encoder.layer.27.attention.output.dense.bias', 'encoder.layer.27.layer_scale1.lambda1', 'encoder.layer.27.norm2.weight', 'encoder.layer.27.norm2.bias', 'encoder.layer.27.mlp.weights_in.weight', 'encoder.layer.27.mlp.weights_in.bias', 'encoder.layer.27.mlp.weights_out.weight', 'encoder.layer.27.mlp.weights_out.bias', 'encoder.layer.27.layer_scale2.lambda1', 'encoder.layer.28.norm1.weight', 'encoder.layer.28.norm1.bias', 'encoder.layer.28.attention.attention.query.weight', 'encoder.layer.28.attention.attention.query.bias', 'encoder.layer.28.attention.attention.key.weight', 'encoder.layer.28.attention.attention.key.bias', 'encoder.layer.28.attention.attention.value.weight', 'encoder.layer.28.attention.attention.value.bias', 'encoder.layer.28.attention.output.dense.weight', 'encoder.layer.28.attention.output.dense.bias', 'encoder.layer.28.layer_scale1.lambda1', 'encoder.layer.28.norm2.weight', 'encoder.layer.28.norm2.bias', 'encoder.layer.28.mlp.weights_in.weight', 'encoder.layer.28.mlp.weights_in.bias', 'encoder.layer.28.mlp.weights_out.weight', 'encoder.layer.28.mlp.weights_out.bias', 'encoder.layer.28.layer_scale2.lambda1', 'encoder.layer.29.norm1.weight', 'encoder.layer.29.norm1.bias', 'encoder.layer.29.attention.attention.query.weight', 'encoder.layer.29.attention.attention.query.bias', 'encoder.layer.29.attention.attention.key.weight', 'encoder.layer.29.attention.attention.key.bias', 'encoder.layer.29.attention.attention.value.weight', 'encoder.layer.29.attention.attention.value.bias', 'encoder.layer.29.attention.output.dense.weight', 'encoder.layer.29.attention.output.dense.bias', 'encoder.layer.29.layer_scale1.lambda1', 'encoder.layer.29.norm2.weight', 'encoder.layer.29.norm2.bias', 'encoder.layer.29.mlp.weights_in.weight', 'encoder.layer.29.mlp.weights_in.bias', 'encoder.layer.29.mlp.weights_out.weight', 'encoder.layer.29.mlp.weights_out.bias', 'encoder.layer.29.layer_scale2.lambda1', 'encoder.layer.30.norm1.weight', 'encoder.layer.30.norm1.bias', 'encoder.layer.30.attention.attention.query.weight', 'encoder.layer.30.attention.attention.query.bias', 'encoder.layer.30.attention.attention.key.weight', 'encoder.layer.30.attention.attention.key.bias', 'encoder.layer.30.attention.attention.value.weight', 'encoder.layer.30.attention.attention.value.bias', 'encoder.layer.30.attention.output.dense.weight', 'encoder.layer.30.attention.output.dense.bias', 'encoder.layer.30.layer_scale1.lambda1', 'encoder.layer.30.norm2.weight', 'encoder.layer.30.norm2.bias', 'encoder.layer.30.mlp.weights_in.weight', 'encoder.layer.30.mlp.weights_in.bias', 'encoder.layer.30.mlp.weights_out.weight', 'encoder.layer.30.mlp.weights_out.bias', 'encoder.layer.30.layer_scale2.lambda1', 'encoder.layer.31.norm1.weight', 'encoder.layer.31.norm1.bias', 'encoder.layer.31.attention.attention.query.weight', 'encoder.layer.31.attention.attention.query.bias', 'encoder.layer.31.attention.attention.key.weight', 'encoder.layer.31.attention.attention.key.bias', 'encoder.layer.31.attention.attention.value.weight', 'encoder.layer.31.attention.attention.value.bias', 'encoder.layer.31.attention.output.dense.weight', 'encoder.layer.31.attention.output.dense.bias', 'encoder.layer.31.layer_scale1.lambda1', 'encoder.layer.31.norm2.weight', 'encoder.layer.31.norm2.bias', 'encoder.layer.31.mlp.weights_in.weight', 'encoder.layer.31.mlp.weights_in.bias', 'encoder.layer.31.mlp.weights_out.weight', 'encoder.layer.31.mlp.weights_out.bias', 'encoder.layer.31.layer_scale2.lambda1', 'encoder.layer.32.norm1.weight', 'encoder.layer.32.norm1.bias', 'encoder.layer.32.attention.attention.query.weight', 'encoder.layer.32.attention.attention.query.bias', 'encoder.layer.32.attention.attention.key.weight', 'encoder.layer.32.attention.attention.key.bias', 'encoder.layer.32.attention.attention.value.weight', 'encoder.layer.32.attention.attention.value.bias', 'encoder.layer.32.attention.output.dense.weight', 'encoder.layer.32.attention.output.dense.bias', 'encoder.layer.32.layer_scale1.lambda1', 'encoder.layer.32.norm2.weight', 'encoder.layer.32.norm2.bias', 'encoder.layer.32.mlp.weights_in.weight', 'encoder.layer.32.mlp.weights_in.bias', 'encoder.layer.32.mlp.weights_out.weight', 'encoder.layer.32.mlp.weights_out.bias', 'encoder.layer.32.layer_scale2.lambda1', 'encoder.layer.33.norm1.weight', 'encoder.layer.33.norm1.bias', 'encoder.layer.33.attention.attention.query.weight', 'encoder.layer.33.attention.attention.query.bias', 'encoder.layer.33.attention.attention.key.weight', 'encoder.layer.33.attention.attention.key.bias', 'encoder.layer.33.attention.attention.value.weight', 'encoder.layer.33.attention.attention.value.bias', 'encoder.layer.33.attention.output.dense.weight', 'encoder.layer.33.attention.output.dense.bias', 'encoder.layer.33.layer_scale1.lambda1', 'encoder.layer.33.norm2.weight', 'encoder.layer.33.norm2.bias', 'encoder.layer.33.mlp.weights_in.weight', 'encoder.layer.33.mlp.weights_in.bias', 'encoder.layer.33.mlp.weights_out.weight', 'encoder.layer.33.mlp.weights_out.bias', 'encoder.layer.33.layer_scale2.lambda1', 'encoder.layer.34.norm1.weight', 'encoder.layer.34.norm1.bias', 'encoder.layer.34.attention.attention.query.weight', 'encoder.layer.34.attention.attention.query.bias', 'encoder.layer.34.attention.attention.key.weight', 'encoder.layer.34.attention.attention.key.bias', 'encoder.layer.34.attention.attention.value.weight', 'encoder.layer.34.attention.attention.value.bias', 'encoder.layer.34.attention.output.dense.weight', 'encoder.layer.34.attention.output.dense.bias', 'encoder.layer.34.layer_scale1.lambda1', 'encoder.layer.34.norm2.weight', 'encoder.layer.34.norm2.bias', 'encoder.layer.34.mlp.weights_in.weight', 'encoder.layer.34.mlp.weights_in.bias', 'encoder.layer.34.mlp.weights_out.weight', 'encoder.layer.34.mlp.weights_out.bias', 'encoder.layer.34.layer_scale2.lambda1', 'encoder.layer.35.norm1.weight', 'encoder.layer.35.norm1.bias', 'encoder.layer.35.attention.attention.query.weight', 'encoder.layer.35.attention.attention.query.bias', 'encoder.layer.35.attention.attention.key.weight', 'encoder.layer.35.attention.attention.key.bias', 'encoder.layer.35.attention.attention.value.weight', 'encoder.layer.35.attention.attention.value.bias', 'encoder.layer.35.attention.output.dense.weight', 'encoder.layer.35.attention.output.dense.bias', 'encoder.layer.35.layer_scale1.lambda1', 'encoder.layer.35.norm2.weight', 'encoder.layer.35.norm2.bias', 'encoder.layer.35.mlp.weights_in.weight', 'encoder.layer.35.mlp.weights_in.bias', 'encoder.layer.35.mlp.weights_out.weight', 'encoder.layer.35.mlp.weights_out.bias', 'encoder.layer.35.layer_scale2.lambda1', 'encoder.layer.36.norm1.weight', 'encoder.layer.36.norm1.bias', 'encoder.layer.36.attention.attention.query.weight', 'encoder.layer.36.attention.attention.query.bias', 'encoder.layer.36.attention.attention.key.weight', 'encoder.layer.36.attention.attention.key.bias', 'encoder.layer.36.attention.attention.value.weight', 'encoder.layer.36.attention.attention.value.bias', 'encoder.layer.36.attention.output.dense.weight', 'encoder.layer.36.attention.output.dense.bias', 'encoder.layer.36.layer_scale1.lambda1', 'encoder.layer.36.norm2.weight', 'encoder.layer.36.norm2.bias', 'encoder.layer.36.mlp.weights_in.weight', 'encoder.layer.36.mlp.weights_in.bias', 'encoder.layer.36.mlp.weights_out.weight', 'encoder.layer.36.mlp.weights_out.bias', 'encoder.layer.36.layer_scale2.lambda1', 'encoder.layer.37.norm1.weight', 'encoder.layer.37.norm1.bias', 'encoder.layer.37.attention.attention.query.weight', 'encoder.layer.37.attention.attention.query.bias', 'encoder.layer.37.attention.attention.key.weight', 'encoder.layer.37.attention.attention.key.bias', 'encoder.layer.37.attention.attention.value.weight', 'encoder.layer.37.attention.attention.value.bias', 'encoder.layer.37.attention.output.dense.weight', 'encoder.layer.37.attention.output.dense.bias', 'encoder.layer.37.layer_scale1.lambda1', 'encoder.layer.37.norm2.weight', 'encoder.layer.37.norm2.bias', 'encoder.layer.37.mlp.weights_in.weight', 'encoder.layer.37.mlp.weights_in.bias', 'encoder.layer.37.mlp.weights_out.weight', 'encoder.layer.37.mlp.weights_out.bias', 'encoder.layer.37.layer_scale2.lambda1', 'encoder.layer.38.norm1.weight', 'encoder.layer.38.norm1.bias', 'encoder.layer.38.attention.attention.query.weight', 'encoder.layer.38.attention.attention.query.bias', 'encoder.layer.38.attention.attention.key.weight', 'encoder.layer.38.attention.attention.key.bias', 'encoder.layer.38.attention.attention.value.weight', 'encoder.layer.38.attention.attention.value.bias', 'encoder.layer.38.attention.output.dense.weight', 'encoder.layer.38.attention.output.dense.bias', 'encoder.layer.38.layer_scale1.lambda1', 'encoder.layer.38.norm2.weight', 'encoder.layer.38.norm2.bias', 'encoder.layer.38.mlp.weights_in.weight', 'encoder.layer.38.mlp.weights_in.bias', 'encoder.layer.38.mlp.weights_out.weight', 'encoder.layer.38.mlp.weights_out.bias', 'encoder.layer.38.layer_scale2.lambda1', 'encoder.layer.39.norm1.weight', 'encoder.layer.39.norm1.bias', 'encoder.layer.39.attention.attention.query.weight', 'encoder.layer.39.attention.attention.query.bias', 'encoder.layer.39.attention.attention.key.weight', 'encoder.layer.39.attention.attention.key.bias', 'encoder.layer.39.attention.attention.value.weight', 'encoder.layer.39.attention.attention.value.bias', 'encoder.layer.39.attention.output.dense.weight', 'encoder.layer.39.attention.output.dense.bias', 'encoder.layer.39.layer_scale1.lambda1', 'encoder.layer.39.norm2.weight', 'encoder.layer.39.norm2.bias', 'encoder.layer.39.mlp.weights_in.weight', 'encoder.layer.39.mlp.weights_in.bias', 'encoder.layer.39.mlp.weights_out.weight', 'encoder.layer.39.mlp.weights_out.bias', 'encoder.layer.39.layer_scale2.lambda1', 'layernorm.weight', 'layernorm.bias']\n","output_type":"stream"}],"execution_count":38},{"id":"f48aea55-555b-4395-bfd0-5646f0e94a80","cell_type":"code","source":"submission = test_df.copy()\nsubmission['target'] = submission.apply(\n    lambda row: ensemble_preds[row['image_path']][cfg.target_names.index(row['target_name'])],\n    axis=1\n)\nsubmission[['sample_id', 'target']].to_csv('submission.csv', index=False)\nsubmission.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T10:47:00.300432Z","iopub.execute_input":"2025-12-29T10:47:00.301052Z","iopub.status.idle":"2025-12-29T10:47:00.316527Z","shell.execute_reply.started":"2025-12-29T10:47:00.301021Z","shell.execute_reply":"2025-12-29T10:47:00.315607Z"}},"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"                    sample_id             image_path   target_name     target\n0  ID1001187975__Dry_Clover_g  test/ID1001187975.jpg  Dry_Clover_g   1.690462\n1    ID1001187975__Dry_Dead_g  test/ID1001187975.jpg    Dry_Dead_g   8.660296\n2   ID1001187975__Dry_Green_g  test/ID1001187975.jpg   Dry_Green_g  21.079224\n3   ID1001187975__Dry_Total_g  test/ID1001187975.jpg   Dry_Total_g  40.596779\n4         ID1001187975__GDM_g  test/ID1001187975.jpg         GDM_g  26.444044","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sample_id</th>\n      <th>image_path</th>\n      <th>target_name</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ID1001187975__Dry_Clover_g</td>\n      <td>test/ID1001187975.jpg</td>\n      <td>Dry_Clover_g</td>\n      <td>1.690462</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ID1001187975__Dry_Dead_g</td>\n      <td>test/ID1001187975.jpg</td>\n      <td>Dry_Dead_g</td>\n      <td>8.660296</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ID1001187975__Dry_Green_g</td>\n      <td>test/ID1001187975.jpg</td>\n      <td>Dry_Green_g</td>\n      <td>21.079224</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ID1001187975__Dry_Total_g</td>\n      <td>test/ID1001187975.jpg</td>\n      <td>Dry_Total_g</td>\n      <td>40.596779</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ID1001187975__GDM_g</td>\n      <td>test/ID1001187975.jpg</td>\n      <td>GDM_g</td>\n      <td>26.444044</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":39}]}