{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "777e21d8",
   "metadata": {
    "papermill": {
     "duration": 0.003787,
     "end_time": "2025-12-29T10:51:01.281283",
     "exception": false,
     "start_time": "2025-12-29T10:51:01.277496",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# CSIRO - Image2Biomass with DINOv2 ViT\n",
    "Fine-tunes Meta's DINOv2 Vision Transformer (via `timm`) on the CSIRO Image2Biomass dataset to predict five pasture biomass measurements from aerial imagery.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca566355",
   "metadata": {
    "papermill": {
     "duration": 0.002698,
     "end_time": "2025-12-29T10:51:01.287077",
     "exception": false,
     "start_time": "2025-12-29T10:51:01.284379",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Approach Overview\n",
    "We load the provided train/test CSVs, reshape labels into per-image targets, and split folds by image. A DINOv2 backbone feeds a lightweight regression head, optimizing Smooth L1 losses across all biomass components. After training, we ensemble fold checkpoints to generate predictions and write the final `submission.csv`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b10a5cd5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T10:51:01.293383Z",
     "iopub.status.busy": "2025-12-29T10:51:01.293080Z",
     "iopub.status.idle": "2025-12-29T10:51:25.457301Z",
     "shell.execute_reply": "2025-12-29T10:51:25.456415Z"
    },
    "papermill": {
     "duration": 24.16918,
     "end_time": "2025-12-29T10:51:25.458726",
     "exception": false,
     "start_time": "2025-12-29T10:51:01.289546",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu124\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, asdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch \n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import timm\n",
    "from timm.data import create_transform\n",
    "print(torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67039beb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T10:51:25.465905Z",
     "iopub.status.busy": "2025-12-29T10:51:25.465423Z",
     "iopub.status.idle": "2025-12-29T10:51:25.472962Z",
     "shell.execute_reply": "2025-12-29T10:51:25.472112Z"
    },
    "papermill": {
     "duration": 0.012511,
     "end_time": "2025-12-29T10:51:25.474215",
     "exception": false,
     "start_time": "2025-12-29T10:51:25.461704",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data from: /kaggle/input/csiro-biomass\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class cfg:\n",
    "    data_dir: Path = Path('/kaggle/input/csiro-biomass')\n",
    "    output_dir: Path = Path('.')\n",
    "    seed: int = 2024\n",
    "    img_size: int = 518\n",
    "    batch_size: int = 8\n",
    "    num_workers: int = 4\n",
    "    epochs: int = 5\n",
    "    lr: float = 2e-4\n",
    "    min_lr: float = 1e-6\n",
    "    weight_decay: float = 1e-4\n",
    "    backbone: str = 'vit_base_patch14_dinov2'\n",
    "    checkpoint_dir: Path = Path('/kaggle/input/dinov2/pytorch/giant/1')\n",
    "    hidden_dim: int = 512\n",
    "    drop_rate: float = 0.1\n",
    "    n_folds: int = 5\n",
    "    train_folds: tuple = (0,)\n",
    "    target_names: tuple = ('Dry_Clover_g','Dry_Dead_g','Dry_Green_g','Dry_Total_g','GDM_g')\n",
    "    num_targets: int = 5\n",
    "    use_amp: bool = True\n",
    "    grad_accum_steps: int = 1\n",
    "\n",
    "cfg = cfg()\n",
    "\n",
    "possible_dirs = [\n",
    "    Path('/kaggle/input/csiro-biomass'),\n",
    "    Path('/kaggle/input/CSIRO-I2B'),\n",
    "    Path('csiro-biomass'),\n",
    "]\n",
    "for p in possible_dirs:\n",
    "    if p.exists():\n",
    "        cfg.data_dir = p\n",
    "        break\n",
    "print(f\"Using data from: {cfg.data_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79f10200",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T10:51:25.480592Z",
     "iopub.status.busy": "2025-12-29T10:51:25.480185Z",
     "iopub.status.idle": "2025-12-29T10:51:25.493806Z",
     "shell.execute_reply": "2025-12-29T10:51:25.493034Z"
    },
    "papermill": {
     "duration": 0.0181,
     "end_time": "2025-12-29T10:51:25.494943",
     "exception": false,
     "start_time": "2025-12-29T10:51:25.476843",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(cfg.seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "691e6d2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T10:51:25.500848Z",
     "iopub.status.busy": "2025-12-29T10:51:25.500627Z",
     "iopub.status.idle": "2025-12-29T10:51:25.602558Z",
     "shell.execute_reply": "2025-12-29T10:51:25.601553Z"
    },
    "papermill": {
     "duration": 0.106365,
     "end_time": "2025-12-29T10:51:25.603788",
     "exception": false,
     "start_time": "2025-12-29T10:51:25.497423",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               image_path  Dry_Clover_g  Dry_Dead_g  Dry_Green_g  Dry_Total_g  \\\n",
      "0  train/ID1011485656.jpg        0.0000     31.9984      16.2751      48.2735   \n",
      "1  train/ID1012260530.jpg        0.0000      0.0000       7.6000       7.6000   \n",
      "2  train/ID1025234388.jpg        6.0500      0.0000       0.0000       6.0500   \n",
      "3  train/ID1028611175.jpg        0.0000     30.9703      24.2376      55.2079   \n",
      "4  train/ID1035947949.jpg        0.4343     23.2239      10.5261      34.1844   \n",
      "\n",
      "     GDM_g  fold  \n",
      "0  16.2750     0  \n",
      "1   7.6000     4  \n",
      "2   6.0500     3  \n",
      "3  24.2376     2  \n",
      "4  10.9605     1  \n",
      "fold\n",
      "0    72\n",
      "1    72\n",
      "4    71\n",
      "3    71\n",
      "2    71\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(cfg.data_dir / 'train.csv')\n",
    "test_df = pd.read_csv(cfg.data_dir / 'test.csv')\n",
    "\n",
    "train_targets = train_df.pivot_table(\n",
    "    index='image_path',\n",
    "    columns='target_name',\n",
    "    values='target'\n",
    ").reset_index().copy()\n",
    "train_targets.columns = ['image_path', *list(cfg.target_names)]\n",
    "train_targets[list(cfg.target_names)] = train_targets[list(cfg.target_names)].fillna(0.0).astype(float)\n",
    "\n",
    "train_targets['fold'] = -1\n",
    "kf = GroupKFold(n_splits=cfg.n_folds)\n",
    "for fold, (_, val_idx) in enumerate(kf.split(train_targets, groups=train_targets['image_path'])):\n",
    "    train_targets.loc[val_idx, 'fold'] = fold\n",
    "\n",
    "print(train_targets.head())\n",
    "print(train_targets['fold'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94023169",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T10:51:25.610586Z",
     "iopub.status.busy": "2025-12-29T10:51:25.610077Z",
     "iopub.status.idle": "2025-12-29T10:51:25.615602Z",
     "shell.execute_reply": "2025-12-29T10:51:25.615057Z"
    },
    "papermill": {
     "duration": 0.009787,
     "end_time": "2025-12-29T10:51:25.616548",
     "exception": false,
     "start_time": "2025-12-29T10:51:25.606761",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BiomassDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, mode: str, transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.mode = mode\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        image_path = cfg.data_dir / row['image_path']\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        else:\n",
    "            image = transforms.ToTensor()(image)\n",
    "        sample = {\n",
    "            'pixel_values': image,\n",
    "            'image_path': row['image_path']\n",
    "        }\n",
    "        if self.mode != 'test':\n",
    "            targets = row[list(cfg.target_names)].astype(float).values\n",
    "            target = torch.tensor(targets, dtype=torch.float32)\n",
    "            sample['targets'] = target\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e72fbb9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T10:51:25.622331Z",
     "iopub.status.busy": "2025-12-29T10:51:25.622147Z",
     "iopub.status.idle": "2025-12-29T10:51:25.625875Z",
     "shell.execute_reply": "2025-12-29T10:51:25.625348Z"
    },
    "papermill": {
     "duration": 0.0077,
     "end_time": "2025-12-29T10:51:25.626834",
     "exception": false,
     "start_time": "2025-12-29T10:51:25.619134",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "def get_transforms(is_train=True):\n",
    "    return create_transform(\n",
    "        input_size=(3, cfg.img_size, cfg.img_size),\n",
    "        is_training=is_train,\n",
    "        auto_augment='rand-m9-mstd0.5-inc1' if is_train else None,\n",
    "        interpolation='bicubic',\n",
    "        re_prob=0.25 if is_train else 0.0,\n",
    "        re_mode='pixel',\n",
    "        re_count=1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e50c105b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T10:51:25.632828Z",
     "iopub.status.busy": "2025-12-29T10:51:25.632634Z",
     "iopub.status.idle": "2025-12-29T10:51:25.639714Z",
     "shell.execute_reply": "2025-12-29T10:51:25.639117Z"
    },
    "papermill": {
     "duration": 0.011496,
     "end_time": "2025-12-29T10:51:25.640880",
     "exception": false,
     "start_time": "2025-12-29T10:51:25.629384",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DinoRegressor(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model(\n",
    "            cfg.backbone,\n",
    "            pretrained=False,\n",
    "            num_classes=0,\n",
    "            global_pool='' \n",
    "        )\n",
    "        checkpoint_path = None\n",
    "        if cfg.checkpoint_dir is not None:\n",
    "            ckpt_dir = Path(cfg.checkpoint_dir)\n",
    "            if ckpt_dir.exists():\n",
    "                for pattern in ('*.pth', '*.pt', '*.bin', '*.safetensors'):\n",
    "                    candidates = sorted(ckpt_dir.rglob(pattern))\n",
    "                    if candidates:\n",
    "                        checkpoint_path = candidates[0]\n",
    "                        break\n",
    "        if checkpoint_path is None:\n",
    "            raise FileNotFoundError('No DINOv2 checkpoint found under ' + str(cfg.checkpoint_dir))\n",
    "        state = torch.load(checkpoint_path, map_location='cpu')\n",
    "        if isinstance(state, dict):\n",
    "            for key in ('state_dict', 'model', 'model_state'):\n",
    "                if key in state:\n",
    "                    state = state[key]\n",
    "                    break\n",
    "        missing, unexpected = self.backbone.load_state_dict(state, strict=False)\n",
    "        if missing or unexpected:\n",
    "            print('Loaded checkpoint with missing keys:', missing)\n",
    "            print('Unexpected keys:', unexpected)\n",
    "        else:\n",
    "            print(f'Successfully loaded weights from {checkpoint_path}')\n",
    "        in_features = self.backbone.num_features\n",
    "        self.head = nn.Sequential(\n",
    "            nn.LayerNorm(in_features),\n",
    "            nn.Dropout(cfg.drop_rate),\n",
    "            nn.Linear(in_features, cfg.hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(cfg.drop_rate),\n",
    "            nn.Linear(cfg.hidden_dim, cfg.num_targets)\n",
    "        )\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        feats = self.backbone(pixel_values)\n",
    "        if feats.ndim == 3:\n",
    "            feats = feats.mean(dim=1)\n",
    "        return self.head(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b659686",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T10:51:25.646956Z",
     "iopub.status.busy": "2025-12-29T10:51:25.646575Z",
     "iopub.status.idle": "2025-12-29T10:51:25.652203Z",
     "shell.execute_reply": "2025-12-29T10:51:25.651512Z"
    },
    "papermill": {
     "duration": 0.00993,
     "end_time": "2025-12-29T10:51:25.653349",
     "exception": false,
     "start_time": "2025-12-29T10:51:25.643419",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AverageMeter:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count if self.count != 0 else 0\n",
    "\n",
    "def get_dataloaders(train_df, valid_df):\n",
    "    train_dataset = BiomassDataset(train_df, mode='train', transform=get_transforms(True))\n",
    "    valid_dataset = BiomassDataset(valid_df, mode='valid', transform=get_transforms(False))\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=cfg.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=cfg.num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=cfg.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=cfg.num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "    )\n",
    "    return train_loader, valid_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c626647",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T10:51:25.659289Z",
     "iopub.status.busy": "2025-12-29T10:51:25.658818Z",
     "iopub.status.idle": "2025-12-29T10:51:25.665123Z",
     "shell.execute_reply": "2025-12-29T10:51:25.664542Z"
    },
    "papermill": {
     "duration": 0.010376,
     "end_time": "2025-12-29T10:51:25.666172",
     "exception": false,
     "start_time": "2025-12-29T10:51:25.655796",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, criterion, optimizer, scheduler, scaler, device):\n",
    "    model.train()\n",
    "    loss_meter = AverageMeter()\n",
    "\n",
    "    for step, batch in enumerate(loader):\n",
    "        images = batch['pixel_values'].to(device, non_blocking=True)\n",
    "        targets = batch['targets'].to(device, non_blocking=True)\n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast(enabled=cfg.use_amp):\n",
    "            preds = model(images)\n",
    "            loss = criterion(preds, targets)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        loss_meter.update(loss.item(), images.size(0))\n",
    "    return loss_meter.avg\n",
    "\n",
    "\n",
    "def validate_one_epoch(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    loss_meter = AverageMeter()\n",
    "    preds_list = []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            images = batch['pixel_values'].to(device, non_blocking=True)\n",
    "            targets = batch['targets'].to(device, non_blocking=True)\n",
    "            with torch.cuda.amp.autocast(enabled=cfg.use_amp):\n",
    "                preds = model(images)\n",
    "                loss = criterion(preds, targets)\n",
    "            loss_meter.update(loss.item(), images.size(0))\n",
    "            preds_list.append(preds.detach().cpu())\n",
    "    predictions = torch.cat(preds_list).numpy()\n",
    "    return loss_meter.avg, predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fe6b9fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T10:51:25.672105Z",
     "iopub.status.busy": "2025-12-29T10:51:25.671873Z",
     "iopub.status.idle": "2025-12-29T10:54:01.080958Z",
     "shell.execute_reply": "2025-12-29T10:54:01.079966Z"
    },
    "papermill": {
     "duration": 155.413624,
     "end_time": "2025-12-29T10:54:01.082236",
     "exception": false,
     "start_time": "2025-12-29T10:51:25.668612",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Fold 0 =====\n",
      "Loaded checkpoint with missing keys: ['cls_token', 'pos_embed', 'patch_embed.proj.weight', 'patch_embed.proj.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.attn.qkv.weight', 'blocks.0.attn.qkv.bias', 'blocks.0.attn.proj.weight', 'blocks.0.attn.proj.bias', 'blocks.0.ls1.gamma', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.0.mlp.fc1.weight', 'blocks.0.mlp.fc1.bias', 'blocks.0.mlp.fc2.weight', 'blocks.0.mlp.fc2.bias', 'blocks.0.ls2.gamma', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.attn.qkv.weight', 'blocks.1.attn.qkv.bias', 'blocks.1.attn.proj.weight', 'blocks.1.attn.proj.bias', 'blocks.1.ls1.gamma', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.1.mlp.fc1.weight', 'blocks.1.mlp.fc1.bias', 'blocks.1.mlp.fc2.weight', 'blocks.1.mlp.fc2.bias', 'blocks.1.ls2.gamma', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.attn.qkv.weight', 'blocks.2.attn.qkv.bias', 'blocks.2.attn.proj.weight', 'blocks.2.attn.proj.bias', 'blocks.2.ls1.gamma', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.2.mlp.fc1.weight', 'blocks.2.mlp.fc1.bias', 'blocks.2.mlp.fc2.weight', 'blocks.2.mlp.fc2.bias', 'blocks.2.ls2.gamma', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.attn.qkv.weight', 'blocks.3.attn.qkv.bias', 'blocks.3.attn.proj.weight', 'blocks.3.attn.proj.bias', 'blocks.3.ls1.gamma', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.3.mlp.fc1.weight', 'blocks.3.mlp.fc1.bias', 'blocks.3.mlp.fc2.weight', 'blocks.3.mlp.fc2.bias', 'blocks.3.ls2.gamma', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.attn.qkv.weight', 'blocks.4.attn.qkv.bias', 'blocks.4.attn.proj.weight', 'blocks.4.attn.proj.bias', 'blocks.4.ls1.gamma', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.4.mlp.fc1.weight', 'blocks.4.mlp.fc1.bias', 'blocks.4.mlp.fc2.weight', 'blocks.4.mlp.fc2.bias', 'blocks.4.ls2.gamma', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.attn.qkv.weight', 'blocks.5.attn.qkv.bias', 'blocks.5.attn.proj.weight', 'blocks.5.attn.proj.bias', 'blocks.5.ls1.gamma', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.5.mlp.fc1.weight', 'blocks.5.mlp.fc1.bias', 'blocks.5.mlp.fc2.weight', 'blocks.5.mlp.fc2.bias', 'blocks.5.ls2.gamma', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.attn.qkv.weight', 'blocks.6.attn.qkv.bias', 'blocks.6.attn.proj.weight', 'blocks.6.attn.proj.bias', 'blocks.6.ls1.gamma', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'blocks.6.mlp.fc1.weight', 'blocks.6.mlp.fc1.bias', 'blocks.6.mlp.fc2.weight', 'blocks.6.mlp.fc2.bias', 'blocks.6.ls2.gamma', 'blocks.7.norm1.weight', 'blocks.7.norm1.bias', 'blocks.7.attn.qkv.weight', 'blocks.7.attn.qkv.bias', 'blocks.7.attn.proj.weight', 'blocks.7.attn.proj.bias', 'blocks.7.ls1.gamma', 'blocks.7.norm2.weight', 'blocks.7.norm2.bias', 'blocks.7.mlp.fc1.weight', 'blocks.7.mlp.fc1.bias', 'blocks.7.mlp.fc2.weight', 'blocks.7.mlp.fc2.bias', 'blocks.7.ls2.gamma', 'blocks.8.norm1.weight', 'blocks.8.norm1.bias', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.qkv.bias', 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.8.ls1.gamma', 'blocks.8.norm2.weight', 'blocks.8.norm2.bias', 'blocks.8.mlp.fc1.weight', 'blocks.8.mlp.fc1.bias', 'blocks.8.mlp.fc2.weight', 'blocks.8.mlp.fc2.bias', 'blocks.8.ls2.gamma', 'blocks.9.norm1.weight', 'blocks.9.norm1.bias', 'blocks.9.attn.qkv.weight', 'blocks.9.attn.qkv.bias', 'blocks.9.attn.proj.weight', 'blocks.9.attn.proj.bias', 'blocks.9.ls1.gamma', 'blocks.9.norm2.weight', 'blocks.9.norm2.bias', 'blocks.9.mlp.fc1.weight', 'blocks.9.mlp.fc1.bias', 'blocks.9.mlp.fc2.weight', 'blocks.9.mlp.fc2.bias', 'blocks.9.ls2.gamma', 'blocks.10.norm1.weight', 'blocks.10.norm1.bias', 'blocks.10.attn.qkv.weight', 'blocks.10.attn.qkv.bias', 'blocks.10.attn.proj.weight', 'blocks.10.attn.proj.bias', 'blocks.10.ls1.gamma', 'blocks.10.norm2.weight', 'blocks.10.norm2.bias', 'blocks.10.mlp.fc1.weight', 'blocks.10.mlp.fc1.bias', 'blocks.10.mlp.fc2.weight', 'blocks.10.mlp.fc2.bias', 'blocks.10.ls2.gamma', 'blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.ls1.gamma', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.11.mlp.fc1.weight', 'blocks.11.mlp.fc1.bias', 'blocks.11.mlp.fc2.weight', 'blocks.11.mlp.fc2.bias', 'blocks.11.ls2.gamma', 'norm.weight', 'norm.bias']\n",
      "Unexpected keys: ['embeddings.cls_token', 'embeddings.mask_token', 'embeddings.position_embeddings', 'embeddings.patch_embeddings.projection.weight', 'embeddings.patch_embeddings.projection.bias', 'encoder.layer.0.norm1.weight', 'encoder.layer.0.norm1.bias', 'encoder.layer.0.attention.attention.query.weight', 'encoder.layer.0.attention.attention.query.bias', 'encoder.layer.0.attention.attention.key.weight', 'encoder.layer.0.attention.attention.key.bias', 'encoder.layer.0.attention.attention.value.weight', 'encoder.layer.0.attention.attention.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.layer_scale1.lambda1', 'encoder.layer.0.norm2.weight', 'encoder.layer.0.norm2.bias', 'encoder.layer.0.mlp.weights_in.weight', 'encoder.layer.0.mlp.weights_in.bias', 'encoder.layer.0.mlp.weights_out.weight', 'encoder.layer.0.mlp.weights_out.bias', 'encoder.layer.0.layer_scale2.lambda1', 'encoder.layer.1.norm1.weight', 'encoder.layer.1.norm1.bias', 'encoder.layer.1.attention.attention.query.weight', 'encoder.layer.1.attention.attention.query.bias', 'encoder.layer.1.attention.attention.key.weight', 'encoder.layer.1.attention.attention.key.bias', 'encoder.layer.1.attention.attention.value.weight', 'encoder.layer.1.attention.attention.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.layer_scale1.lambda1', 'encoder.layer.1.norm2.weight', 'encoder.layer.1.norm2.bias', 'encoder.layer.1.mlp.weights_in.weight', 'encoder.layer.1.mlp.weights_in.bias', 'encoder.layer.1.mlp.weights_out.weight', 'encoder.layer.1.mlp.weights_out.bias', 'encoder.layer.1.layer_scale2.lambda1', 'encoder.layer.2.norm1.weight', 'encoder.layer.2.norm1.bias', 'encoder.layer.2.attention.attention.query.weight', 'encoder.layer.2.attention.attention.query.bias', 'encoder.layer.2.attention.attention.key.weight', 'encoder.layer.2.attention.attention.key.bias', 'encoder.layer.2.attention.attention.value.weight', 'encoder.layer.2.attention.attention.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.layer_scale1.lambda1', 'encoder.layer.2.norm2.weight', 'encoder.layer.2.norm2.bias', 'encoder.layer.2.mlp.weights_in.weight', 'encoder.layer.2.mlp.weights_in.bias', 'encoder.layer.2.mlp.weights_out.weight', 'encoder.layer.2.mlp.weights_out.bias', 'encoder.layer.2.layer_scale2.lambda1', 'encoder.layer.3.norm1.weight', 'encoder.layer.3.norm1.bias', 'encoder.layer.3.attention.attention.query.weight', 'encoder.layer.3.attention.attention.query.bias', 'encoder.layer.3.attention.attention.key.weight', 'encoder.layer.3.attention.attention.key.bias', 'encoder.layer.3.attention.attention.value.weight', 'encoder.layer.3.attention.attention.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.layer_scale1.lambda1', 'encoder.layer.3.norm2.weight', 'encoder.layer.3.norm2.bias', 'encoder.layer.3.mlp.weights_in.weight', 'encoder.layer.3.mlp.weights_in.bias', 'encoder.layer.3.mlp.weights_out.weight', 'encoder.layer.3.mlp.weights_out.bias', 'encoder.layer.3.layer_scale2.lambda1', 'encoder.layer.4.norm1.weight', 'encoder.layer.4.norm1.bias', 'encoder.layer.4.attention.attention.query.weight', 'encoder.layer.4.attention.attention.query.bias', 'encoder.layer.4.attention.attention.key.weight', 'encoder.layer.4.attention.attention.key.bias', 'encoder.layer.4.attention.attention.value.weight', 'encoder.layer.4.attention.attention.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.layer_scale1.lambda1', 'encoder.layer.4.norm2.weight', 'encoder.layer.4.norm2.bias', 'encoder.layer.4.mlp.weights_in.weight', 'encoder.layer.4.mlp.weights_in.bias', 'encoder.layer.4.mlp.weights_out.weight', 'encoder.layer.4.mlp.weights_out.bias', 'encoder.layer.4.layer_scale2.lambda1', 'encoder.layer.5.norm1.weight', 'encoder.layer.5.norm1.bias', 'encoder.layer.5.attention.attention.query.weight', 'encoder.layer.5.attention.attention.query.bias', 'encoder.layer.5.attention.attention.key.weight', 'encoder.layer.5.attention.attention.key.bias', 'encoder.layer.5.attention.attention.value.weight', 'encoder.layer.5.attention.attention.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.layer_scale1.lambda1', 'encoder.layer.5.norm2.weight', 'encoder.layer.5.norm2.bias', 'encoder.layer.5.mlp.weights_in.weight', 'encoder.layer.5.mlp.weights_in.bias', 'encoder.layer.5.mlp.weights_out.weight', 'encoder.layer.5.mlp.weights_out.bias', 'encoder.layer.5.layer_scale2.lambda1', 'encoder.layer.6.norm1.weight', 'encoder.layer.6.norm1.bias', 'encoder.layer.6.attention.attention.query.weight', 'encoder.layer.6.attention.attention.query.bias', 'encoder.layer.6.attention.attention.key.weight', 'encoder.layer.6.attention.attention.key.bias', 'encoder.layer.6.attention.attention.value.weight', 'encoder.layer.6.attention.attention.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.layer_scale1.lambda1', 'encoder.layer.6.norm2.weight', 'encoder.layer.6.norm2.bias', 'encoder.layer.6.mlp.weights_in.weight', 'encoder.layer.6.mlp.weights_in.bias', 'encoder.layer.6.mlp.weights_out.weight', 'encoder.layer.6.mlp.weights_out.bias', 'encoder.layer.6.layer_scale2.lambda1', 'encoder.layer.7.norm1.weight', 'encoder.layer.7.norm1.bias', 'encoder.layer.7.attention.attention.query.weight', 'encoder.layer.7.attention.attention.query.bias', 'encoder.layer.7.attention.attention.key.weight', 'encoder.layer.7.attention.attention.key.bias', 'encoder.layer.7.attention.attention.value.weight', 'encoder.layer.7.attention.attention.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.layer_scale1.lambda1', 'encoder.layer.7.norm2.weight', 'encoder.layer.7.norm2.bias', 'encoder.layer.7.mlp.weights_in.weight', 'encoder.layer.7.mlp.weights_in.bias', 'encoder.layer.7.mlp.weights_out.weight', 'encoder.layer.7.mlp.weights_out.bias', 'encoder.layer.7.layer_scale2.lambda1', 'encoder.layer.8.norm1.weight', 'encoder.layer.8.norm1.bias', 'encoder.layer.8.attention.attention.query.weight', 'encoder.layer.8.attention.attention.query.bias', 'encoder.layer.8.attention.attention.key.weight', 'encoder.layer.8.attention.attention.key.bias', 'encoder.layer.8.attention.attention.value.weight', 'encoder.layer.8.attention.attention.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.layer_scale1.lambda1', 'encoder.layer.8.norm2.weight', 'encoder.layer.8.norm2.bias', 'encoder.layer.8.mlp.weights_in.weight', 'encoder.layer.8.mlp.weights_in.bias', 'encoder.layer.8.mlp.weights_out.weight', 'encoder.layer.8.mlp.weights_out.bias', 'encoder.layer.8.layer_scale2.lambda1', 'encoder.layer.9.norm1.weight', 'encoder.layer.9.norm1.bias', 'encoder.layer.9.attention.attention.query.weight', 'encoder.layer.9.attention.attention.query.bias', 'encoder.layer.9.attention.attention.key.weight', 'encoder.layer.9.attention.attention.key.bias', 'encoder.layer.9.attention.attention.value.weight', 'encoder.layer.9.attention.attention.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.layer_scale1.lambda1', 'encoder.layer.9.norm2.weight', 'encoder.layer.9.norm2.bias', 'encoder.layer.9.mlp.weights_in.weight', 'encoder.layer.9.mlp.weights_in.bias', 'encoder.layer.9.mlp.weights_out.weight', 'encoder.layer.9.mlp.weights_out.bias', 'encoder.layer.9.layer_scale2.lambda1', 'encoder.layer.10.norm1.weight', 'encoder.layer.10.norm1.bias', 'encoder.layer.10.attention.attention.query.weight', 'encoder.layer.10.attention.attention.query.bias', 'encoder.layer.10.attention.attention.key.weight', 'encoder.layer.10.attention.attention.key.bias', 'encoder.layer.10.attention.attention.value.weight', 'encoder.layer.10.attention.attention.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.layer_scale1.lambda1', 'encoder.layer.10.norm2.weight', 'encoder.layer.10.norm2.bias', 'encoder.layer.10.mlp.weights_in.weight', 'encoder.layer.10.mlp.weights_in.bias', 'encoder.layer.10.mlp.weights_out.weight', 'encoder.layer.10.mlp.weights_out.bias', 'encoder.layer.10.layer_scale2.lambda1', 'encoder.layer.11.norm1.weight', 'encoder.layer.11.norm1.bias', 'encoder.layer.11.attention.attention.query.weight', 'encoder.layer.11.attention.attention.query.bias', 'encoder.layer.11.attention.attention.key.weight', 'encoder.layer.11.attention.attention.key.bias', 'encoder.layer.11.attention.attention.value.weight', 'encoder.layer.11.attention.attention.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.layer_scale1.lambda1', 'encoder.layer.11.norm2.weight', 'encoder.layer.11.norm2.bias', 'encoder.layer.11.mlp.weights_in.weight', 'encoder.layer.11.mlp.weights_in.bias', 'encoder.layer.11.mlp.weights_out.weight', 'encoder.layer.11.mlp.weights_out.bias', 'encoder.layer.11.layer_scale2.lambda1', 'encoder.layer.12.norm1.weight', 'encoder.layer.12.norm1.bias', 'encoder.layer.12.attention.attention.query.weight', 'encoder.layer.12.attention.attention.query.bias', 'encoder.layer.12.attention.attention.key.weight', 'encoder.layer.12.attention.attention.key.bias', 'encoder.layer.12.attention.attention.value.weight', 'encoder.layer.12.attention.attention.value.bias', 'encoder.layer.12.attention.output.dense.weight', 'encoder.layer.12.attention.output.dense.bias', 'encoder.layer.12.layer_scale1.lambda1', 'encoder.layer.12.norm2.weight', 'encoder.layer.12.norm2.bias', 'encoder.layer.12.mlp.weights_in.weight', 'encoder.layer.12.mlp.weights_in.bias', 'encoder.layer.12.mlp.weights_out.weight', 'encoder.layer.12.mlp.weights_out.bias', 'encoder.layer.12.layer_scale2.lambda1', 'encoder.layer.13.norm1.weight', 'encoder.layer.13.norm1.bias', 'encoder.layer.13.attention.attention.query.weight', 'encoder.layer.13.attention.attention.query.bias', 'encoder.layer.13.attention.attention.key.weight', 'encoder.layer.13.attention.attention.key.bias', 'encoder.layer.13.attention.attention.value.weight', 'encoder.layer.13.attention.attention.value.bias', 'encoder.layer.13.attention.output.dense.weight', 'encoder.layer.13.attention.output.dense.bias', 'encoder.layer.13.layer_scale1.lambda1', 'encoder.layer.13.norm2.weight', 'encoder.layer.13.norm2.bias', 'encoder.layer.13.mlp.weights_in.weight', 'encoder.layer.13.mlp.weights_in.bias', 'encoder.layer.13.mlp.weights_out.weight', 'encoder.layer.13.mlp.weights_out.bias', 'encoder.layer.13.layer_scale2.lambda1', 'encoder.layer.14.norm1.weight', 'encoder.layer.14.norm1.bias', 'encoder.layer.14.attention.attention.query.weight', 'encoder.layer.14.attention.attention.query.bias', 'encoder.layer.14.attention.attention.key.weight', 'encoder.layer.14.attention.attention.key.bias', 'encoder.layer.14.attention.attention.value.weight', 'encoder.layer.14.attention.attention.value.bias', 'encoder.layer.14.attention.output.dense.weight', 'encoder.layer.14.attention.output.dense.bias', 'encoder.layer.14.layer_scale1.lambda1', 'encoder.layer.14.norm2.weight', 'encoder.layer.14.norm2.bias', 'encoder.layer.14.mlp.weights_in.weight', 'encoder.layer.14.mlp.weights_in.bias', 'encoder.layer.14.mlp.weights_out.weight', 'encoder.layer.14.mlp.weights_out.bias', 'encoder.layer.14.layer_scale2.lambda1', 'encoder.layer.15.norm1.weight', 'encoder.layer.15.norm1.bias', 'encoder.layer.15.attention.attention.query.weight', 'encoder.layer.15.attention.attention.query.bias', 'encoder.layer.15.attention.attention.key.weight', 'encoder.layer.15.attention.attention.key.bias', 'encoder.layer.15.attention.attention.value.weight', 'encoder.layer.15.attention.attention.value.bias', 'encoder.layer.15.attention.output.dense.weight', 'encoder.layer.15.attention.output.dense.bias', 'encoder.layer.15.layer_scale1.lambda1', 'encoder.layer.15.norm2.weight', 'encoder.layer.15.norm2.bias', 'encoder.layer.15.mlp.weights_in.weight', 'encoder.layer.15.mlp.weights_in.bias', 'encoder.layer.15.mlp.weights_out.weight', 'encoder.layer.15.mlp.weights_out.bias', 'encoder.layer.15.layer_scale2.lambda1', 'encoder.layer.16.norm1.weight', 'encoder.layer.16.norm1.bias', 'encoder.layer.16.attention.attention.query.weight', 'encoder.layer.16.attention.attention.query.bias', 'encoder.layer.16.attention.attention.key.weight', 'encoder.layer.16.attention.attention.key.bias', 'encoder.layer.16.attention.attention.value.weight', 'encoder.layer.16.attention.attention.value.bias', 'encoder.layer.16.attention.output.dense.weight', 'encoder.layer.16.attention.output.dense.bias', 'encoder.layer.16.layer_scale1.lambda1', 'encoder.layer.16.norm2.weight', 'encoder.layer.16.norm2.bias', 'encoder.layer.16.mlp.weights_in.weight', 'encoder.layer.16.mlp.weights_in.bias', 'encoder.layer.16.mlp.weights_out.weight', 'encoder.layer.16.mlp.weights_out.bias', 'encoder.layer.16.layer_scale2.lambda1', 'encoder.layer.17.norm1.weight', 'encoder.layer.17.norm1.bias', 'encoder.layer.17.attention.attention.query.weight', 'encoder.layer.17.attention.attention.query.bias', 'encoder.layer.17.attention.attention.key.weight', 'encoder.layer.17.attention.attention.key.bias', 'encoder.layer.17.attention.attention.value.weight', 'encoder.layer.17.attention.attention.value.bias', 'encoder.layer.17.attention.output.dense.weight', 'encoder.layer.17.attention.output.dense.bias', 'encoder.layer.17.layer_scale1.lambda1', 'encoder.layer.17.norm2.weight', 'encoder.layer.17.norm2.bias', 'encoder.layer.17.mlp.weights_in.weight', 'encoder.layer.17.mlp.weights_in.bias', 'encoder.layer.17.mlp.weights_out.weight', 'encoder.layer.17.mlp.weights_out.bias', 'encoder.layer.17.layer_scale2.lambda1', 'encoder.layer.18.norm1.weight', 'encoder.layer.18.norm1.bias', 'encoder.layer.18.attention.attention.query.weight', 'encoder.layer.18.attention.attention.query.bias', 'encoder.layer.18.attention.attention.key.weight', 'encoder.layer.18.attention.attention.key.bias', 'encoder.layer.18.attention.attention.value.weight', 'encoder.layer.18.attention.attention.value.bias', 'encoder.layer.18.attention.output.dense.weight', 'encoder.layer.18.attention.output.dense.bias', 'encoder.layer.18.layer_scale1.lambda1', 'encoder.layer.18.norm2.weight', 'encoder.layer.18.norm2.bias', 'encoder.layer.18.mlp.weights_in.weight', 'encoder.layer.18.mlp.weights_in.bias', 'encoder.layer.18.mlp.weights_out.weight', 'encoder.layer.18.mlp.weights_out.bias', 'encoder.layer.18.layer_scale2.lambda1', 'encoder.layer.19.norm1.weight', 'encoder.layer.19.norm1.bias', 'encoder.layer.19.attention.attention.query.weight', 'encoder.layer.19.attention.attention.query.bias', 'encoder.layer.19.attention.attention.key.weight', 'encoder.layer.19.attention.attention.key.bias', 'encoder.layer.19.attention.attention.value.weight', 'encoder.layer.19.attention.attention.value.bias', 'encoder.layer.19.attention.output.dense.weight', 'encoder.layer.19.attention.output.dense.bias', 'encoder.layer.19.layer_scale1.lambda1', 'encoder.layer.19.norm2.weight', 'encoder.layer.19.norm2.bias', 'encoder.layer.19.mlp.weights_in.weight', 'encoder.layer.19.mlp.weights_in.bias', 'encoder.layer.19.mlp.weights_out.weight', 'encoder.layer.19.mlp.weights_out.bias', 'encoder.layer.19.layer_scale2.lambda1', 'encoder.layer.20.norm1.weight', 'encoder.layer.20.norm1.bias', 'encoder.layer.20.attention.attention.query.weight', 'encoder.layer.20.attention.attention.query.bias', 'encoder.layer.20.attention.attention.key.weight', 'encoder.layer.20.attention.attention.key.bias', 'encoder.layer.20.attention.attention.value.weight', 'encoder.layer.20.attention.attention.value.bias', 'encoder.layer.20.attention.output.dense.weight', 'encoder.layer.20.attention.output.dense.bias', 'encoder.layer.20.layer_scale1.lambda1', 'encoder.layer.20.norm2.weight', 'encoder.layer.20.norm2.bias', 'encoder.layer.20.mlp.weights_in.weight', 'encoder.layer.20.mlp.weights_in.bias', 'encoder.layer.20.mlp.weights_out.weight', 'encoder.layer.20.mlp.weights_out.bias', 'encoder.layer.20.layer_scale2.lambda1', 'encoder.layer.21.norm1.weight', 'encoder.layer.21.norm1.bias', 'encoder.layer.21.attention.attention.query.weight', 'encoder.layer.21.attention.attention.query.bias', 'encoder.layer.21.attention.attention.key.weight', 'encoder.layer.21.attention.attention.key.bias', 'encoder.layer.21.attention.attention.value.weight', 'encoder.layer.21.attention.attention.value.bias', 'encoder.layer.21.attention.output.dense.weight', 'encoder.layer.21.attention.output.dense.bias', 'encoder.layer.21.layer_scale1.lambda1', 'encoder.layer.21.norm2.weight', 'encoder.layer.21.norm2.bias', 'encoder.layer.21.mlp.weights_in.weight', 'encoder.layer.21.mlp.weights_in.bias', 'encoder.layer.21.mlp.weights_out.weight', 'encoder.layer.21.mlp.weights_out.bias', 'encoder.layer.21.layer_scale2.lambda1', 'encoder.layer.22.norm1.weight', 'encoder.layer.22.norm1.bias', 'encoder.layer.22.attention.attention.query.weight', 'encoder.layer.22.attention.attention.query.bias', 'encoder.layer.22.attention.attention.key.weight', 'encoder.layer.22.attention.attention.key.bias', 'encoder.layer.22.attention.attention.value.weight', 'encoder.layer.22.attention.attention.value.bias', 'encoder.layer.22.attention.output.dense.weight', 'encoder.layer.22.attention.output.dense.bias', 'encoder.layer.22.layer_scale1.lambda1', 'encoder.layer.22.norm2.weight', 'encoder.layer.22.norm2.bias', 'encoder.layer.22.mlp.weights_in.weight', 'encoder.layer.22.mlp.weights_in.bias', 'encoder.layer.22.mlp.weights_out.weight', 'encoder.layer.22.mlp.weights_out.bias', 'encoder.layer.22.layer_scale2.lambda1', 'encoder.layer.23.norm1.weight', 'encoder.layer.23.norm1.bias', 'encoder.layer.23.attention.attention.query.weight', 'encoder.layer.23.attention.attention.query.bias', 'encoder.layer.23.attention.attention.key.weight', 'encoder.layer.23.attention.attention.key.bias', 'encoder.layer.23.attention.attention.value.weight', 'encoder.layer.23.attention.attention.value.bias', 'encoder.layer.23.attention.output.dense.weight', 'encoder.layer.23.attention.output.dense.bias', 'encoder.layer.23.layer_scale1.lambda1', 'encoder.layer.23.norm2.weight', 'encoder.layer.23.norm2.bias', 'encoder.layer.23.mlp.weights_in.weight', 'encoder.layer.23.mlp.weights_in.bias', 'encoder.layer.23.mlp.weights_out.weight', 'encoder.layer.23.mlp.weights_out.bias', 'encoder.layer.23.layer_scale2.lambda1', 'encoder.layer.24.norm1.weight', 'encoder.layer.24.norm1.bias', 'encoder.layer.24.attention.attention.query.weight', 'encoder.layer.24.attention.attention.query.bias', 'encoder.layer.24.attention.attention.key.weight', 'encoder.layer.24.attention.attention.key.bias', 'encoder.layer.24.attention.attention.value.weight', 'encoder.layer.24.attention.attention.value.bias', 'encoder.layer.24.attention.output.dense.weight', 'encoder.layer.24.attention.output.dense.bias', 'encoder.layer.24.layer_scale1.lambda1', 'encoder.layer.24.norm2.weight', 'encoder.layer.24.norm2.bias', 'encoder.layer.24.mlp.weights_in.weight', 'encoder.layer.24.mlp.weights_in.bias', 'encoder.layer.24.mlp.weights_out.weight', 'encoder.layer.24.mlp.weights_out.bias', 'encoder.layer.24.layer_scale2.lambda1', 'encoder.layer.25.norm1.weight', 'encoder.layer.25.norm1.bias', 'encoder.layer.25.attention.attention.query.weight', 'encoder.layer.25.attention.attention.query.bias', 'encoder.layer.25.attention.attention.key.weight', 'encoder.layer.25.attention.attention.key.bias', 'encoder.layer.25.attention.attention.value.weight', 'encoder.layer.25.attention.attention.value.bias', 'encoder.layer.25.attention.output.dense.weight', 'encoder.layer.25.attention.output.dense.bias', 'encoder.layer.25.layer_scale1.lambda1', 'encoder.layer.25.norm2.weight', 'encoder.layer.25.norm2.bias', 'encoder.layer.25.mlp.weights_in.weight', 'encoder.layer.25.mlp.weights_in.bias', 'encoder.layer.25.mlp.weights_out.weight', 'encoder.layer.25.mlp.weights_out.bias', 'encoder.layer.25.layer_scale2.lambda1', 'encoder.layer.26.norm1.weight', 'encoder.layer.26.norm1.bias', 'encoder.layer.26.attention.attention.query.weight', 'encoder.layer.26.attention.attention.query.bias', 'encoder.layer.26.attention.attention.key.weight', 'encoder.layer.26.attention.attention.key.bias', 'encoder.layer.26.attention.attention.value.weight', 'encoder.layer.26.attention.attention.value.bias', 'encoder.layer.26.attention.output.dense.weight', 'encoder.layer.26.attention.output.dense.bias', 'encoder.layer.26.layer_scale1.lambda1', 'encoder.layer.26.norm2.weight', 'encoder.layer.26.norm2.bias', 'encoder.layer.26.mlp.weights_in.weight', 'encoder.layer.26.mlp.weights_in.bias', 'encoder.layer.26.mlp.weights_out.weight', 'encoder.layer.26.mlp.weights_out.bias', 'encoder.layer.26.layer_scale2.lambda1', 'encoder.layer.27.norm1.weight', 'encoder.layer.27.norm1.bias', 'encoder.layer.27.attention.attention.query.weight', 'encoder.layer.27.attention.attention.query.bias', 'encoder.layer.27.attention.attention.key.weight', 'encoder.layer.27.attention.attention.key.bias', 'encoder.layer.27.attention.attention.value.weight', 'encoder.layer.27.attention.attention.value.bias', 'encoder.layer.27.attention.output.dense.weight', 'encoder.layer.27.attention.output.dense.bias', 'encoder.layer.27.layer_scale1.lambda1', 'encoder.layer.27.norm2.weight', 'encoder.layer.27.norm2.bias', 'encoder.layer.27.mlp.weights_in.weight', 'encoder.layer.27.mlp.weights_in.bias', 'encoder.layer.27.mlp.weights_out.weight', 'encoder.layer.27.mlp.weights_out.bias', 'encoder.layer.27.layer_scale2.lambda1', 'encoder.layer.28.norm1.weight', 'encoder.layer.28.norm1.bias', 'encoder.layer.28.attention.attention.query.weight', 'encoder.layer.28.attention.attention.query.bias', 'encoder.layer.28.attention.attention.key.weight', 'encoder.layer.28.attention.attention.key.bias', 'encoder.layer.28.attention.attention.value.weight', 'encoder.layer.28.attention.attention.value.bias', 'encoder.layer.28.attention.output.dense.weight', 'encoder.layer.28.attention.output.dense.bias', 'encoder.layer.28.layer_scale1.lambda1', 'encoder.layer.28.norm2.weight', 'encoder.layer.28.norm2.bias', 'encoder.layer.28.mlp.weights_in.weight', 'encoder.layer.28.mlp.weights_in.bias', 'encoder.layer.28.mlp.weights_out.weight', 'encoder.layer.28.mlp.weights_out.bias', 'encoder.layer.28.layer_scale2.lambda1', 'encoder.layer.29.norm1.weight', 'encoder.layer.29.norm1.bias', 'encoder.layer.29.attention.attention.query.weight', 'encoder.layer.29.attention.attention.query.bias', 'encoder.layer.29.attention.attention.key.weight', 'encoder.layer.29.attention.attention.key.bias', 'encoder.layer.29.attention.attention.value.weight', 'encoder.layer.29.attention.attention.value.bias', 'encoder.layer.29.attention.output.dense.weight', 'encoder.layer.29.attention.output.dense.bias', 'encoder.layer.29.layer_scale1.lambda1', 'encoder.layer.29.norm2.weight', 'encoder.layer.29.norm2.bias', 'encoder.layer.29.mlp.weights_in.weight', 'encoder.layer.29.mlp.weights_in.bias', 'encoder.layer.29.mlp.weights_out.weight', 'encoder.layer.29.mlp.weights_out.bias', 'encoder.layer.29.layer_scale2.lambda1', 'encoder.layer.30.norm1.weight', 'encoder.layer.30.norm1.bias', 'encoder.layer.30.attention.attention.query.weight', 'encoder.layer.30.attention.attention.query.bias', 'encoder.layer.30.attention.attention.key.weight', 'encoder.layer.30.attention.attention.key.bias', 'encoder.layer.30.attention.attention.value.weight', 'encoder.layer.30.attention.attention.value.bias', 'encoder.layer.30.attention.output.dense.weight', 'encoder.layer.30.attention.output.dense.bias', 'encoder.layer.30.layer_scale1.lambda1', 'encoder.layer.30.norm2.weight', 'encoder.layer.30.norm2.bias', 'encoder.layer.30.mlp.weights_in.weight', 'encoder.layer.30.mlp.weights_in.bias', 'encoder.layer.30.mlp.weights_out.weight', 'encoder.layer.30.mlp.weights_out.bias', 'encoder.layer.30.layer_scale2.lambda1', 'encoder.layer.31.norm1.weight', 'encoder.layer.31.norm1.bias', 'encoder.layer.31.attention.attention.query.weight', 'encoder.layer.31.attention.attention.query.bias', 'encoder.layer.31.attention.attention.key.weight', 'encoder.layer.31.attention.attention.key.bias', 'encoder.layer.31.attention.attention.value.weight', 'encoder.layer.31.attention.attention.value.bias', 'encoder.layer.31.attention.output.dense.weight', 'encoder.layer.31.attention.output.dense.bias', 'encoder.layer.31.layer_scale1.lambda1', 'encoder.layer.31.norm2.weight', 'encoder.layer.31.norm2.bias', 'encoder.layer.31.mlp.weights_in.weight', 'encoder.layer.31.mlp.weights_in.bias', 'encoder.layer.31.mlp.weights_out.weight', 'encoder.layer.31.mlp.weights_out.bias', 'encoder.layer.31.layer_scale2.lambda1', 'encoder.layer.32.norm1.weight', 'encoder.layer.32.norm1.bias', 'encoder.layer.32.attention.attention.query.weight', 'encoder.layer.32.attention.attention.query.bias', 'encoder.layer.32.attention.attention.key.weight', 'encoder.layer.32.attention.attention.key.bias', 'encoder.layer.32.attention.attention.value.weight', 'encoder.layer.32.attention.attention.value.bias', 'encoder.layer.32.attention.output.dense.weight', 'encoder.layer.32.attention.output.dense.bias', 'encoder.layer.32.layer_scale1.lambda1', 'encoder.layer.32.norm2.weight', 'encoder.layer.32.norm2.bias', 'encoder.layer.32.mlp.weights_in.weight', 'encoder.layer.32.mlp.weights_in.bias', 'encoder.layer.32.mlp.weights_out.weight', 'encoder.layer.32.mlp.weights_out.bias', 'encoder.layer.32.layer_scale2.lambda1', 'encoder.layer.33.norm1.weight', 'encoder.layer.33.norm1.bias', 'encoder.layer.33.attention.attention.query.weight', 'encoder.layer.33.attention.attention.query.bias', 'encoder.layer.33.attention.attention.key.weight', 'encoder.layer.33.attention.attention.key.bias', 'encoder.layer.33.attention.attention.value.weight', 'encoder.layer.33.attention.attention.value.bias', 'encoder.layer.33.attention.output.dense.weight', 'encoder.layer.33.attention.output.dense.bias', 'encoder.layer.33.layer_scale1.lambda1', 'encoder.layer.33.norm2.weight', 'encoder.layer.33.norm2.bias', 'encoder.layer.33.mlp.weights_in.weight', 'encoder.layer.33.mlp.weights_in.bias', 'encoder.layer.33.mlp.weights_out.weight', 'encoder.layer.33.mlp.weights_out.bias', 'encoder.layer.33.layer_scale2.lambda1', 'encoder.layer.34.norm1.weight', 'encoder.layer.34.norm1.bias', 'encoder.layer.34.attention.attention.query.weight', 'encoder.layer.34.attention.attention.query.bias', 'encoder.layer.34.attention.attention.key.weight', 'encoder.layer.34.attention.attention.key.bias', 'encoder.layer.34.attention.attention.value.weight', 'encoder.layer.34.attention.attention.value.bias', 'encoder.layer.34.attention.output.dense.weight', 'encoder.layer.34.attention.output.dense.bias', 'encoder.layer.34.layer_scale1.lambda1', 'encoder.layer.34.norm2.weight', 'encoder.layer.34.norm2.bias', 'encoder.layer.34.mlp.weights_in.weight', 'encoder.layer.34.mlp.weights_in.bias', 'encoder.layer.34.mlp.weights_out.weight', 'encoder.layer.34.mlp.weights_out.bias', 'encoder.layer.34.layer_scale2.lambda1', 'encoder.layer.35.norm1.weight', 'encoder.layer.35.norm1.bias', 'encoder.layer.35.attention.attention.query.weight', 'encoder.layer.35.attention.attention.query.bias', 'encoder.layer.35.attention.attention.key.weight', 'encoder.layer.35.attention.attention.key.bias', 'encoder.layer.35.attention.attention.value.weight', 'encoder.layer.35.attention.attention.value.bias', 'encoder.layer.35.attention.output.dense.weight', 'encoder.layer.35.attention.output.dense.bias', 'encoder.layer.35.layer_scale1.lambda1', 'encoder.layer.35.norm2.weight', 'encoder.layer.35.norm2.bias', 'encoder.layer.35.mlp.weights_in.weight', 'encoder.layer.35.mlp.weights_in.bias', 'encoder.layer.35.mlp.weights_out.weight', 'encoder.layer.35.mlp.weights_out.bias', 'encoder.layer.35.layer_scale2.lambda1', 'encoder.layer.36.norm1.weight', 'encoder.layer.36.norm1.bias', 'encoder.layer.36.attention.attention.query.weight', 'encoder.layer.36.attention.attention.query.bias', 'encoder.layer.36.attention.attention.key.weight', 'encoder.layer.36.attention.attention.key.bias', 'encoder.layer.36.attention.attention.value.weight', 'encoder.layer.36.attention.attention.value.bias', 'encoder.layer.36.attention.output.dense.weight', 'encoder.layer.36.attention.output.dense.bias', 'encoder.layer.36.layer_scale1.lambda1', 'encoder.layer.36.norm2.weight', 'encoder.layer.36.norm2.bias', 'encoder.layer.36.mlp.weights_in.weight', 'encoder.layer.36.mlp.weights_in.bias', 'encoder.layer.36.mlp.weights_out.weight', 'encoder.layer.36.mlp.weights_out.bias', 'encoder.layer.36.layer_scale2.lambda1', 'encoder.layer.37.norm1.weight', 'encoder.layer.37.norm1.bias', 'encoder.layer.37.attention.attention.query.weight', 'encoder.layer.37.attention.attention.query.bias', 'encoder.layer.37.attention.attention.key.weight', 'encoder.layer.37.attention.attention.key.bias', 'encoder.layer.37.attention.attention.value.weight', 'encoder.layer.37.attention.attention.value.bias', 'encoder.layer.37.attention.output.dense.weight', 'encoder.layer.37.attention.output.dense.bias', 'encoder.layer.37.layer_scale1.lambda1', 'encoder.layer.37.norm2.weight', 'encoder.layer.37.norm2.bias', 'encoder.layer.37.mlp.weights_in.weight', 'encoder.layer.37.mlp.weights_in.bias', 'encoder.layer.37.mlp.weights_out.weight', 'encoder.layer.37.mlp.weights_out.bias', 'encoder.layer.37.layer_scale2.lambda1', 'encoder.layer.38.norm1.weight', 'encoder.layer.38.norm1.bias', 'encoder.layer.38.attention.attention.query.weight', 'encoder.layer.38.attention.attention.query.bias', 'encoder.layer.38.attention.attention.key.weight', 'encoder.layer.38.attention.attention.key.bias', 'encoder.layer.38.attention.attention.value.weight', 'encoder.layer.38.attention.attention.value.bias', 'encoder.layer.38.attention.output.dense.weight', 'encoder.layer.38.attention.output.dense.bias', 'encoder.layer.38.layer_scale1.lambda1', 'encoder.layer.38.norm2.weight', 'encoder.layer.38.norm2.bias', 'encoder.layer.38.mlp.weights_in.weight', 'encoder.layer.38.mlp.weights_in.bias', 'encoder.layer.38.mlp.weights_out.weight', 'encoder.layer.38.mlp.weights_out.bias', 'encoder.layer.38.layer_scale2.lambda1', 'encoder.layer.39.norm1.weight', 'encoder.layer.39.norm1.bias', 'encoder.layer.39.attention.attention.query.weight', 'encoder.layer.39.attention.attention.query.bias', 'encoder.layer.39.attention.attention.key.weight', 'encoder.layer.39.attention.attention.key.bias', 'encoder.layer.39.attention.attention.value.weight', 'encoder.layer.39.attention.attention.value.bias', 'encoder.layer.39.attention.output.dense.weight', 'encoder.layer.39.attention.output.dense.bias', 'encoder.layer.39.layer_scale1.lambda1', 'encoder.layer.39.norm2.weight', 'encoder.layer.39.norm2.bias', 'encoder.layer.39.mlp.weights_in.weight', 'encoder.layer.39.mlp.weights_in.bias', 'encoder.layer.39.mlp.weights_out.weight', 'encoder.layer.39.mlp.weights_out.bias', 'encoder.layer.39.layer_scale2.lambda1', 'layernorm.weight', 'layernorm.bias']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20/532262637.py:28: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=cfg.use_amp)\n",
      "/tmp/ipykernel_20/3107769954.py:9: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=cfg.use_amp):\n",
      "/tmp/ipykernel_20/3107769954.py:29: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=cfg.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 | train: 22.1410 | valid: 17.3100 | 26.5s\n",
      "Epoch 2/5 | train: 14.8693 | valid: 14.7403 | 24.3s\n",
      "Epoch 3/5 | train: 13.5472 | valid: 14.4964 | 24.6s\n",
      "Epoch 4/5 | train: 13.5967 | valid: 14.4834 | 25.2s\n",
      "Epoch 5/5 | train: 13.5650 | valid: 14.5113 | 25.4s\n"
     ]
    }
   ],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "criterion = nn.SmoothL1Loss()\n",
    "oof_predictions = np.zeros((len(train_targets), cfg.num_targets), dtype=np.float32)\n",
    "test_images = pd.DataFrame({'image_path': test_df['image_path'].unique()})\n",
    "\n",
    "all_fold_models = []\n",
    "\n",
    "for fold in range(cfg.n_folds):\n",
    "    if fold not in cfg.train_folds:\n",
    "        continue\n",
    "    print(f\"===== Fold {fold} =====\")\n",
    "    train_split = train_targets[train_targets['fold'] != fold].reset_index(drop=True)\n",
    "    valid_split = train_targets[train_targets['fold'] == fold].reset_index(drop=True)\n",
    "    train_loader, valid_loader = get_dataloaders(train_split, valid_split)\n",
    "\n",
    "    model = DinoRegressor(cfg).to(DEVICE)\n",
    "    optimizer = AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "    steps_per_epoch = len(train_loader)\n",
    "    scheduler = OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=cfg.lr,\n",
    "        epochs=cfg.epochs,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        pct_start=0.1,\n",
    "        div_factor=25,\n",
    "        final_div_factor=cfg.lr / cfg.min_lr,\n",
    "    )\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=cfg.use_amp)\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    best_state = None\n",
    "\n",
    "    for epoch in range(cfg.epochs):\n",
    "        start = time.time()\n",
    "        train_loss = train_one_epoch(model, train_loader, criterion, optimizer, scheduler, scaler, DEVICE)\n",
    "        val_loss, val_preds = validate_one_epoch(model, valid_loader, criterion, DEVICE)\n",
    "        duration = time.time() - start\n",
    "        print(f\"Epoch {epoch+1}/{cfg.epochs} | train: {train_loss:.4f} | valid: {val_loss:.4f} | {duration:.1f}s\")\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_state = model.state_dict()\n",
    "    model.load_state_dict(best_state)\n",
    "    all_fold_models.append(best_state)\n",
    "\n",
    "    fold_idx = valid_split.index\n",
    "    oof_predictions[fold_idx] = val_preds\n",
    "\n",
    "np.save(cfg.output_dir / 'oof_predictions.npy', oof_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2feb6676",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T10:54:01.090655Z",
     "iopub.status.busy": "2025-12-29T10:54:01.090092Z",
     "iopub.status.idle": "2025-12-29T10:54:06.416344Z",
     "shell.execute_reply": "2025-12-29T10:54:06.415316Z"
    },
    "papermill": {
     "duration": 5.332319,
     "end_time": "2025-12-29T10:54:06.418060",
     "exception": false,
     "start_time": "2025-12-29T10:54:01.085741",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint with missing keys: ['cls_token', 'pos_embed', 'patch_embed.proj.weight', 'patch_embed.proj.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.attn.qkv.weight', 'blocks.0.attn.qkv.bias', 'blocks.0.attn.proj.weight', 'blocks.0.attn.proj.bias', 'blocks.0.ls1.gamma', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.0.mlp.fc1.weight', 'blocks.0.mlp.fc1.bias', 'blocks.0.mlp.fc2.weight', 'blocks.0.mlp.fc2.bias', 'blocks.0.ls2.gamma', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.attn.qkv.weight', 'blocks.1.attn.qkv.bias', 'blocks.1.attn.proj.weight', 'blocks.1.attn.proj.bias', 'blocks.1.ls1.gamma', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.1.mlp.fc1.weight', 'blocks.1.mlp.fc1.bias', 'blocks.1.mlp.fc2.weight', 'blocks.1.mlp.fc2.bias', 'blocks.1.ls2.gamma', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.attn.qkv.weight', 'blocks.2.attn.qkv.bias', 'blocks.2.attn.proj.weight', 'blocks.2.attn.proj.bias', 'blocks.2.ls1.gamma', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.2.mlp.fc1.weight', 'blocks.2.mlp.fc1.bias', 'blocks.2.mlp.fc2.weight', 'blocks.2.mlp.fc2.bias', 'blocks.2.ls2.gamma', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.attn.qkv.weight', 'blocks.3.attn.qkv.bias', 'blocks.3.attn.proj.weight', 'blocks.3.attn.proj.bias', 'blocks.3.ls1.gamma', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.3.mlp.fc1.weight', 'blocks.3.mlp.fc1.bias', 'blocks.3.mlp.fc2.weight', 'blocks.3.mlp.fc2.bias', 'blocks.3.ls2.gamma', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.attn.qkv.weight', 'blocks.4.attn.qkv.bias', 'blocks.4.attn.proj.weight', 'blocks.4.attn.proj.bias', 'blocks.4.ls1.gamma', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.4.mlp.fc1.weight', 'blocks.4.mlp.fc1.bias', 'blocks.4.mlp.fc2.weight', 'blocks.4.mlp.fc2.bias', 'blocks.4.ls2.gamma', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.attn.qkv.weight', 'blocks.5.attn.qkv.bias', 'blocks.5.attn.proj.weight', 'blocks.5.attn.proj.bias', 'blocks.5.ls1.gamma', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.5.mlp.fc1.weight', 'blocks.5.mlp.fc1.bias', 'blocks.5.mlp.fc2.weight', 'blocks.5.mlp.fc2.bias', 'blocks.5.ls2.gamma', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.attn.qkv.weight', 'blocks.6.attn.qkv.bias', 'blocks.6.attn.proj.weight', 'blocks.6.attn.proj.bias', 'blocks.6.ls1.gamma', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'blocks.6.mlp.fc1.weight', 'blocks.6.mlp.fc1.bias', 'blocks.6.mlp.fc2.weight', 'blocks.6.mlp.fc2.bias', 'blocks.6.ls2.gamma', 'blocks.7.norm1.weight', 'blocks.7.norm1.bias', 'blocks.7.attn.qkv.weight', 'blocks.7.attn.qkv.bias', 'blocks.7.attn.proj.weight', 'blocks.7.attn.proj.bias', 'blocks.7.ls1.gamma', 'blocks.7.norm2.weight', 'blocks.7.norm2.bias', 'blocks.7.mlp.fc1.weight', 'blocks.7.mlp.fc1.bias', 'blocks.7.mlp.fc2.weight', 'blocks.7.mlp.fc2.bias', 'blocks.7.ls2.gamma', 'blocks.8.norm1.weight', 'blocks.8.norm1.bias', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.qkv.bias', 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.8.ls1.gamma', 'blocks.8.norm2.weight', 'blocks.8.norm2.bias', 'blocks.8.mlp.fc1.weight', 'blocks.8.mlp.fc1.bias', 'blocks.8.mlp.fc2.weight', 'blocks.8.mlp.fc2.bias', 'blocks.8.ls2.gamma', 'blocks.9.norm1.weight', 'blocks.9.norm1.bias', 'blocks.9.attn.qkv.weight', 'blocks.9.attn.qkv.bias', 'blocks.9.attn.proj.weight', 'blocks.9.attn.proj.bias', 'blocks.9.ls1.gamma', 'blocks.9.norm2.weight', 'blocks.9.norm2.bias', 'blocks.9.mlp.fc1.weight', 'blocks.9.mlp.fc1.bias', 'blocks.9.mlp.fc2.weight', 'blocks.9.mlp.fc2.bias', 'blocks.9.ls2.gamma', 'blocks.10.norm1.weight', 'blocks.10.norm1.bias', 'blocks.10.attn.qkv.weight', 'blocks.10.attn.qkv.bias', 'blocks.10.attn.proj.weight', 'blocks.10.attn.proj.bias', 'blocks.10.ls1.gamma', 'blocks.10.norm2.weight', 'blocks.10.norm2.bias', 'blocks.10.mlp.fc1.weight', 'blocks.10.mlp.fc1.bias', 'blocks.10.mlp.fc2.weight', 'blocks.10.mlp.fc2.bias', 'blocks.10.ls2.gamma', 'blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.ls1.gamma', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.11.mlp.fc1.weight', 'blocks.11.mlp.fc1.bias', 'blocks.11.mlp.fc2.weight', 'blocks.11.mlp.fc2.bias', 'blocks.11.ls2.gamma', 'norm.weight', 'norm.bias']\n",
      "Unexpected keys: ['embeddings.cls_token', 'embeddings.mask_token', 'embeddings.position_embeddings', 'embeddings.patch_embeddings.projection.weight', 'embeddings.patch_embeddings.projection.bias', 'encoder.layer.0.norm1.weight', 'encoder.layer.0.norm1.bias', 'encoder.layer.0.attention.attention.query.weight', 'encoder.layer.0.attention.attention.query.bias', 'encoder.layer.0.attention.attention.key.weight', 'encoder.layer.0.attention.attention.key.bias', 'encoder.layer.0.attention.attention.value.weight', 'encoder.layer.0.attention.attention.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.layer_scale1.lambda1', 'encoder.layer.0.norm2.weight', 'encoder.layer.0.norm2.bias', 'encoder.layer.0.mlp.weights_in.weight', 'encoder.layer.0.mlp.weights_in.bias', 'encoder.layer.0.mlp.weights_out.weight', 'encoder.layer.0.mlp.weights_out.bias', 'encoder.layer.0.layer_scale2.lambda1', 'encoder.layer.1.norm1.weight', 'encoder.layer.1.norm1.bias', 'encoder.layer.1.attention.attention.query.weight', 'encoder.layer.1.attention.attention.query.bias', 'encoder.layer.1.attention.attention.key.weight', 'encoder.layer.1.attention.attention.key.bias', 'encoder.layer.1.attention.attention.value.weight', 'encoder.layer.1.attention.attention.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.layer_scale1.lambda1', 'encoder.layer.1.norm2.weight', 'encoder.layer.1.norm2.bias', 'encoder.layer.1.mlp.weights_in.weight', 'encoder.layer.1.mlp.weights_in.bias', 'encoder.layer.1.mlp.weights_out.weight', 'encoder.layer.1.mlp.weights_out.bias', 'encoder.layer.1.layer_scale2.lambda1', 'encoder.layer.2.norm1.weight', 'encoder.layer.2.norm1.bias', 'encoder.layer.2.attention.attention.query.weight', 'encoder.layer.2.attention.attention.query.bias', 'encoder.layer.2.attention.attention.key.weight', 'encoder.layer.2.attention.attention.key.bias', 'encoder.layer.2.attention.attention.value.weight', 'encoder.layer.2.attention.attention.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.layer_scale1.lambda1', 'encoder.layer.2.norm2.weight', 'encoder.layer.2.norm2.bias', 'encoder.layer.2.mlp.weights_in.weight', 'encoder.layer.2.mlp.weights_in.bias', 'encoder.layer.2.mlp.weights_out.weight', 'encoder.layer.2.mlp.weights_out.bias', 'encoder.layer.2.layer_scale2.lambda1', 'encoder.layer.3.norm1.weight', 'encoder.layer.3.norm1.bias', 'encoder.layer.3.attention.attention.query.weight', 'encoder.layer.3.attention.attention.query.bias', 'encoder.layer.3.attention.attention.key.weight', 'encoder.layer.3.attention.attention.key.bias', 'encoder.layer.3.attention.attention.value.weight', 'encoder.layer.3.attention.attention.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.layer_scale1.lambda1', 'encoder.layer.3.norm2.weight', 'encoder.layer.3.norm2.bias', 'encoder.layer.3.mlp.weights_in.weight', 'encoder.layer.3.mlp.weights_in.bias', 'encoder.layer.3.mlp.weights_out.weight', 'encoder.layer.3.mlp.weights_out.bias', 'encoder.layer.3.layer_scale2.lambda1', 'encoder.layer.4.norm1.weight', 'encoder.layer.4.norm1.bias', 'encoder.layer.4.attention.attention.query.weight', 'encoder.layer.4.attention.attention.query.bias', 'encoder.layer.4.attention.attention.key.weight', 'encoder.layer.4.attention.attention.key.bias', 'encoder.layer.4.attention.attention.value.weight', 'encoder.layer.4.attention.attention.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.layer_scale1.lambda1', 'encoder.layer.4.norm2.weight', 'encoder.layer.4.norm2.bias', 'encoder.layer.4.mlp.weights_in.weight', 'encoder.layer.4.mlp.weights_in.bias', 'encoder.layer.4.mlp.weights_out.weight', 'encoder.layer.4.mlp.weights_out.bias', 'encoder.layer.4.layer_scale2.lambda1', 'encoder.layer.5.norm1.weight', 'encoder.layer.5.norm1.bias', 'encoder.layer.5.attention.attention.query.weight', 'encoder.layer.5.attention.attention.query.bias', 'encoder.layer.5.attention.attention.key.weight', 'encoder.layer.5.attention.attention.key.bias', 'encoder.layer.5.attention.attention.value.weight', 'encoder.layer.5.attention.attention.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.layer_scale1.lambda1', 'encoder.layer.5.norm2.weight', 'encoder.layer.5.norm2.bias', 'encoder.layer.5.mlp.weights_in.weight', 'encoder.layer.5.mlp.weights_in.bias', 'encoder.layer.5.mlp.weights_out.weight', 'encoder.layer.5.mlp.weights_out.bias', 'encoder.layer.5.layer_scale2.lambda1', 'encoder.layer.6.norm1.weight', 'encoder.layer.6.norm1.bias', 'encoder.layer.6.attention.attention.query.weight', 'encoder.layer.6.attention.attention.query.bias', 'encoder.layer.6.attention.attention.key.weight', 'encoder.layer.6.attention.attention.key.bias', 'encoder.layer.6.attention.attention.value.weight', 'encoder.layer.6.attention.attention.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.layer_scale1.lambda1', 'encoder.layer.6.norm2.weight', 'encoder.layer.6.norm2.bias', 'encoder.layer.6.mlp.weights_in.weight', 'encoder.layer.6.mlp.weights_in.bias', 'encoder.layer.6.mlp.weights_out.weight', 'encoder.layer.6.mlp.weights_out.bias', 'encoder.layer.6.layer_scale2.lambda1', 'encoder.layer.7.norm1.weight', 'encoder.layer.7.norm1.bias', 'encoder.layer.7.attention.attention.query.weight', 'encoder.layer.7.attention.attention.query.bias', 'encoder.layer.7.attention.attention.key.weight', 'encoder.layer.7.attention.attention.key.bias', 'encoder.layer.7.attention.attention.value.weight', 'encoder.layer.7.attention.attention.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.layer_scale1.lambda1', 'encoder.layer.7.norm2.weight', 'encoder.layer.7.norm2.bias', 'encoder.layer.7.mlp.weights_in.weight', 'encoder.layer.7.mlp.weights_in.bias', 'encoder.layer.7.mlp.weights_out.weight', 'encoder.layer.7.mlp.weights_out.bias', 'encoder.layer.7.layer_scale2.lambda1', 'encoder.layer.8.norm1.weight', 'encoder.layer.8.norm1.bias', 'encoder.layer.8.attention.attention.query.weight', 'encoder.layer.8.attention.attention.query.bias', 'encoder.layer.8.attention.attention.key.weight', 'encoder.layer.8.attention.attention.key.bias', 'encoder.layer.8.attention.attention.value.weight', 'encoder.layer.8.attention.attention.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.layer_scale1.lambda1', 'encoder.layer.8.norm2.weight', 'encoder.layer.8.norm2.bias', 'encoder.layer.8.mlp.weights_in.weight', 'encoder.layer.8.mlp.weights_in.bias', 'encoder.layer.8.mlp.weights_out.weight', 'encoder.layer.8.mlp.weights_out.bias', 'encoder.layer.8.layer_scale2.lambda1', 'encoder.layer.9.norm1.weight', 'encoder.layer.9.norm1.bias', 'encoder.layer.9.attention.attention.query.weight', 'encoder.layer.9.attention.attention.query.bias', 'encoder.layer.9.attention.attention.key.weight', 'encoder.layer.9.attention.attention.key.bias', 'encoder.layer.9.attention.attention.value.weight', 'encoder.layer.9.attention.attention.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.layer_scale1.lambda1', 'encoder.layer.9.norm2.weight', 'encoder.layer.9.norm2.bias', 'encoder.layer.9.mlp.weights_in.weight', 'encoder.layer.9.mlp.weights_in.bias', 'encoder.layer.9.mlp.weights_out.weight', 'encoder.layer.9.mlp.weights_out.bias', 'encoder.layer.9.layer_scale2.lambda1', 'encoder.layer.10.norm1.weight', 'encoder.layer.10.norm1.bias', 'encoder.layer.10.attention.attention.query.weight', 'encoder.layer.10.attention.attention.query.bias', 'encoder.layer.10.attention.attention.key.weight', 'encoder.layer.10.attention.attention.key.bias', 'encoder.layer.10.attention.attention.value.weight', 'encoder.layer.10.attention.attention.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.layer_scale1.lambda1', 'encoder.layer.10.norm2.weight', 'encoder.layer.10.norm2.bias', 'encoder.layer.10.mlp.weights_in.weight', 'encoder.layer.10.mlp.weights_in.bias', 'encoder.layer.10.mlp.weights_out.weight', 'encoder.layer.10.mlp.weights_out.bias', 'encoder.layer.10.layer_scale2.lambda1', 'encoder.layer.11.norm1.weight', 'encoder.layer.11.norm1.bias', 'encoder.layer.11.attention.attention.query.weight', 'encoder.layer.11.attention.attention.query.bias', 'encoder.layer.11.attention.attention.key.weight', 'encoder.layer.11.attention.attention.key.bias', 'encoder.layer.11.attention.attention.value.weight', 'encoder.layer.11.attention.attention.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.layer_scale1.lambda1', 'encoder.layer.11.norm2.weight', 'encoder.layer.11.norm2.bias', 'encoder.layer.11.mlp.weights_in.weight', 'encoder.layer.11.mlp.weights_in.bias', 'encoder.layer.11.mlp.weights_out.weight', 'encoder.layer.11.mlp.weights_out.bias', 'encoder.layer.11.layer_scale2.lambda1', 'encoder.layer.12.norm1.weight', 'encoder.layer.12.norm1.bias', 'encoder.layer.12.attention.attention.query.weight', 'encoder.layer.12.attention.attention.query.bias', 'encoder.layer.12.attention.attention.key.weight', 'encoder.layer.12.attention.attention.key.bias', 'encoder.layer.12.attention.attention.value.weight', 'encoder.layer.12.attention.attention.value.bias', 'encoder.layer.12.attention.output.dense.weight', 'encoder.layer.12.attention.output.dense.bias', 'encoder.layer.12.layer_scale1.lambda1', 'encoder.layer.12.norm2.weight', 'encoder.layer.12.norm2.bias', 'encoder.layer.12.mlp.weights_in.weight', 'encoder.layer.12.mlp.weights_in.bias', 'encoder.layer.12.mlp.weights_out.weight', 'encoder.layer.12.mlp.weights_out.bias', 'encoder.layer.12.layer_scale2.lambda1', 'encoder.layer.13.norm1.weight', 'encoder.layer.13.norm1.bias', 'encoder.layer.13.attention.attention.query.weight', 'encoder.layer.13.attention.attention.query.bias', 'encoder.layer.13.attention.attention.key.weight', 'encoder.layer.13.attention.attention.key.bias', 'encoder.layer.13.attention.attention.value.weight', 'encoder.layer.13.attention.attention.value.bias', 'encoder.layer.13.attention.output.dense.weight', 'encoder.layer.13.attention.output.dense.bias', 'encoder.layer.13.layer_scale1.lambda1', 'encoder.layer.13.norm2.weight', 'encoder.layer.13.norm2.bias', 'encoder.layer.13.mlp.weights_in.weight', 'encoder.layer.13.mlp.weights_in.bias', 'encoder.layer.13.mlp.weights_out.weight', 'encoder.layer.13.mlp.weights_out.bias', 'encoder.layer.13.layer_scale2.lambda1', 'encoder.layer.14.norm1.weight', 'encoder.layer.14.norm1.bias', 'encoder.layer.14.attention.attention.query.weight', 'encoder.layer.14.attention.attention.query.bias', 'encoder.layer.14.attention.attention.key.weight', 'encoder.layer.14.attention.attention.key.bias', 'encoder.layer.14.attention.attention.value.weight', 'encoder.layer.14.attention.attention.value.bias', 'encoder.layer.14.attention.output.dense.weight', 'encoder.layer.14.attention.output.dense.bias', 'encoder.layer.14.layer_scale1.lambda1', 'encoder.layer.14.norm2.weight', 'encoder.layer.14.norm2.bias', 'encoder.layer.14.mlp.weights_in.weight', 'encoder.layer.14.mlp.weights_in.bias', 'encoder.layer.14.mlp.weights_out.weight', 'encoder.layer.14.mlp.weights_out.bias', 'encoder.layer.14.layer_scale2.lambda1', 'encoder.layer.15.norm1.weight', 'encoder.layer.15.norm1.bias', 'encoder.layer.15.attention.attention.query.weight', 'encoder.layer.15.attention.attention.query.bias', 'encoder.layer.15.attention.attention.key.weight', 'encoder.layer.15.attention.attention.key.bias', 'encoder.layer.15.attention.attention.value.weight', 'encoder.layer.15.attention.attention.value.bias', 'encoder.layer.15.attention.output.dense.weight', 'encoder.layer.15.attention.output.dense.bias', 'encoder.layer.15.layer_scale1.lambda1', 'encoder.layer.15.norm2.weight', 'encoder.layer.15.norm2.bias', 'encoder.layer.15.mlp.weights_in.weight', 'encoder.layer.15.mlp.weights_in.bias', 'encoder.layer.15.mlp.weights_out.weight', 'encoder.layer.15.mlp.weights_out.bias', 'encoder.layer.15.layer_scale2.lambda1', 'encoder.layer.16.norm1.weight', 'encoder.layer.16.norm1.bias', 'encoder.layer.16.attention.attention.query.weight', 'encoder.layer.16.attention.attention.query.bias', 'encoder.layer.16.attention.attention.key.weight', 'encoder.layer.16.attention.attention.key.bias', 'encoder.layer.16.attention.attention.value.weight', 'encoder.layer.16.attention.attention.value.bias', 'encoder.layer.16.attention.output.dense.weight', 'encoder.layer.16.attention.output.dense.bias', 'encoder.layer.16.layer_scale1.lambda1', 'encoder.layer.16.norm2.weight', 'encoder.layer.16.norm2.bias', 'encoder.layer.16.mlp.weights_in.weight', 'encoder.layer.16.mlp.weights_in.bias', 'encoder.layer.16.mlp.weights_out.weight', 'encoder.layer.16.mlp.weights_out.bias', 'encoder.layer.16.layer_scale2.lambda1', 'encoder.layer.17.norm1.weight', 'encoder.layer.17.norm1.bias', 'encoder.layer.17.attention.attention.query.weight', 'encoder.layer.17.attention.attention.query.bias', 'encoder.layer.17.attention.attention.key.weight', 'encoder.layer.17.attention.attention.key.bias', 'encoder.layer.17.attention.attention.value.weight', 'encoder.layer.17.attention.attention.value.bias', 'encoder.layer.17.attention.output.dense.weight', 'encoder.layer.17.attention.output.dense.bias', 'encoder.layer.17.layer_scale1.lambda1', 'encoder.layer.17.norm2.weight', 'encoder.layer.17.norm2.bias', 'encoder.layer.17.mlp.weights_in.weight', 'encoder.layer.17.mlp.weights_in.bias', 'encoder.layer.17.mlp.weights_out.weight', 'encoder.layer.17.mlp.weights_out.bias', 'encoder.layer.17.layer_scale2.lambda1', 'encoder.layer.18.norm1.weight', 'encoder.layer.18.norm1.bias', 'encoder.layer.18.attention.attention.query.weight', 'encoder.layer.18.attention.attention.query.bias', 'encoder.layer.18.attention.attention.key.weight', 'encoder.layer.18.attention.attention.key.bias', 'encoder.layer.18.attention.attention.value.weight', 'encoder.layer.18.attention.attention.value.bias', 'encoder.layer.18.attention.output.dense.weight', 'encoder.layer.18.attention.output.dense.bias', 'encoder.layer.18.layer_scale1.lambda1', 'encoder.layer.18.norm2.weight', 'encoder.layer.18.norm2.bias', 'encoder.layer.18.mlp.weights_in.weight', 'encoder.layer.18.mlp.weights_in.bias', 'encoder.layer.18.mlp.weights_out.weight', 'encoder.layer.18.mlp.weights_out.bias', 'encoder.layer.18.layer_scale2.lambda1', 'encoder.layer.19.norm1.weight', 'encoder.layer.19.norm1.bias', 'encoder.layer.19.attention.attention.query.weight', 'encoder.layer.19.attention.attention.query.bias', 'encoder.layer.19.attention.attention.key.weight', 'encoder.layer.19.attention.attention.key.bias', 'encoder.layer.19.attention.attention.value.weight', 'encoder.layer.19.attention.attention.value.bias', 'encoder.layer.19.attention.output.dense.weight', 'encoder.layer.19.attention.output.dense.bias', 'encoder.layer.19.layer_scale1.lambda1', 'encoder.layer.19.norm2.weight', 'encoder.layer.19.norm2.bias', 'encoder.layer.19.mlp.weights_in.weight', 'encoder.layer.19.mlp.weights_in.bias', 'encoder.layer.19.mlp.weights_out.weight', 'encoder.layer.19.mlp.weights_out.bias', 'encoder.layer.19.layer_scale2.lambda1', 'encoder.layer.20.norm1.weight', 'encoder.layer.20.norm1.bias', 'encoder.layer.20.attention.attention.query.weight', 'encoder.layer.20.attention.attention.query.bias', 'encoder.layer.20.attention.attention.key.weight', 'encoder.layer.20.attention.attention.key.bias', 'encoder.layer.20.attention.attention.value.weight', 'encoder.layer.20.attention.attention.value.bias', 'encoder.layer.20.attention.output.dense.weight', 'encoder.layer.20.attention.output.dense.bias', 'encoder.layer.20.layer_scale1.lambda1', 'encoder.layer.20.norm2.weight', 'encoder.layer.20.norm2.bias', 'encoder.layer.20.mlp.weights_in.weight', 'encoder.layer.20.mlp.weights_in.bias', 'encoder.layer.20.mlp.weights_out.weight', 'encoder.layer.20.mlp.weights_out.bias', 'encoder.layer.20.layer_scale2.lambda1', 'encoder.layer.21.norm1.weight', 'encoder.layer.21.norm1.bias', 'encoder.layer.21.attention.attention.query.weight', 'encoder.layer.21.attention.attention.query.bias', 'encoder.layer.21.attention.attention.key.weight', 'encoder.layer.21.attention.attention.key.bias', 'encoder.layer.21.attention.attention.value.weight', 'encoder.layer.21.attention.attention.value.bias', 'encoder.layer.21.attention.output.dense.weight', 'encoder.layer.21.attention.output.dense.bias', 'encoder.layer.21.layer_scale1.lambda1', 'encoder.layer.21.norm2.weight', 'encoder.layer.21.norm2.bias', 'encoder.layer.21.mlp.weights_in.weight', 'encoder.layer.21.mlp.weights_in.bias', 'encoder.layer.21.mlp.weights_out.weight', 'encoder.layer.21.mlp.weights_out.bias', 'encoder.layer.21.layer_scale2.lambda1', 'encoder.layer.22.norm1.weight', 'encoder.layer.22.norm1.bias', 'encoder.layer.22.attention.attention.query.weight', 'encoder.layer.22.attention.attention.query.bias', 'encoder.layer.22.attention.attention.key.weight', 'encoder.layer.22.attention.attention.key.bias', 'encoder.layer.22.attention.attention.value.weight', 'encoder.layer.22.attention.attention.value.bias', 'encoder.layer.22.attention.output.dense.weight', 'encoder.layer.22.attention.output.dense.bias', 'encoder.layer.22.layer_scale1.lambda1', 'encoder.layer.22.norm2.weight', 'encoder.layer.22.norm2.bias', 'encoder.layer.22.mlp.weights_in.weight', 'encoder.layer.22.mlp.weights_in.bias', 'encoder.layer.22.mlp.weights_out.weight', 'encoder.layer.22.mlp.weights_out.bias', 'encoder.layer.22.layer_scale2.lambda1', 'encoder.layer.23.norm1.weight', 'encoder.layer.23.norm1.bias', 'encoder.layer.23.attention.attention.query.weight', 'encoder.layer.23.attention.attention.query.bias', 'encoder.layer.23.attention.attention.key.weight', 'encoder.layer.23.attention.attention.key.bias', 'encoder.layer.23.attention.attention.value.weight', 'encoder.layer.23.attention.attention.value.bias', 'encoder.layer.23.attention.output.dense.weight', 'encoder.layer.23.attention.output.dense.bias', 'encoder.layer.23.layer_scale1.lambda1', 'encoder.layer.23.norm2.weight', 'encoder.layer.23.norm2.bias', 'encoder.layer.23.mlp.weights_in.weight', 'encoder.layer.23.mlp.weights_in.bias', 'encoder.layer.23.mlp.weights_out.weight', 'encoder.layer.23.mlp.weights_out.bias', 'encoder.layer.23.layer_scale2.lambda1', 'encoder.layer.24.norm1.weight', 'encoder.layer.24.norm1.bias', 'encoder.layer.24.attention.attention.query.weight', 'encoder.layer.24.attention.attention.query.bias', 'encoder.layer.24.attention.attention.key.weight', 'encoder.layer.24.attention.attention.key.bias', 'encoder.layer.24.attention.attention.value.weight', 'encoder.layer.24.attention.attention.value.bias', 'encoder.layer.24.attention.output.dense.weight', 'encoder.layer.24.attention.output.dense.bias', 'encoder.layer.24.layer_scale1.lambda1', 'encoder.layer.24.norm2.weight', 'encoder.layer.24.norm2.bias', 'encoder.layer.24.mlp.weights_in.weight', 'encoder.layer.24.mlp.weights_in.bias', 'encoder.layer.24.mlp.weights_out.weight', 'encoder.layer.24.mlp.weights_out.bias', 'encoder.layer.24.layer_scale2.lambda1', 'encoder.layer.25.norm1.weight', 'encoder.layer.25.norm1.bias', 'encoder.layer.25.attention.attention.query.weight', 'encoder.layer.25.attention.attention.query.bias', 'encoder.layer.25.attention.attention.key.weight', 'encoder.layer.25.attention.attention.key.bias', 'encoder.layer.25.attention.attention.value.weight', 'encoder.layer.25.attention.attention.value.bias', 'encoder.layer.25.attention.output.dense.weight', 'encoder.layer.25.attention.output.dense.bias', 'encoder.layer.25.layer_scale1.lambda1', 'encoder.layer.25.norm2.weight', 'encoder.layer.25.norm2.bias', 'encoder.layer.25.mlp.weights_in.weight', 'encoder.layer.25.mlp.weights_in.bias', 'encoder.layer.25.mlp.weights_out.weight', 'encoder.layer.25.mlp.weights_out.bias', 'encoder.layer.25.layer_scale2.lambda1', 'encoder.layer.26.norm1.weight', 'encoder.layer.26.norm1.bias', 'encoder.layer.26.attention.attention.query.weight', 'encoder.layer.26.attention.attention.query.bias', 'encoder.layer.26.attention.attention.key.weight', 'encoder.layer.26.attention.attention.key.bias', 'encoder.layer.26.attention.attention.value.weight', 'encoder.layer.26.attention.attention.value.bias', 'encoder.layer.26.attention.output.dense.weight', 'encoder.layer.26.attention.output.dense.bias', 'encoder.layer.26.layer_scale1.lambda1', 'encoder.layer.26.norm2.weight', 'encoder.layer.26.norm2.bias', 'encoder.layer.26.mlp.weights_in.weight', 'encoder.layer.26.mlp.weights_in.bias', 'encoder.layer.26.mlp.weights_out.weight', 'encoder.layer.26.mlp.weights_out.bias', 'encoder.layer.26.layer_scale2.lambda1', 'encoder.layer.27.norm1.weight', 'encoder.layer.27.norm1.bias', 'encoder.layer.27.attention.attention.query.weight', 'encoder.layer.27.attention.attention.query.bias', 'encoder.layer.27.attention.attention.key.weight', 'encoder.layer.27.attention.attention.key.bias', 'encoder.layer.27.attention.attention.value.weight', 'encoder.layer.27.attention.attention.value.bias', 'encoder.layer.27.attention.output.dense.weight', 'encoder.layer.27.attention.output.dense.bias', 'encoder.layer.27.layer_scale1.lambda1', 'encoder.layer.27.norm2.weight', 'encoder.layer.27.norm2.bias', 'encoder.layer.27.mlp.weights_in.weight', 'encoder.layer.27.mlp.weights_in.bias', 'encoder.layer.27.mlp.weights_out.weight', 'encoder.layer.27.mlp.weights_out.bias', 'encoder.layer.27.layer_scale2.lambda1', 'encoder.layer.28.norm1.weight', 'encoder.layer.28.norm1.bias', 'encoder.layer.28.attention.attention.query.weight', 'encoder.layer.28.attention.attention.query.bias', 'encoder.layer.28.attention.attention.key.weight', 'encoder.layer.28.attention.attention.key.bias', 'encoder.layer.28.attention.attention.value.weight', 'encoder.layer.28.attention.attention.value.bias', 'encoder.layer.28.attention.output.dense.weight', 'encoder.layer.28.attention.output.dense.bias', 'encoder.layer.28.layer_scale1.lambda1', 'encoder.layer.28.norm2.weight', 'encoder.layer.28.norm2.bias', 'encoder.layer.28.mlp.weights_in.weight', 'encoder.layer.28.mlp.weights_in.bias', 'encoder.layer.28.mlp.weights_out.weight', 'encoder.layer.28.mlp.weights_out.bias', 'encoder.layer.28.layer_scale2.lambda1', 'encoder.layer.29.norm1.weight', 'encoder.layer.29.norm1.bias', 'encoder.layer.29.attention.attention.query.weight', 'encoder.layer.29.attention.attention.query.bias', 'encoder.layer.29.attention.attention.key.weight', 'encoder.layer.29.attention.attention.key.bias', 'encoder.layer.29.attention.attention.value.weight', 'encoder.layer.29.attention.attention.value.bias', 'encoder.layer.29.attention.output.dense.weight', 'encoder.layer.29.attention.output.dense.bias', 'encoder.layer.29.layer_scale1.lambda1', 'encoder.layer.29.norm2.weight', 'encoder.layer.29.norm2.bias', 'encoder.layer.29.mlp.weights_in.weight', 'encoder.layer.29.mlp.weights_in.bias', 'encoder.layer.29.mlp.weights_out.weight', 'encoder.layer.29.mlp.weights_out.bias', 'encoder.layer.29.layer_scale2.lambda1', 'encoder.layer.30.norm1.weight', 'encoder.layer.30.norm1.bias', 'encoder.layer.30.attention.attention.query.weight', 'encoder.layer.30.attention.attention.query.bias', 'encoder.layer.30.attention.attention.key.weight', 'encoder.layer.30.attention.attention.key.bias', 'encoder.layer.30.attention.attention.value.weight', 'encoder.layer.30.attention.attention.value.bias', 'encoder.layer.30.attention.output.dense.weight', 'encoder.layer.30.attention.output.dense.bias', 'encoder.layer.30.layer_scale1.lambda1', 'encoder.layer.30.norm2.weight', 'encoder.layer.30.norm2.bias', 'encoder.layer.30.mlp.weights_in.weight', 'encoder.layer.30.mlp.weights_in.bias', 'encoder.layer.30.mlp.weights_out.weight', 'encoder.layer.30.mlp.weights_out.bias', 'encoder.layer.30.layer_scale2.lambda1', 'encoder.layer.31.norm1.weight', 'encoder.layer.31.norm1.bias', 'encoder.layer.31.attention.attention.query.weight', 'encoder.layer.31.attention.attention.query.bias', 'encoder.layer.31.attention.attention.key.weight', 'encoder.layer.31.attention.attention.key.bias', 'encoder.layer.31.attention.attention.value.weight', 'encoder.layer.31.attention.attention.value.bias', 'encoder.layer.31.attention.output.dense.weight', 'encoder.layer.31.attention.output.dense.bias', 'encoder.layer.31.layer_scale1.lambda1', 'encoder.layer.31.norm2.weight', 'encoder.layer.31.norm2.bias', 'encoder.layer.31.mlp.weights_in.weight', 'encoder.layer.31.mlp.weights_in.bias', 'encoder.layer.31.mlp.weights_out.weight', 'encoder.layer.31.mlp.weights_out.bias', 'encoder.layer.31.layer_scale2.lambda1', 'encoder.layer.32.norm1.weight', 'encoder.layer.32.norm1.bias', 'encoder.layer.32.attention.attention.query.weight', 'encoder.layer.32.attention.attention.query.bias', 'encoder.layer.32.attention.attention.key.weight', 'encoder.layer.32.attention.attention.key.bias', 'encoder.layer.32.attention.attention.value.weight', 'encoder.layer.32.attention.attention.value.bias', 'encoder.layer.32.attention.output.dense.weight', 'encoder.layer.32.attention.output.dense.bias', 'encoder.layer.32.layer_scale1.lambda1', 'encoder.layer.32.norm2.weight', 'encoder.layer.32.norm2.bias', 'encoder.layer.32.mlp.weights_in.weight', 'encoder.layer.32.mlp.weights_in.bias', 'encoder.layer.32.mlp.weights_out.weight', 'encoder.layer.32.mlp.weights_out.bias', 'encoder.layer.32.layer_scale2.lambda1', 'encoder.layer.33.norm1.weight', 'encoder.layer.33.norm1.bias', 'encoder.layer.33.attention.attention.query.weight', 'encoder.layer.33.attention.attention.query.bias', 'encoder.layer.33.attention.attention.key.weight', 'encoder.layer.33.attention.attention.key.bias', 'encoder.layer.33.attention.attention.value.weight', 'encoder.layer.33.attention.attention.value.bias', 'encoder.layer.33.attention.output.dense.weight', 'encoder.layer.33.attention.output.dense.bias', 'encoder.layer.33.layer_scale1.lambda1', 'encoder.layer.33.norm2.weight', 'encoder.layer.33.norm2.bias', 'encoder.layer.33.mlp.weights_in.weight', 'encoder.layer.33.mlp.weights_in.bias', 'encoder.layer.33.mlp.weights_out.weight', 'encoder.layer.33.mlp.weights_out.bias', 'encoder.layer.33.layer_scale2.lambda1', 'encoder.layer.34.norm1.weight', 'encoder.layer.34.norm1.bias', 'encoder.layer.34.attention.attention.query.weight', 'encoder.layer.34.attention.attention.query.bias', 'encoder.layer.34.attention.attention.key.weight', 'encoder.layer.34.attention.attention.key.bias', 'encoder.layer.34.attention.attention.value.weight', 'encoder.layer.34.attention.attention.value.bias', 'encoder.layer.34.attention.output.dense.weight', 'encoder.layer.34.attention.output.dense.bias', 'encoder.layer.34.layer_scale1.lambda1', 'encoder.layer.34.norm2.weight', 'encoder.layer.34.norm2.bias', 'encoder.layer.34.mlp.weights_in.weight', 'encoder.layer.34.mlp.weights_in.bias', 'encoder.layer.34.mlp.weights_out.weight', 'encoder.layer.34.mlp.weights_out.bias', 'encoder.layer.34.layer_scale2.lambda1', 'encoder.layer.35.norm1.weight', 'encoder.layer.35.norm1.bias', 'encoder.layer.35.attention.attention.query.weight', 'encoder.layer.35.attention.attention.query.bias', 'encoder.layer.35.attention.attention.key.weight', 'encoder.layer.35.attention.attention.key.bias', 'encoder.layer.35.attention.attention.value.weight', 'encoder.layer.35.attention.attention.value.bias', 'encoder.layer.35.attention.output.dense.weight', 'encoder.layer.35.attention.output.dense.bias', 'encoder.layer.35.layer_scale1.lambda1', 'encoder.layer.35.norm2.weight', 'encoder.layer.35.norm2.bias', 'encoder.layer.35.mlp.weights_in.weight', 'encoder.layer.35.mlp.weights_in.bias', 'encoder.layer.35.mlp.weights_out.weight', 'encoder.layer.35.mlp.weights_out.bias', 'encoder.layer.35.layer_scale2.lambda1', 'encoder.layer.36.norm1.weight', 'encoder.layer.36.norm1.bias', 'encoder.layer.36.attention.attention.query.weight', 'encoder.layer.36.attention.attention.query.bias', 'encoder.layer.36.attention.attention.key.weight', 'encoder.layer.36.attention.attention.key.bias', 'encoder.layer.36.attention.attention.value.weight', 'encoder.layer.36.attention.attention.value.bias', 'encoder.layer.36.attention.output.dense.weight', 'encoder.layer.36.attention.output.dense.bias', 'encoder.layer.36.layer_scale1.lambda1', 'encoder.layer.36.norm2.weight', 'encoder.layer.36.norm2.bias', 'encoder.layer.36.mlp.weights_in.weight', 'encoder.layer.36.mlp.weights_in.bias', 'encoder.layer.36.mlp.weights_out.weight', 'encoder.layer.36.mlp.weights_out.bias', 'encoder.layer.36.layer_scale2.lambda1', 'encoder.layer.37.norm1.weight', 'encoder.layer.37.norm1.bias', 'encoder.layer.37.attention.attention.query.weight', 'encoder.layer.37.attention.attention.query.bias', 'encoder.layer.37.attention.attention.key.weight', 'encoder.layer.37.attention.attention.key.bias', 'encoder.layer.37.attention.attention.value.weight', 'encoder.layer.37.attention.attention.value.bias', 'encoder.layer.37.attention.output.dense.weight', 'encoder.layer.37.attention.output.dense.bias', 'encoder.layer.37.layer_scale1.lambda1', 'encoder.layer.37.norm2.weight', 'encoder.layer.37.norm2.bias', 'encoder.layer.37.mlp.weights_in.weight', 'encoder.layer.37.mlp.weights_in.bias', 'encoder.layer.37.mlp.weights_out.weight', 'encoder.layer.37.mlp.weights_out.bias', 'encoder.layer.37.layer_scale2.lambda1', 'encoder.layer.38.norm1.weight', 'encoder.layer.38.norm1.bias', 'encoder.layer.38.attention.attention.query.weight', 'encoder.layer.38.attention.attention.query.bias', 'encoder.layer.38.attention.attention.key.weight', 'encoder.layer.38.attention.attention.key.bias', 'encoder.layer.38.attention.attention.value.weight', 'encoder.layer.38.attention.attention.value.bias', 'encoder.layer.38.attention.output.dense.weight', 'encoder.layer.38.attention.output.dense.bias', 'encoder.layer.38.layer_scale1.lambda1', 'encoder.layer.38.norm2.weight', 'encoder.layer.38.norm2.bias', 'encoder.layer.38.mlp.weights_in.weight', 'encoder.layer.38.mlp.weights_in.bias', 'encoder.layer.38.mlp.weights_out.weight', 'encoder.layer.38.mlp.weights_out.bias', 'encoder.layer.38.layer_scale2.lambda1', 'encoder.layer.39.norm1.weight', 'encoder.layer.39.norm1.bias', 'encoder.layer.39.attention.attention.query.weight', 'encoder.layer.39.attention.attention.query.bias', 'encoder.layer.39.attention.attention.key.weight', 'encoder.layer.39.attention.attention.key.bias', 'encoder.layer.39.attention.attention.value.weight', 'encoder.layer.39.attention.attention.value.bias', 'encoder.layer.39.attention.output.dense.weight', 'encoder.layer.39.attention.output.dense.bias', 'encoder.layer.39.layer_scale1.lambda1', 'encoder.layer.39.norm2.weight', 'encoder.layer.39.norm2.bias', 'encoder.layer.39.mlp.weights_in.weight', 'encoder.layer.39.mlp.weights_in.bias', 'encoder.layer.39.mlp.weights_out.weight', 'encoder.layer.39.mlp.weights_out.bias', 'encoder.layer.39.layer_scale2.lambda1', 'layernorm.weight', 'layernorm.bias']\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def infer(model_states, df):\n",
    "    models = []\n",
    "    for state in model_states:\n",
    "        model = DinoRegressor(cfg).to(DEVICE)\n",
    "        model.load_state_dict(state, strict=True)\n",
    "        model.eval()\n",
    "        models.append(model)\n",
    "    dataset = BiomassDataset(df, mode='test', transform=get_transforms(False))\n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=cfg.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=cfg.num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    image_preds = {}\n",
    "    for batch in loader:\n",
    "        images = batch['pixel_values'].to(DEVICE, non_blocking=True)\n",
    "        preds = torch.zeros((images.size(0), cfg.num_targets), device=DEVICE)\n",
    "        for model in models:\n",
    "            preds += model(images)\n",
    "        preds /= len(models)\n",
    "        preds = preds.detach().cpu().numpy()\n",
    "        for path, pred in zip(batch['image_path'], preds):\n",
    "            image_preds[path] = pred\n",
    "    return image_preds\n",
    "\n",
    "ensemble_preds = infer(all_fold_models, test_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "181bbb9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T10:54:06.429432Z",
     "iopub.status.busy": "2025-12-29T10:54:06.428463Z",
     "iopub.status.idle": "2025-12-29T10:54:06.458095Z",
     "shell.execute_reply": "2025-12-29T10:54:06.457277Z"
    },
    "papermill": {
     "duration": 0.036866,
     "end_time": "2025-12-29T10:54:06.459369",
     "exception": false,
     "start_time": "2025-12-29T10:54:06.422503",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>image_path</th>\n",
       "      <th>target_name</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID1001187975__Dry_Clover_g</td>\n",
       "      <td>test/ID1001187975.jpg</td>\n",
       "      <td>Dry_Clover_g</td>\n",
       "      <td>1.687910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID1001187975__Dry_Dead_g</td>\n",
       "      <td>test/ID1001187975.jpg</td>\n",
       "      <td>Dry_Dead_g</td>\n",
       "      <td>8.642527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID1001187975__Dry_Green_g</td>\n",
       "      <td>test/ID1001187975.jpg</td>\n",
       "      <td>Dry_Green_g</td>\n",
       "      <td>21.038294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID1001187975__Dry_Total_g</td>\n",
       "      <td>test/ID1001187975.jpg</td>\n",
       "      <td>Dry_Total_g</td>\n",
       "      <td>40.556011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID1001187975__GDM_g</td>\n",
       "      <td>test/ID1001187975.jpg</td>\n",
       "      <td>GDM_g</td>\n",
       "      <td>26.401012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    sample_id             image_path   target_name     target\n",
       "0  ID1001187975__Dry_Clover_g  test/ID1001187975.jpg  Dry_Clover_g   1.687910\n",
       "1    ID1001187975__Dry_Dead_g  test/ID1001187975.jpg    Dry_Dead_g   8.642527\n",
       "2   ID1001187975__Dry_Green_g  test/ID1001187975.jpg   Dry_Green_g  21.038294\n",
       "3   ID1001187975__Dry_Total_g  test/ID1001187975.jpg   Dry_Total_g  40.556011\n",
       "4         ID1001187975__GDM_g  test/ID1001187975.jpg         GDM_g  26.401012"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = test_df.copy()\n",
    "submission['target'] = submission.apply(\n",
    "    lambda row: ensemble_preds[row['image_path']][cfg.target_names.index(row['target_name'])],\n",
    "    axis=1\n",
    ")\n",
    "submission[['sample_id', 'target']].to_csv('submission.csv', index=False)\n",
    "submission.head()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 14254895,
     "isSourceIdPinned": false,
     "sourceId": 112509,
     "sourceType": "competition"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 986,
     "modelInstanceId": 3329,
     "sourceId": 4537,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 194.001887,
   "end_time": "2025-12-29T10:54:09.577561",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-29T10:50:55.575674",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
